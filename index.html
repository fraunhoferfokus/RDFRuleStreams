<!DOCTYPE html>
<html>
  <head>
    <title>Extension of the SPARQL Abstract Data Model for RDF Stream Processing</title>
    <meta charset="utf-8" />
    <script src="http://www.w3.org/Tools/respec/respec-w3c-common" async="async" class="remove"></script>
    <script class="remove">
      var respecConfig = {
          specStatus: "CG-DRAFT",
          shortName:  "rdf-stream-semantics",
          editors: [
            {
              name:       "Editor 1",
              company:    "Company 1",
              companyURL: "http://example1.com/",
              mailto: "ed1@example1.com"
            },
            {
              name:       "Editor 2",
              company:    "Company 2",
              companyURL: "http://example2.com/",
              mailto: "ed2@example1.com"
            }
          ],
          authors: [
            {
              name:       "Tara Athan",
              company:    "Athan Services",
              companyURL: "http://athant.com",
              mailto: "taraathan@gmail.com"
            },
            {
              name:       "Author 2",
              company:    "Company 2",
              companyURL: "http://example2.com/",
              mailto: "auth2@example1.com"
            }
          ],
          wg:           "RDF Stream Processing Community Group",
          wgURI:        "https://www.w3.org/community/rsp/",
          wgPublicList: "public-rsp",

          localBiblio:  {
            "LINEARCOMPOSITION": {
              title: "Multi-device Linear Composition on the Web: Enabling Multi-device Linear Media with HTMLTimingObject and Shared Motion",
              href: "https://sites.google.com/site/mediasynchronization/Paper4_Arntzen_webComposition_CR.pdf?attredirects=0&amp;d=1",
              authors: [
                "Ingar M. Arntzen",
                "Njål T. Borch",
                "François Daoust",
                "Dominique Hazaël-Massieux"
              ]
            },
            "BLOGIC": {
              title:    "Blogic",
              href:     "http://www.slideshare.net/PatHayes/blogic-iswc-2009-invited-talk",
              authors:  [
                "Pat Hayes"
              ]
            },
            "DUL": {
              title:    "DOLCE+DnS Ultralite (dul)",
              href:     "http://lov.okfn.org/dataset/lov/vocabs/dul",
              authors:  [
                "Aldo Gangemi"
              ]
            },
            "DVB-CSS": {
              title: "ETSI TS 103 256-2 V1.1.1 Digital Video Broadcasting (DVB); Companion Screens and Streams; Part 2: Content Identification and Media Synchronization",
              href: "http://www.etsi.org/modules/mod_StandardSearch/pdf.png"
            },
            "SHAREDMOTION": {
              title: "Shared Motion",
              href: "http://motioncorporation.com"
            },
            "MEDIASYNC":{
              title: "MediaSync",
              href: "https://github.com/webtiming/mediasync"
            },
            "SEQUENCER" : {
              title: "Open-source sequencer library",
              href: "https://github.com/webtiming/sequencer"
            }
          },
          otherLinks: [
            {
              key: "Version history",
              data: [
                {
                  value: "GitHub streamreasoning/RSP-QL/commits",
                  href: "https://github.com/streamreasoning/RSP-QL/commits/"
                }
              ]
            },
            {
              key: "Participate",
              data: [
                {
                  value: "GitHub streamreasoning/RSP-QL",
                  href: "https://github.com/streamreasoning/RSP-QL"
                },
                {
                  value: "File an issue",
                  href: "https://github.com/streamreasoning/RSP-QL/issues/new"
                },
                {
                  value: "Open issues",
                  href: "https://github.com/streamreasoning/RSP-QL/issues/"
                },
                {
                  value: "Mailing-list (public-rsp@w3.org)",
                  href: "https://lists.w3.org/Archives/Public/public-rsp/"
                }
              ]
            }
          ],

          issueBase: "https://github.com/streamreasoning/RSP-QL/issues/",
          githubAPI: "https://api.github.com/repos/streamreasoning/RSP-QL"
      };
    </script>
    <style type="text/css">
      table{
          border-collapse: collapse;
          border-style: hidden hidden none hidden;
      }
      table thead,
      table tbody{
          border-bottom: solid;
      }
      table td,
      table th{
          border-left: solid;
          border-right: solid;
          border-bottom: solid thin;
          vertical-align: top;
          padding: 0.2em;
      }</style>
  </head>
  <body>
    <!-- ABSTRACT -->
    <section id="abstract"> </section>
    <!-- STATUS OF DOCUMENT -->
    <section id="sotd">
      <p> The specification is intended for discussion within the RDF Stream Processing Community
        Group. Its content does not yet represent the consensus of the Community Group. </p>
      <p class="warning"> This specification is incomplete. </p>
    </section>
    <!-- INTRODUCTION -->
    <section class="informative" id="intro">
      <h2>Introduction</h2>

    </section>

    <section id="basics">
      <h2>The Basics</h2>
      <section id="namespaces-and-prefixes">
        <h3>Namespaces and Prefixes</h3>
        <p class="ednote">Do we need any new namespaces for this document?
        </p>		
        <p>Additional prefixes used in this document are the following:
        </p>
        <div style="text-align: left;">
          <table class="thinborder" style="margin-left: auto; margin-right: auto;">
            <caption id="namespace-table"><a href="#namespace-table">Table 1</a>: Prefix and Namespaces used in this specification</caption> <!-- Table 1-->
            <tbody><tr><td><b>prefix</b></td><td><b>namespace IRI</b></td> <td><b>definition</b></td></tr>
              
              <tr><td>rsp</td><td><code>http://www.w3.org/ns/rsp#</code></td><td>The RSP rnamespace</td></tr>
              
              <tr><td>rdf</td><td><code>http://www.w3.org/1999/02/22-rdf-syntax-ns#</code></td><td>The RDF namespace [[!RDF-SCHEMA]]</td></tr>
              
              <tr><td>rdfs</td><td><code>http://www.w3.org/2000/01/rdf-schema#</code></td><td>The RDFS namespace [[!RDF-SCHEMA]]</td></tr>
              
              <tr><td>xsd</td><td><code>http://www.w3.org/2000/10/XMLSchema#</code></td><td>XML Schema Namespace [[!XMLSCHEMA11-2]]</td></tr>
              
              <tr><td>owl</td><td><code>http://www.w3.org/2002/07/owl#</code></td><td>The OWL namespace [[!OWL2-OVERVIEW]]</td></tr>
              
              <tr><td>owl-time</td><td><code>http://www.w3.org/2006/time#</code></td><td>The OWL-TIME namespace [[!OWL-TIME]]</td></tr>
              
              <tr><td>prov</td><td><code>http://www.w3.org/ns/prov#</code></td>
                <td>The PROV namespace [[!PROV-DM]]</td></tr>

              <tr><td>dul</td><td><code>http://www.ontologydesignpatterns.org/ont/dul/DUL.owl#</code></td>
                <td>The DOLCE+DnS Ultralet namespace (http://lov.okfn.org/dataset/lov/vocabs/dul) [[DUL]]</td></tr>
              
              <tr><td>(others)</td><td>(various)</td><td>All other namespace prefixes are used in examples only. <br/> In particular, IRIs starting with "http://example.com" represent<br/> some application-dependent IRI [[!IRI]]</td></tr>
        </div>
            </tbody></table>
      </section>
	  <section id="constraints">
	    <h3>Restrictions on SPARQL Query Language</h3>
		For simplicity, we use here a subset of the SPARQL Query language, with the aim to
		minimize ambiguity of semantics.
		Constraints are:
		<ol>
		  <li>The Sample aggregation operator is not allowed either explicitly or implicitly.</li>
		</ol>
	  </section>
	 </section>

    <section id="datatypes">
      <h2>Datatypes</h2>
        <p>In this section, we review the datatypes defined in other recommendations and
		state the definitions of new datatypes that support the signature definitions of new algebra operators.s</p>
      <section id="existing-datatypes">
        <h3>Existing Datatypes</h3>
		<p class="ednote>These definitions are being duplicated elsewhere in the document. For now, I keep this table
		as a quick reference for all existing datatypes that we depend on.
		</p>
        <p>We make use of the following datatypes from other specifications in the RDF and SPARQL family:</p>
        <div style="text-align: left;">
          <table class="thinborder" style="margin-left: auto; margin-right: auto;">
            <caption id="namespace-table"><a href="#namespace-table">Table 1</a>: Existing datatypes used in this specification</caption> <!-- Table 1-->
            <tbody>
			  <tr>
			    <td><b<>Datatype</b></td>
				<td><b>Reference</b></td> 
				<td><b>Definition</b>
				</td>
			  </tr>
              <tr>
			    <td><dfn>RDF-T</dfn></td>
			    <td><a href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/#sparqlBasicTerms">https://www.w3.org/TR/2013/REC-sparql11-query-20130321/#sparqlBasicTerms</a></td>
				<td> Let <dfn>I</dfn> be the set of all IRIs.
					Let <dfn>RDF-L</dfn> be the set of all RDF Literals
					Let <dfn>RDF-B</dfn> be the set of all blank nodes in RDF graphs.
					The set of RDF Terms, RDF-T, is I ∪ RDF-L ∪ RDF-B.
				</td>
			  </tr>              
			  <tr>
			    <td><dfn>V</dfn></td>
				<td><a href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/#sparqlQueryVariables">https://www.w3.org/TR/2013/REC-sparql11-query-20130321/#sparqlQueryVariables</a></td> 
				<td>A query variable is a member of the set V where V is infinite and disjoint from RDF-T.
				</td>
			  </tr>
              <tr>
			    <td><dfn>solution mapping</dfn></td>
			    <td><a href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/#sparqlSolutions">https://www.w3.org/TR/2013/REC-sparql11-query-20130321/#sparqlSolutions</a></td>
				<td> partial function μ : V -> RDF-T
				</td>
			  </tr>
              <tr>
			    <td><dfn>multiset of solution mappings</dfn></td>
				<td><a href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/#BasicGraphPattern">https://www.w3.org/TR/2013/REC-sparql11-query-20130321/#BasicGraphPattern</a></td>
				<td>A multiset is an unordered collection of elements in which each element may appear more than once. It is described by a set of elements and a cardinality function giving the number of occurrences of each element from the set in the multiset.
				</td>
			  </tr>
              <tr>
			    <td><dfn>sequence of solution mappings</dfn></td>
				<td><a href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/#BasicGraphPattern">https://www.w3.org/TR/2013/REC-sparql11-query-20130321/#BasicGraphPattern</a></td>
				<td>A sequence is a mapping (total function) on the positive integers. 	
				</td>
			  </tr>
			  <tr>
			    <td>RDF Datase</td>
				<td>[[!RDF11-Concepts]]</td> 
				<td><b>Definition</b>
				</td>
			  </tr>
			  <tr>
			    <td><dfn>expression</dfn></td>
				<td><b>Reference</b></td> 
				<td><b>Definition</b>
				</td>
			  </tr>
			  <tr>
			    <td><dfn>Graph Pattern</dfn></td>
				<td><b>Reference</b></td> 
				<td><b>Definition</b>
				</td>
			  </tr>              
			  <tr>
			    <td><dfn>Query</dfn></td>
				<td><b>Reference</b></td> 
				<td><b>Definition</b>
				</td>
			  </tr>
			  <tr>
			    <td><dfn>RDF Surface (sensu Pat Hayes)</dfn></td>
				<td><b>Reference</b></td> 
				<td><b>Definition</b>
				</td>
			  </tr>			  
			  <tr>
			    <td>D<sup>1</sup></td>
				<td><a href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/#sparqlQueryVariables">https://www.w3.org/TR/2013/REC-sparql11-query-20130321/#sparqlQueryVariables</a></td> 
				<td>A  is a member of the set V where V is infinite and disjoint from RDF-T.
				</td>
			  </tr>
			</tbody></table>
        </div>
		
		<p class="ednote">
		  In mathematics, the "cardinality" (typically called "multiplicity") mapping of multisets is generally assumed to range over non-negative integers.
		  The cardinality of a multiset is, typically, defined as the sum of the multiplicities of all its elements.
		</p>
		<p class="ednote">Note that not all sequences can be converted (by forgetting the order) to multisets with finite cardinality; 
				some sequences would correspond to multisets with countable cardinality (where cardinality of an element may be a non-negative integer or the
				cardinal number aleph-nought).
				All multisets with countable cardinality can be expressed as sequences.
				However, not all multisets with countable cardinality can be expressed as ordered sequences.
				SPARQL implicitly assumes sequences of solution mappings may be ordered, and hence we may suppose that
				the intention is to use only sequences with finite replication of any value, corresponding to
				multisets with finite cardinality when order is forgotten.
				</p>
      </section>
	<section id="def-datasets">
        <h3>RDF Datasets</h3>
		<section id="def-rdf-dataset">
		Recall the definition of RDF Dataset from [[!RDF11-Concepts]]:
		  <p>An <dfn data-lt="RDF Dataset|RDF Datasets|Dataset|Datasets">RDF dataset</dfn> is a collection of
      <a>RDF graphs</a>, and comprises:</p>

    <ul>
      <li>Exactly one <dfn>default graph</dfn>, being an <a>RDF graph</a>.
        The default graph does not have a name and MAY be empty.</li>
      <li>Zero or more <dfn def-lt="named graph|named graphs">named graphs</dfn>.
        Each named graph is a pair consisting of an IRI or a blank node
        (the <dfn>graph name</dfn>), and an <a>RDF graph</a>.
        Graph names are unique within an RDF dataset.</li>
    </ul>


		</section>
		<section id="def-kth-order-dataset">
          <h4>k-th order RDF Dataset</h4>
		  <p>A <dfn data-lt="k-th Order RDF Dataset|k-th Order RDF Datasets|k-th Order Dataset|k-th Order Datasets">k-th Order RDF Dataset</dfn> 
		  is defined inductively as follows.
          </p>
		  <p> A Zeroth Order RDF Dataset is an RDF Graph.
		  </p>
		  <p>For k = 0, 1, ... , a (k+1)-th Order RDF Dataset is composed of a distinguished k-th Order RDF Dataset, called the default dataset, 
		  and a set of <a>Named k-th Order RDF Datasets</a>.
		  The names of the k-th Order Datasets are unique within the k+1-th Order Dataset.		  
		  The set of k-th order dataset names of a k+1-th order dataset D is denoted by datasetNames(D).
		  </p>
		  <p class="ednote">
		  To be clarified in a separate semantics section:
		  just as there is not a single recommended semantics for RDF Datasets,
		  it is important to be inclusive regarding possible semantics for k-th Order RDF Datasets.
		  An important consideration is whether the component Named Datasets lie on a single
		  <a>RDF Surface (sensu Pat Hayes)</a>, i.e. do they share blank nodes.
		  </p>		  
		</section>
		<section id="def-named-kth-order-dataset">
          <h4>Named k-th order  RDF Dataset</h4>
		  <p>A <dfn data-lt="Named k-th Order RDF Dataset|Named k-th Order RDF Datasets|Named Dataset|Named Datasets|Named k-th Order Dataset|Named k-th Order Datasets"
		  >Named k-th Order RDF Dataset</dfn> 
		  is an ordered pair [n, D] of an IRI or blank node n 
		  that is the <a>k-th Order RDF Dataset</a> name, and <a>k-th Order RDF Dataset</a> D.
		  </p>	
		</section>
		</section>
		<section id="def-repositories">
          <h3>RDF Repositories</h3>
		<section id="def-kth-order-rdf-repository">
          <h4>k-th Order RDF Repositories</h4>
		  <p>A <dfn data-lt="k-th Order RDF Repository|k-th Order RDF Repositories|k-th Order Repository|k-th Order Repositories|RDF Repository|RDF Repositories|repository|repositories"
		  |k-th Order Repository|k-th Order Repositories
		  >k-th Order RDF Repository</dfn> is 
		  defined by cases:
		  </p>
		  <ol>
		  <li>a zeroth Order RDF Repository is an RDF Graph.
		  </li>
		  <li>k+1-th Order RDF Repository, k = 0, 1, ...,
		  is a k+1-th Order RDF Dataset with the additional structure of a total order 
		  on its component set of <a>Named k-th Order RDF Datasets</a>.
		  As such, the sequential nature of the structure of Named Datasets may be considered as a linear linked data structure,
		  as opposed to a mapping over integers.
		  This linked data structure may be finite (with unique initial and terminal elements),
		  semi-infinite (lacking either an initial or terminal element) or 
		  bi-infinite (lacking both initial or terminal elements).
		  Each <a>k-th order dataset</a> of the sequence is called a <dfn>version</dfn> of the RDF Repository.
		  Every version that is not the initial version has a unique previous version,
		  and we say that
		  each such version revises the previous version.
		  The set of version names of a repository R is denoted by versionNames(R).
		  </li>
		  </ol>
		  <p class="ednote">To be clarified in a separate semantics section: 
		  The intended semantics of a k-th Order RDF Repository
		  under the simple entailment regime is that of the underlying k-th Order RDF Dataset. 
		  That is,
		  the order among the named datasets is forgotten in simple entailment.
		  </p>
		  <p class="ednote">To be clarified in a separate semantics section: Further point on
		  the intended semantics is that an entailment regime, called 
		  e.g. "provenance entailment regime",
		  has additional entailments beyond those of the simple entailment regimes in that 
		  each non-initial version is related to its previous version by the 
		  <a href="https://www.w3.org/TR/2012/WD-prov-o-20120503/#wasRevisionOf">prov:wasRevisionOf</a>
		  property.
		  </p>	
		  <p class="ednote">A second-order <a>RDF Repository</a> is similar to a Graph Store, 
		  but with the additional structure
		  that every version is named (albeit the name may be a blank node).
		  </p>
		  <p class="ednote">
		  Where Update Operations are conceived as functional specifications for 
		  transitions of the state of a Graph Store,
		  for a RDF Repository these operations play the role of helper functions used in the "lazy" construction of the
		  RDF Repository.
		  </p>
		</section>
		<section id="def-named-kth-order-RDF Repository">
          <h4>Named k-th Order RDF Repository</h4>
		  <p>A <dfn data-lt="Named k-th Order RDF Repository|Named k-th Order RDF Repositories|Named RDF Repository|Named RDF Repositories|Named Repository|Named Repositories">Named k-th Order RDF Repository</dfn> is an ordered pair [n, R] of 
		  
		  an IRI or blank node n 
		  that is the <a>RDF Repository</a> name, and <a>k-th Order RDF Repository</a> R.
		  </p>
		  <p class="ednote">		  
		  Under the provenance entailment regime
		  a <a>named RDF repository</a> has additional entailments that 
		  each version
		  is related to the parent RDF Repository by the 
		  <a href="https://www.w3.org/TR/2012/WD-prov-o-20120503/#specializationOf">prov:specializationOf</a>
		  property.
          </p>		
		</section>
		</section>
		<section id="def-stores">
          <h3>Stores</h3>
		<section id="def-kth-order-store">
          <h4>k-th Order RDF Store</h4>
		  <p>A <dfn data-lt="k-th Order RDF Store|k-th Order RDF Stores|k-th Order Store|k-th Order Stores|RDF Store|RDF Stores|Store|Stores">k-th Order RDF Store</dfn>
		  is defined by cases:
		  </p>
		  <ol>
		  <li>A zeroth Order RDF Store is an RDF Graph.
		  </li>
		  <li>For k=0, 1, ..., a k+1-th Order RDF Store
		 is a k+1-th Order RDF Repository whose component k-th Order RDF Datasets have the additionl structure
		 of RDF Repositories.
		 The history of each member k-th Order RDF Repository is a restriction of the 
		 RDF Store history to that RDF Repository.
		 </li>
		 </ol>
		  <p class="ednote">We need to define a restriction of the history of a k+1-th Order RDF Repository
		  to a member k-th Order RDF Repository.
		  In particular, we need to take care to remove adjacent versions that have identical content.
		  </p>
		  <p class="ednote">The intended semantics, under any entailment regime,
		  of a k+1-th Order <a>RDF Store</a> is derived from 
		  that of the corresponding k+1-th Order RDF Repository,
		  supplemented by that of its member k-th Order Repositories,
		  possibly augmented (e.g. in the provenance regime) by entailments that link the two.
		  </p>
		</section>
		<section id="def-kth-order-rdf-recursive-store">
          <h4>k-th Order RDF Recursive Store</h4>
		  <p>A <dfn data-lt="k-th Order RDF Recursive Store|k-th Order RDF Recursive Stores|k-th Order Recursive Store|k-th Order Recursive Stores|RDF Recursive Store|RDF Recursive Stores|Recursive Store|Recursive Stores">k-th Order RDF Recursive Store</dfn>
		  is defined by cases:
		  </p>
		  <ol>
		  <li>A zeroth Order RDF Recursive Store is an RDF Graph.
		  </li>
		  <li>For k=0, 1, ..., a k+1-th Order RDF Recursive Store
		 is a k+1-th Order RDF Repository whose component k-th Order RDF Datasets have the additionl structure
		 of RDF Recursive Stores.
		 </li>
		 </ol>
		  <p class="ednote">The intended semantics, under any entailment regime,
		  of a k+1-th Order <a>RDF Recursive Store</a> is derived from 
		  that of the corresponding k+1-th Order RDF Store,
		  supplemented by that of its member k-th Order RDF Store,
		  possibly augmented (e.g. in the provenance regime) by entailments that link the two.
		  </p>
		</section>		  
		<section id="def-named-rdf-recursive-store">
          <h4>Named RDF Recursive Store</h4>
		  <p>A <dfn data-lt="Named RDF Recursive Store|Named RDF Recursive Stores">Named RDF Recursive Store</dfn> is an ordered pair [n, S] of an IRI n that is the 
		  <a>Store</a> name, and <a>RDF Recursive Store</a> S.
		  </p>	
		  <p class="ednote">We do need this type for the components of Recursive Stores.
		  Also at the top level, a Recursive Store, or even the non-recursive version, may be
		  named. The effect is that in the provenance entailment regime there are entailments of
		  the property prov:isSpecializationOf, which require the parent Store to have an identifier.
		  </p>
		</section>
		</section>
		<!--
		<section id="def-term-segment">
          <h4>Term Segment</h4>
		  <p>The <dfn data-lt="Term Segment|Term Segments">Term Segment</dfn> (of a Store) is the mutable (write-only) set of 
		       the RDF Terms known at a given time to appear 
				or have appeared in the Named Repositories of the Store 
				</a>.
		  </p>	
		  <p class="ednote">The Term Segment includes terms that might not currently appear in any statement in the current Revision of any 
				RDF Repository in the Store.
		  </p>
		  <p class="ednote">Do we need this term?it appears to be connected with the iterator over the history of a Repository,
		  which does not appear to be relevant for the abstract data model.
		  </p>		  
		</section>-->
		<!--
		<section id="def-term">
          <h4>The Term</h4>
		  <p>A <dfn data-lt="the term|the terms">term</dfn> is a (supertype) where (specialization)</a>.
		  </p>	
		  <p class="ednote">The intended semantics of a <a>the term</a> is (semantics)
		  </p>
		</section>
		<section id="def-property">
          <h4>the property</h4>
		  <p>A <dfn data-lt="the property|the properties">Store</dfn> (of a domain type) is a (range type) where (requirements)</a>.
		  </p>	
		  <p class="ednote">This property is a subproperty of @@@
		  </p>
		</section>		-->
		<div style="text-align: left;">
			  <table class="thinborder" style="margin-left: auto; margin-right: auto;">
            <caption id="namespace-table"><a href="#namespace-table">Table 2</a>: New datatypes considered for this specification but probably not needed</caption> <!-- Table 1-->
            <tbody>
			  <tr>
			    <td><b>Name</b></td>
				<td><b>Definition</b>
				</td>
			  </tr>
			  <!-- <tr>
			    <td>Store</td>
				<td>structure on a fixed RDF Surface (sensu Pat Hayes).
				This Surface contains all the RDF Terms that (ever) appear in the Term Segment. 
				The structure is as a set R of Named Repositories and a default RDF Repository. 
				Every Store has an open world assumption in that at any point in time we may not know all the 
				Repositories in it and all the Terms in it.
				</td>
			  </tr>              
			  <tr>
			    <td>Named RDF Recursive Store</td>
				<td>ordered pair [n, S] of an IRI n that is the Store name, and Store S.
				</td>
			  </tr>   -->           
			  <tr>
			    <td>Term Segment (of a Store)</td>
				<td>mutable (write-only) set of the RDF Terms known at a given time to appear 
				or have appeared in the Named Repositories of the Store 
				(i.e includes terms that might not currently appear in any statement in the current Revision of any 
				RDF Repository currently in the Store)
				</td>
			  </tr>      
			  <tr>
			    <td>version (of a RDF Repository)</td>
				<td>relationship between the RDF Repository and each of the elements in its sequence of Named RDF Datsets.
                    A subproperty of prov:specializationOf. The "versionOf" relationship can be inferred from a prov:specializationOf, 
					together with participation in a
                    prov:wasRevisionOf relationship with another specialization of the same RDF Repository.
                    Therefore it is not necessary to mint this subproperty. 					
				</td>
			  </tr>              
			  <tr>
			    <td>RDF Databag</td>
				<td>structure consisting of an RDF Dataset D and a cardinality mapping
                     from the set of triples and quads in D into positive integers.
                     For an RDF Databag B, the dataset is data[B] and the cardinality mapping is card[B].					 
				</td>
			  </tr>
			  <tr>
			    <td>Named RDF Databag</td>
				<td>ordered pair [n, B] of an IRI 
				that is the name of the RDF Databag, and RDF Databag B.
				</td>
			  </tr>
		      <tr>
			    <td>Multirepository</td>
				<td>like a Repository, but with RDF Databags instead of RDF Datasets.
				 (move to a note) Explanation: The purpose of the additional types which incorporate cardinality is to 
				accommodate a representation of an arbitrary RDF Stream with a "cumulative" strategy 
				(just making this name up on the spot). This strategy is what I am using in the current experimental repository, 
				 where each new timestamped graph of the stream is inserted into the repository that holds its representation, 
				 and never deleted. 
				 The cardinality is needed for the case when the same triple, or quad, appears in more than one element of 
				 the stream, which is allowed in the most general RDF stream. This strategy was the best match to the 
				 visibility mapping where the default is persistence (if no value is given for run length, then it is 
				 assumed to be present from then on, and never deleted).
				 
				 An alternative strategy would be to use a  "noncumulataive" strategy, where the repository is "cleared" 
				 (without deleting the history) when a new stream element arrives, and the new stream element becomes the 
				 repository content. 
				 The visibility map that is the best match to this case is where the default is a runlength of 1. 
				 This strategy would not need the additional types with cardinality, making the abstract data model more parsimonious.
				
				</td>
			  </tr>              
			  <tr>
			    <td>Named Multirepository</td>
				<td>ordered pair [n, M] of an IRI n that is the Multirepository name, and Multirepository M.
				</td>
			  </tr>
			  <tr>
			    <td>Multistore</td>
				<td>like a Store, but with Multirepositories instead of Repositories.
				</td>
			  </tr>
			</tbody></table>
        </div>
      </section>
	</section>

	<section id="algebra-syntax">
		<h2>Abstract Algebra Syntax</h2>
		<section id="def-existing-basic-dataset-union">
          <h4>Dataset-UNION</h4>
		  <p><dfn>Dataset-UNION</dfn> is defined in [[!sparql11-update]] as a basic operation with the following signature:</p>
		  <ul>
		    <li>On the domain RDF Dataset x RDF Dataset, the range is RDF Dataset.</li>
		  </ul>
		  <p>The semantics of <a>Dataset-UNION</a> is defined in [[!sparql11-update]] as follows.</p> 
		  <p>
		    Let DS={DG} union {(iri<sub>i</sub>, G<sub>i</sub>) | 1 ≤ i ≤ n} and DS' = {DG'} union {(iri'<sub>j</sub>, G'<sub>j</sub>) | 1 ≤ j ≤ m} be two RDF Datasets. Let further graphNames(DS) = { iri<sub>i</sub> | 1 ≤ i ≤ n} and graphNames(DS') = {iri'<sub>j</sub> | 1 ≤ j ≤ m}. The Dataset-UNION between DS and DS' is defined as follows:
	      </p>
	      <p>Dataset-UNION(DS, DS') = {DG union DG'} union {(iri, G) | iri in graphNames(DS) union graphNames(DS')}</p>
		  <p>and G defined as </p>
		  <ul>
			<li>G<sub>i</sub> for iri = iri<sub>i</sub> such that iri<sub>i</sub> in graphNames(DS) minus graphNames(DS')</li>
			<li>G<sub>j</sub> for iri = iri'<sub>j</sub> such that iri<sub>j</sub> in graphNames(DS') minus graphNames(DS)</li>
			<li>G<sub>i</sub> union G<sub>j</sub> for iri = iri<sub>i</sub> = iri'<sub>j</sub> in graphNames(DS) intersect graphNames(DS')</li>
		  </ul>
		  <p>where union between graphs is defined as set-union of triples in those graphs.</p>
		  <p>The Dataset-UNION operation constructs a new RDF Dataset from two existing RDF Datasets by
		  forming the union of the default graphs, and taking the union of the sets of named graphs, with the exception
          that if the sets of graph names of the two datasets have a non-empty intersection, the graphs with names
          in that intersection must be combined by set union. 		  	  
		  </p>
		  <p>
		  We extend the Dataset-UNION operation to <a>k-th Order RDF Datasets</a>.
		  Also, the names are extended from IRIs to also allow blank node names.
		  </p>
		  <ul>
		    <li>On the domain <a>k-th Order RDF Dataset</a> x k'-th Order RDF Dataset, the range is max(k, k')-th Order RDF Dataset.</li>
		  </ul>
		  <p>The extended semantics of <a>Dataset-UNION</a> is defined by cases as follows.</p>
          <p>In the case that both arguments are zeroth Order RDF Datasets, i.e., RDF Graphs, then the result is the union
		     of the sets of triples in the two graphs.
          </p>		  
		  <p>In the case that both arguments, DSP and DSP', are k+1-th Order RDF Datasets, 
		  let DSP = {DSC} ∪ 
		  {(name<sub>i</sub>, DSC<sub>i</sub>) | 1 ≤ i ≤ n} and 
		  datasetNames(DSP) = { name<sub>i</sub> | 1 ≤ i ≤ n}, and similarly for DSP',
		  where DSC and DSC<sub>i</sub> are <a>k-th Order RDF Datasets</a>, and
		  name and name<sub>i</sub> are IRIs or blank nodes (I ∪ RDF-B).
		  Then</p>
          <p>
		    Dataset-UNION(DSP, DSP') = 
           Dataset-UNION(DSC , DSC') ∪ {(iri, DS) | iri in datasetNames(DSP) ∪ 
		   datasetNames(DSP')}</p>
		  <p>and DS defined as </p>
		  <ul>
			<li>DSC<sub>i</sub> for iri = iri<sub>i</sub> such that iri<sub>i</sub> in datasetNames(DSP) minus datasetNames(DSP')</li>
			<li>DSC'<sub>j</sub> for iri = iri'<sub>j</sub> such that iri<sub>j</sub> in datasetNames(DSP') minus datasetNames(DSP)</li>
			<li>Dataset-UNION(DSC<sub>i</sub> , DSC'<sub>j</sub>) for iri = iri<sub>i</sub> = iri'<sub>j</sub> in 
			    datasetNames(DSP) intersect datasetNames(DSP')</li>
		  </ul>

		  <p>In the case that k and k' are different, let k'' = max(k, k').
		  Then Dataset-UNION(DS, DS') = Dataset-UNION(Dataset-ELEVATE(DS, k''), Dataset-ELEVATE(DS', k'')).
		  </p>
		  <p class="ednote">Define the basic operation Dataset-ELEVATE to lift a k'-th order dataset to a k-th order dataset, 
		  with k' &le; k.
		  In the case that k' &lt; k, then it will be embedded in the default dataset.
		  The definition can be recursive, with base case of k = k' + 1, and the input dataset is the default dataset of the output, and the
		  output contains no named datasets. 
		  When k=k', then the operation returns the input dataset.
		  </p>
		  <p class="ednote">
		    It is possible to informally apply the Dataset-UNION operation to pairs of <a>repositories</a> or <a>stores</a> by first applying
			a forgetful functor that retains all the versions but forgets the order among them.
			However, the result is not a <a>repository</a> or <a>store</a>, but is just a dataset.
            To truly extend the UNION operation to <a>repositories</a> and <a>stores</a> to generate new  <a>repositories</a> and <a>stores</a>,
            it would be necessary to mint new version names and to specify how versions are ordered and combined when necessary
			e.g. according to timestamps, but timestamps are not mandatory in our definitions 
            There is no precedent for such definitions, since the Update spec does not define a union operation on Graph Stores.			
		  </p>
		  </section>
		  <section>
		  <h4>Other Basic Operations</h4>
		  <p class="ednote">Similarly, the basic operations of Dataset-DIFF and Dataset can be extended to k-th order datasets.
		  However, when working with the Dataset operation that takes a pattern as an argument, it is necessary to extend
		  the Graph Pattern syntax to enable the specifications of the multiple names (up to k of them) needed to identify
          some triples in a k-th order dataset. 		  
		  </p>
		  </section>
		  <section id="def-existing-update-insert-data">
          <h4>Insert Data</h4>
		  <p><dfn>Insert Data</dfn> is defined in [[!sparql11-update]] as an Update Operation with the following signature:</p>
		  <ul>
		    <li>On the domain RDF Dataset x IRI, the range is RDF Dataset.</li>
		  </ul>
		  <p>The semantics of <a>Insert Data</a> is defined in [[!sparql11-update]] as follows.</p> 
		  <p>OpInsertData(GS, <em>QuadPattern</em>) = Dataset-UNION(GS, Dataset(<em>QuadPattern</em>,{},GS,GS))</p>
		  <p>where {} is the empty solution mapping,</p>
		  <p>
		  GS is an RDF dataset, and <em>QuadPattern</em> is a ground quad pattern.
		  </p>
		  <p>The Insert Data operation constructs a new RDF Dataset from an existing RDF Dataset by adding 
		  triples and quads to it as expressed in the ground <em>QuadPattern</em>.
		  Note that Insert Data is functional in that it does not actually "change" the input RDF Dataset.
          Update operations may be used to define a Graph Store as a state machine, where the state of a Graph Store
          is an RDF Dataset, and Update Operations take the current state of a Graph Store as one of their input arguments,
          generating a new RDF Dataset which then is taken as the new state of the Graph Store.		  
		  </p>
		  <p>
		  We extend the Insert Data operation to <a>k-th Order RDF Datasets</a>.
		  </p>
		  <ul>
		    <li>On the domain <a>k-th Order RDF Datasets</a> x Ground <a>k-th Order Patterns</a>, the range is <a>k-th Order RDF Datasets</a>.</li>
		  </ul>
		  <p>The extended semantics of <a>Insert Data</a> is defined as follows.</p> 
		  <p>OpInsertData(DS, <em>k-Pattern</em>) = Dataset-UNION(DS, Dataset(<em>k-Pattern</em>,{},DS,DS))</p>
		  <p>where {} is the empty solution mapping,</p>
		  </section>
		<section id="def-existing-update-opcreate">
          <h4>OpCreate</h4>
		  <p><dfn>OpCreate</dfn> is defined in [[!sparql11-update]] as an Update Operation with the following signature:</p>
		  <ul>
		    <li>On the domain RDF Dataset x IRI, the range is RDF Dataset.</li>
		  </ul>
		  <p>The semantics of <a>OpCreate</a> is defined in [[!sparql11-update]] as follows.</p> 
		  <p>Let GS = {DG} union {(iri<sub>i</sub>, G<sub>i</sub>) | 1 ≤ i ≤ n} and graphNames(GS) = { iri<sub>i</sub> | 1 ≤ i ≤ n}, then</p>
		  <p>OpCreate(GS, iri) = GS union {(iri, {})} if iri not in graphNames(GS); 
		  otherwise, OpCreate(GS, iri) = GS</p>
		  <p>
		  where DG and G<sub>i</sub> are graphs, iri and iri<sub>i</sub> are IRIs.
		  </p>
		  <p>The OpCreate operation constructs a new RDF Dataset from an existing RDF Dataset by adding 
		  an empty named graph to it with a specified IRI as name, 
		  provided it does not already contain a named graph of the specified name.
          Note that OpCreate is functional in that it does not actually "change" the input RDF Dataset.
          Update operations may be used to define a Graph Store as a state machine, where the state of a Graph Store
          is an RDF Dataset, and Update Operations take the current state of a Graph Store as input,
          generating a new RDF Dataset which then is taken as the new state of the Graph Store.		  
		  </p>
		  <p>
		  We extend the OpCreate operation to <a>k-th Order RDF Datasets</a>.
		  Also, the names are extended from IRIs to also allow blank node names.
		  </p>
		  <ul>
		    <li>On the domain <a>k-th Order RDF Dataset</a> x I ∪ RDF-B, the range is <a>k-th Order RDF Dataset</a>.</li>
		  </ul>
		  <p>The extended semantics of <a>OpCreate</a> is defined as follows.</p> 
		  <p>Let DSP = {DSC} ∪ 
		  {(name<sub>i</sub>, DSC<sub>i</sub>) | 1 ≤ i ≤ n} and 
		  datasetNames(DSP) = { name<sub>i</sub> | 1 ≤ i ≤ n}, then</p>
		  <p>OpCreate(DSP, name) = DSP ∪ {(name, {})} 
		  if name not in datasetNames(DSP); 
		  otherwise, OpCreate(DSP, name) = DSP</p>
		  <p>
		  where DSC and DSC are <a>k-th Order RDF Datasets</a>, and
		  name and name<sub>i</sub> are IRIs or blank nodes (I ∪ RDF-B).
		  </p>
		  </section>
		  <section id="def-new-apply-operation">
		  <h4>Apply Operation</h4>
		  <p class="ednote">The above extensions only allow management, e.g. creation of a new "slot", at the top level of
		  a k-th order dataset. However, it is also useful to be able to apply operations at a deeper level within
		  a k-th order dataset. This capability can be enabled with a generic operation, called <dfn>Apply</dfn>, which takes
		  a k-th order dataset as first argument,
		  an Operation specification (Operation name and tuple of arguments with a placehoder in one position) as second argument, 
		  and an optional third argument that is an IRI or blank node.
		  The result would be the result of applying that Operation with the appropriate (named or default) (k-1)-th order dataset 
		  replacing the placeholder,
		  and then substituting for the (k-1)-th order dataset with the operation result within the original k-th order dataset. 
		  If the name is not in the set of
		  database names, then the result would be the original dataset.
		  </p>
		  </section>
		  <section>
		  <h4>Other Update Operations</h4>
		  <p class="ednote">Similarly, the Update Operations of SPARQL Update can be extended to k-th order datasets,
		  making use of the extended basic operations.
		  </p>
		  </section>		  
		  <section id="def-new-revise-operation">
		  <h4>Revise Operation</h4>
		  <p>Update Operations as defined in SPARQL Update, and here extended, are not directly applicable to <a>repositories</a> 
		  and <a>stores</a>.
		  In order to enable this, we define a Revise operation, 
		  that takes as input a <a>repository</a> (with a terminal version) as first argument,
		  an Update Operation specification as second argument and a name (IRI or blank node) as third argument.
		  The result is a new <a>repository</a> that has an additional version, that being the result of applying
		  the Update Operation to the terminal version of the input repository, and making that the new terminal
		  version, using the specified name as the version name.
		  </p>
		  <p>The Revise Operation may be extended to <a>stores</a> in the obvious fashion. However, this 
		  only allows manipulation of the version sequence of the store.
		  To enable direct manipulation of the version sequence of repositories within the store, we extend
		  the <a>Apply</a> Operation, defined above for <a>k-th order datasets</a> and Update Operation specifications, 
		  to <a>stores</a> and Revise Operation specifications on repositories (only for unnested Revise).
		  </p>
		  <p>The Revise Operation may be further extended to <a>recursive stores</a> and nested Revise Operation Specifications.
		  </p>
		  <p>Application of a Revise operation to a structure without a terminal version is an error.
		  </p>
		  </section>		  
		  <!--
		  <section>
		  <p class="ednote">Options for extending Update Operations to the Store/Repository model are:
		  </p>
		  <ol>
		    <li>Follow the functional approach by defining functions that act on the "state" of a Store, 
			(defined in terms of the Repositories, named or default, that it owns), 
			producing a new state.
			This state then presumably becomes the current state of the mutable Store, just like in the SPARQL UPDATE spec,
			but the mutability of a Store is not addressed explicitly.
			However, the mutability of Repositories is explicitly addressed, 
			these being the objects that make up the state of a Store.
			</li>
			<li>Handle the mutability of Stores explicitly by defining mutator methods on Store "objects".
			</li>
			<li>Take a full-blown functional approach by hiding all mutability.
			The "state" of a Store is then not expressed in terms of Repositories, but in terms of the "states"
			of its Repositories.
			The state of a Repository is an RDF Dataset, in parallel to the state of a Graph Store being an RDF graph.
			</li>
		  </ol>
		  
		  <p>Here is what the first option would look like:
          </p>
		  <p>
		  We further extend the OpCreate operation to <a>k-th Order Stores</a>.
		  </p>
		  <ul>
		    <li>On the domain <a>k-th Order Store</a> x I ∪ RDF-B, the range is <a>k-th Order Store</a>.</li>
		  </ul>
		  <p>The extended semantics of <a>OpCreate</a> is defined as follows.</p> 
		  <p>Let S<sup>k</sup> = {R<sup>k</sup>} ∪ 
		  {(name<sub>i</sub>, R<sup>k</sup><sub>i</sub>) | 1 ≤ i ≤ n} and 
		  datasetNames(R<sup>k</sup>) = { name<sub>i</sub> | 1 ≤ i ≤ n}, then</p>
		  <p>OpCreate(S<sup>k</sup>, name) = S<sup>k</sup> ∪ {(name, R<sup>k</sup><sub>0</sub>)} 
		  if name not in datasetNames(S<sup>k</sup>); 
		  otherwise, OpCreate(S<sup>k</sup>, name) = S<sup>k</sup></p>
		  <p>
		  where S<sup>k</sup> is a <a>k-th Order Store</a>, 
		  R<sup>k</sup><sub>i</sub> are <a>k-th Order Repositories</a>, 
		  name and name<sub>i</sub> are IRIs or blank nodes (I ∪ RDF-B)
		  and  R<sup>k</sup><sub>0</sub> is the empty <a>k-th Order Repository</a>.
		  </p>
		  -->
          <!--
		  <p>We extend the syntax of OpCreate to include the following additional signature:</p>
		  <ul>
		    <li>On the domain StoreState x IRI, the range is StoreState.</li>
		  </ul>
		  <p>The semantics of <a>OpCreate</a> on the domain extension is defined as follows.</p> 
		  <p>Let SS = {DR} union {NR<sub>i</sub> | 1 ≤ i ≤ n} and repositoryNames(SS) = { name(NR<sub>i</sub>) | 1 ≤ i ≤ n}, then</p>
		  <p>OpCreate(SS, iri) = SS union { NamedRepository(iri, R0)} if iri not in repositoryNames(SS); 
		  otherwise, OpCreate(SS, iri) = SS</p>
		  <p>
		  where DR is a repository, NR<sub>i</sub> are named repositories, iri is an IRI, and 
		  R0 is the "empty repository".
		  </p>		  
		  <p>Here is what the second option would look like:
          </p>
		  <p>
		  The corresponding "create" operation for <a>Stores</a> is a mutator operation on the mutable Store object.
		  </p>
		  <p><dfn>Store.create(self, iri)</dfn> has domain Store x IRI and is void.
		  </p>
		  <p class="ednote">The semantics of Store.create is to have the side-effect of 
             constructing a new Named Repository object with name iri whose initial content is empty, 
			 that is owned by the Store self.		  
		  </p>
		  <p class="ednote">We must defined the meaning of the "initial content of a (Named) Repository is empty" somewhere,
		  probably as a note to the definition of (Named) Repository.
		  </p>
		  <p class="ednote">Is "owned" the property term to use for the relationship between a Named Repository and
		  its parent Store?
		  </p>
		  <p>Here is what the third option would look like:
          </p>
		  <p>We extend the syntax of OpCreate to include the following additional signature:</p>
		  <ul>
		    <li>On the domain StoreState x IRI, the range is StoreState.</li>
		  </ul>
		  <p>The semantics of <a>OpCreate</a> on the domain extension is defined as follows.</p> 
		  <p>Let SS = {DD} union {(iri<sub>i</sub>, D<sub>i</sub>) | 1 ≤ i ≤ n} and 
		  repositoryNames(SS) = { iri<sub>i</sub> | 1 ≤ i ≤ n}, then</p>
		  <p>OpCreate(SS, iri) = SS union { (iri, {})} if iri not in repositoryNames(SS); 
		  otherwise, OpCreate(SS, iri) = SS</p>
		  <p>
		  where DD and D<sub>i</sub> are RDF Datasets, iri and iri<sub>i</sub> are IRIs.
		  </p>
         </section>
		-->		  
		<!--
		<section id="def-existing-update-operator">
          <h4>Operator</h4>
		  <p><dfn>Operator</dfn> is defined in [[!sparql11-update]] as an Update Operation with the following signatures:</p>
		  <ul>
		    <li>On the domain X x Y, the range is Z.</li>
			<li>On the domain X x U, the range is V.</li>
		  </ul>	
		  <p class="ednote">The semantics of <a>Operator</a> is defined in [[!sparql11-update]] to be (semantics)
		  </p>
    	</section>
		<section id="def-extension-of-existing-update-operator">
          <h4>Operator</h4>
		  <p><dfn>Operator</dfn> is defined in [[!sparql11-update]] as a (supertype) with the following signatures:</p>
		  <ul>
		    <li>On the domain X x Y, the range is Z.</li>
			<li>On the domain X x U, the range is V.</li>
		  </ul>
		  <p>We extend the syntax of Operator to include the following additional signatures:</p>
		  <ul>
		    <li>On the domain X x Y, the range is Z.</li>
			<li>On the domain X x U, the range is V.</li>
		  </ul>
		  <p class="ednote">The semantics of <a>Operator</a> is defined in [[!sparql11-update]] to be (semantics)
		  </p>
		  <p class="ednote">The intended semantics of <a>Operator</a> on the extended signature is (semantics)
		  </p>
		</section>
		<section id="def-new-operator">
          <h4>Operator</h4>
		  <p>A <dfn>Operator</dfn> is a (supertype) where (specialization)</a>.
		  </p>	
		  <p class="ednote">The intended semantics of <a>Operator</a> is (semantics)
		  </p>
		</section>
		-->
	</section>
	<section>
	<h2>Examples</h2>
	<section>
	<h3>FHIR RDF Stream Example</h3>
	<aside  class="example" id="fhir-graph" title="FHIR Statements as RDF Graph">
	The following is a representation of an RDF graph in Trig.
	These statements have a dynamic aspect
	that is implicitly expressed in timestamps
	of the events of type  fhir:Observations.
	Although the statements appear in a particular order within the graph
	representation, this order is not significant to inference.
	In particualr, the temporal order is not directly available through simple entailment.
	We will show how this same data could be handled within some 
	of the new datatypes so that the dynamic
	nature of the statements
	is accessible through simple entailment.
	<p>The following graph is denoted G<sub>FHIR</sub>.</p>
	<pre>
@prefix fhir: &lt;http://hl7.org/fhir/&gt; .
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix obs: &lt;http://hl7.org/fhir/Observation.&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix sct: &lt;http://snomed.info/id/&gt; .

_:b1 a fhir:Observation ;
		obs:code sct:36407505 ;
		obs:device lr:sensor1234 ;
		obs:effectiveDateTime "2015-01-01T12:00:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		obs:subject lr:patientA ;
		obs:valueQuantity _:b2 .
	
_:b2 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		lr:value "50"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .

_:b4 a fhir:Observation ;
		obs:code sct:36407505 ;
		obs:device lr:sensor1234 ;
		obs:effectiveDateTime "2015-01-01T12:01:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		obs:subject lr:patientA ;
		obs:valueQuantity _:b5 .
	
_:b5 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		lr:value "53"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .

_:b7 a fhir:Observation ;
		obs:code sct:36407505 ;
		obs:device lr:sensor1234 ;
		obs:effectiveDateTime "2015-01-01T12:02:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		obs:subject lr:patientA ;
		obs:valueQuantity _:b8 .
	
_:b8 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		lr:value "48"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
</pre>
	</aside>
	<aside  class="example" id="fhir-stream" title="FHIR Statements as RDF Stream">
	An RDF Stream is a sequence of RDF Datasets (possibly graphs) satisfying certain additional constraints.
	<p>The RDF Stream as a whole is denoted S<sub>FHIR</sub>.</p>
	<p>A representation of the first element of the stream is as follows.
	The first triple in each document is the timestamp triple.
	This graph is denoted G<sub>1, FHIR</sub>.
	</p>
	<pre>
@prefix fhir: &lt;http://hl7.org/fhir/&gt; .
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix obs: &lt;http://hl7.org/fhir/Observation.&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix sct: &lt;http://snomed.info/id/&gt; .

_:b1 a fhir:Observation ;
		obs:effectiveDateTime "2015-01-01T12:00:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		obs:code sct:36407505 ;
		obs:device lr:sensor1234 ;
		obs:subject lr:patientA ;
		obs:valueQuantity _:b2 .
	
_:b2 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		lr:value "50"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
</pre>
	<p>A representation of the second element of the stream is as follows.
		This graph is denoted G<sub>2, FHIR</sub>.
</p>
	<pre>
@prefix fhir: &lt;http://hl7.org/fhir/&gt; .
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix obs: &lt;http://hl7.org/fhir/Observation.&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix sct: &lt;http://snomed.info/id/&gt; .


_:b4 a fhir:Observation ;
		obs:effectiveDateTime "2015-01-01T12:01:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		obs:code sct:36407505 ;
		obs:device lr:sensor1234 ;
		obs:subject lr:patientA ;
		obs:valueQuantity _:b5 .
	
_:b5 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		lr:value "53"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
</pre>
	<p>A representation of the third element of the stream is as follows.
		This graph is denoted G<sub>3, FHIR</sub>.
</p>
	<pre>
@prefix fhir: &lt;http://hl7.org/fhir/&gt; .
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix obs: &lt;http://hl7.org/fhir/Observation.&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix sct: &lt;http://snomed.info/id/&gt; .

_:b7 a fhir:Observation ;
		obs:effectiveDateTime "2015-01-01T12:02:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		obs:code sct:36407505 ;
		obs:device lr:sensor1234 ;
		obs:subject lr:patientA ;
		obs:valueQuantity _:b8 .
	
_:b8 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		lr:value "48"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
</pre>
One possible way that this RDF Stream would be transmitted is as a sequence of RDF documents, each one containing
an RDF graph corresponding to one element of the stream.
	</aside>
	<aside  class="example" id="fhir-dataset" title="FHIR Statements as RDF Dataset">
	This information can also be represented using the <a>RDF Dataset</a> datatype.
	The named graph structure allows the statements of each stream element to be grouped together,
	and also allows us to materialize, in the default graph, 
	statements giving the order among these elements.	
		This RDF dataset is denoted D<sub>FHIR</sub>.
<pre>
@prefix fhir: &lt;http://hl7.org/fhir/&gt; .
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix obs: &lt;http://hl7.org/fhir/Observation.&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix sct: &lt;http://snomed.info/id/&gt; .

_:b102 prov:wasRevisionOf _:b101.
_:b103 prov:wasRevisionOf _:b102.
_"b101{
	    _:b1 a fhir:Observation ;
		    obs:code sct:36407505 ;
		    obs:device lr:sensor1234 ;
		    obs:effectiveDateTime "2015-01-01T12:00:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		    obs:subject lr:patientA ;
		    obs:valueQuantity _:b2 .
	
	    _:b2 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		    lr:value "50"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
}	

_:b102{
		_:b4 a fhir:Observation ;
			obs:code sct:36407505 ;
			obs:device lr:sensor1234 ;
			obs:effectiveDateTime "2015-01-01T12:01:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
			obs:subject lr:patientA ;
			obs:valueQuantity _:b5 .
	
		_:b5 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
			lr:value "53"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
}
_:b103{
		_:b7 a fhir:Observation ;
			obs:code sct:36407505 ;
			obs:device lr:sensor1234 ;
			obs:effectiveDateTime "2015-01-01T12:02:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
			obs:subject lr:patientA ;
			obs:valueQuantity _:b8 .
	
		_:b8 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
			lr:value "48"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
}
</pre>
	</aside>
	<aside  class="example" id="fhir-repository" title="FHIR Statements as a Repository">
	The dataset above is a static structure representing the entailment,
	under the provenance regime,
	of a first-order <a>Repository</a>.
	This repository is denoted R<sub>FHIR</sub>.
	Since repositories are dynamic entities, one form the transmission of the repository could
	take is a sequence of documents, as shown below.
	In this example, the default dataset of the repository is empty.
 	<p>The first document of the sequence, representing a dataset, is as follows.
	The dataset is denoted D<sub>1, FHIR</sub>.
	</p>
	<pre>
@prefix fhir: &lt;http://hl7.org/fhir/&gt; .
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix obs: &lt;http://hl7.org/fhir/Observation.&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix sct: &lt;http://snomed.info/id/&gt; .

_:b101{
	    _:b1 a fhir:Observation ;
		    obs:code sct:36407505 ;
		    obs:device lr:sensor1234 ;
		    obs:effectiveDateTime "2015-01-01T12:00:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		    obs:subject lr:patientA ;
		    obs:valueQuantity _:b2 .
	
	    _:b2 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		    lr:value "50"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
}	
</pre>
 	<p>The second document of the sequence is as follows.
		The dataset is denoted D<sub>2, FHIR</sub>.
</p>
	<pre>
@prefix fhir: &lt;http://hl7.org/fhir/&gt; .
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix obs: &lt;http://hl7.org/fhir/Observation.&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix sct: &lt;http://snomed.info/id/&gt; .

_:b102{
		_:b4 a fhir:Observation ;
			obs:code sct:36407505 ;
			obs:device lr:sensor1234 ;
			obs:effectiveDateTime "2015-01-01T12:01:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
			obs:subject lr:patientA ;
			obs:valueQuantity _:b5 .
	
		_:b5 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
			lr:value "53"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
}
</pre>
 	<p>The third document of the sequence is as follows.
		The dataset is denoted D<sub>3, FHIR</sub>.
</p>
	<pre>
@prefix fhir: &lt;http://hl7.org/fhir/&gt; .
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix obs: &lt;http://hl7.org/fhir/Observation.&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix sct: &lt;http://snomed.info/id/&gt; .

_:b103{
		_:b7 a fhir:Observation ;
			obs:code sct:36407505 ;
			obs:device lr:sensor1234 ;
			obs:effectiveDateTime "2015-01-01T12:02:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
			obs:subject lr:patientA ;
			obs:valueQuantity _:b8 .
	
		_:b8 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
			lr:value "48"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
}
</pre>
	</aside>
	</section>
	<section>
	<h3>General RDF Stream Example</h3>
	<aside  class="example" id="prov-dataset" title="FHIR Provenance in an RDF Dataset">
	We consider a more general RDF Stream example, showing a typically
	case when sets of statements have multiple temporal characteristics.
	In this example, we have the time of observation as one temporal
	characteristics, and the time the observation report was generated
	as the second temporal characteristic,
	The following is a representation of this information as an RDF dataset in Trig.
	The dataset is denoted D<sub>prov</sub>.
	In this representation, the dytnamic aspects are implicitly expressed 
	in timestamps,
	but the temporal order is not directly available through simple entailment.
	<pre>
@prefix fhir: &lt;http://hl7.org/fhir/&gt; .
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix obs: &lt;http://hl7.org/fhir/Observation.&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix sct: &lt;http://snomed.info/id/&gt; .

_:b0 
	prov:generatedAtTime "2015-01-01T12:00:33Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
    lr:stream lr:streamA12342015-01-01T12:00:00Zm .
_:b0 {
	_:b1 a fhir:Observation ;
		obs:code sct:36407505 ;
		obs:device lr:sensor1234 ;
		obs:effectiveDateTime "2015-01-01T12:00:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		obs:subject lr:patientA ;
		obs:valueQuantity _:b2 .
	
	_:b2 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		lr:value "50"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
}

_:b3 
	prov:generatedAtTime "2015-01-01T12:01:45Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
    lr:stream lr:streamA12342015-01-01T12:00:00Z .
_:b3 {
	_:b4 a fhir:Observation ;
		obs:code sct:36407505 ;
		obs:device lr:sensor1234 ;
		obs:effectiveDateTime "2015-01-01T12:01:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		obs:subject lr:patientA ;
		obs:valueQuantity _:b5 .
	
	_:b5 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		lr:value "53"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
}

_:b6 
	prov:generatedAtTime "2015-01-01T12:02:54Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
    lr:stream lr:streamA12342015-01-01T12:00:00Z .
_:b6 {
	_:b7 a fhir:Observation ;
		obs:code sct:36407505 ;
		obs:device lr:sensor1234 ;
		obs:effectiveDateTime "2015-01-01T12:02:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		obs:subject lr:patientA ;
		obs:valueQuantity _:b8 .
	
	_:b8 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		lr:value "48"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
}
</pre>
	</aside>
	<aside  class="example" id="prov-stream" title="FHIR Provenance in an RDF Stream">
	The representation as an RDF Stream takes the form of the following sequence.
	The stream as a whole is denoted S<sub>prov</sub>.
	<p>A representation of the first element of the stream in an RDF dataset as follows.
	The dataset is denoted D<sub>1, prov</sub>.
	The first triple in each document is the timestamp triple.
	</p>
	<pre>
@prefix fhir: &lt;http://hl7.org/fhir/&gt; .
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix obs: &lt;http://hl7.org/fhir/Observation.&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix sct: &lt;http://snomed.info/id/&gt; .

_:b0 
	prov:generatedAtTime "2015-01-01T12:00:33Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
    lr:stream lr:streamA12342015-01-01T12:00:00Zm .
_:b0 {
	_:b1 a fhir:Observation ;
		obs:code sct:36407505 ;
		obs:device lr:sensor1234 ;
		obs:effectiveDateTime "2015-01-01T12:00:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		obs:subject lr:patientA ;
		obs:valueQuantity _:b2 .
	
	_:b2 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		lr:value "50"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
}
</pre>
	<p>A representation of the second element of the stream is as follows.
		The dataset is denoted D<sub>2, prov</sub>.
</p>
	<pre>
@prefix fhir: &lt;http://hl7.org/fhir/&gt; .
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix obs: &lt;http://hl7.org/fhir/Observation.&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix sct: &lt;http://snomed.info/id/&gt; .


_:b3 
	prov:generatedAtTime "2015-01-01T12:01:45Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
    lr:stream lr:streamA12342015-01-01T12:00:00Z .
_:b3 {
	_:b4 a fhir:Observation ;
		obs:code sct:36407505 ;
		obs:device lr:sensor1234 ;
		obs:effectiveDateTime "2015-01-01T12:01:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		obs:subject lr:patientA ;
		obs:valueQuantity _:b5 .
	
	_:b5 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		lr:value "53"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
}
</pre>
	<p>A representation of the third element of the stream is as follows.
		The dataset is denoted D<sub>3, prov</sub>.
</p>
	<pre>
@prefix fhir: &lt;http://hl7.org/fhir/&gt; .
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix obs: &lt;http://hl7.org/fhir/Observation.&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix sct: &lt;http://snomed.info/id/&gt; .

_:b6 
	prov:generatedAtTime "2015-01-01T12:02:54Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
    lr:stream lr:streamA12342015-01-01T12:00:00Z .
_:b6 {
	_:b7 a fhir:Observation ;
		obs:code sct:36407505 ;
		obs:device lr:sensor1234 ;
		obs:effectiveDateTime "2015-01-01T12:02:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		obs:subject lr:patientA ;
		obs:valueQuantity _:b8 .
	
	_:b8 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		lr:value "48"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
}
</pre>
One possible way that this RDF Stream would be transmitted is as a sequence of RDF documents, each one containing
an RDF Dataset corresponding to one element of the stream.
In this example, each element of the stream contains a triple in its default graph that provides an identifier for the
stream as a whole (<code>lr:streamA12342015-01-01T12:00:00Z</code>). This is optional for RDF streams. 
	</aside>
	<aside  class="example" id="prov-dataset-2" title="FHIR Provenance in a Second-Order Dataset">
	This data can also be represented using the second-order <a>Dataset</a> datatype.
	For this representation, we use an extension of the Trig syntax to "quints".
	This additional name allows the statements of each stream element to be grouped together,
	and also allows us to materialize, in the default (first-order) dataset, 
	statements giving the order among these elements.	
	The second-order dataset is denoted D2<sub>prov</sub>.
	<pre>
@prefix fhir: &lt;http://hl7.org/fhir/&gt; .
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix obs: &lt;http://hl7.org/fhir/Observation.&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix sct: &lt;http://snomed.info/id/&gt; .

_:b202 prov:wasRevisionOf _:b201.
_:b203 prov:wasRevisionOf _:b202.

_:b201{
    _:b0 
	    prov:generatedAtTime "2015-01-01T12:00:33Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
        lr:stream lr:streamA12342015-01-01T12:00:00Zm .
    _:b0 {
	    _:b1 a fhir:Observation ;
		    obs:code sct:36407505 ;
		    obs:device lr:sensor1234 ;
		    obs:effectiveDateTime "2015-01-01T12:00:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		    obs:subject lr:patientA ;
		    obs:valueQuantity _:b2 .
	
	    _:b2 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		    lr:value "50"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
    }
}	

_:b202{
	_:b3 
		prov:generatedAtTime "2015-01-01T12:01:45Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
	    lr:stream lr:streamA12342015-01-01T12:00:00Z .
	_:b3 {
		_:b4 a fhir:Observation ;
			obs:code sct:36407505 ;
			obs:device lr:sensor1234 ;
			obs:effectiveDateTime "2015-01-01T12:01:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
			obs:subject lr:patientA ;
			obs:valueQuantity _:b5 .
	
		_:b5 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
			lr:value "53"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
	}
}
_:b203{
	_:b6 
		prov:generatedAtTime "2015-01-01T12:02:54Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
	    lr:stream lr:streamA12342015-01-01T12:00:00Z .
	_:b6 {
		_:b7 a fhir:Observation ;
			obs:code sct:36407505 ;
			obs:device lr:sensor1234 ;
			obs:effectiveDateTime "2015-01-01T12:02:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
			obs:subject lr:patientA ;
			obs:valueQuantity _:b8 .
	
		_:b8 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
			lr:value "48"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
	}
}
</pre>
	</aside>
	<aside  class="example" id="psrov-repository" title="FHIR Provenance as a Second-Order Repository">
	The second-order dataset above is a static structure representing the entailment,
	under the provenance regime,
	of a second-order <a>Repository</a>.
	The repository as a whole is denoted R<sub>prov</sub>.
	For this representation, we use the same extension of the Trig syntax to "quints"
	as in the previous example.
	Since repositories are dynamic entities, one form of the transmission of the repository would
	be a sequence of documents representing named first-order datasets, as shown below.
	In this example, the default dataset of the repository is empty.
 	<p>The first document of the sequence is as follows.
		The named dataset is denoted ND<sub>1, prov</sub>.
</p>
	<pre>
@prefix fhir: &lt;http://hl7.org/fhir/&gt; .
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix obs: &lt;http://hl7.org/fhir/Observation.&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix sct: &lt;http://snomed.info/id/&gt; .

_:b201{
    _:b0 
	    prov:generatedAtTime "2015-01-01T12:00:33Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
        lr:stream lr:streamA12342015-01-01T12:00:00Zm .
    _:b0 {
	    _:b1 a fhir:Observation ;
		    obs:code sct:36407505 ;
		    obs:device lr:sensor1234 ;
		    obs:effectiveDateTime "2015-01-01T12:00:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		    obs:subject lr:patientA ;
		    obs:valueQuantity _:b2 .
	
	    _:b2 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		    lr:value "50"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
    }
}	
</pre>
 	<p>The second document of the sequence is as follows.
			The named dataset is denoted ND<sub>2, prov</sub>.
</p>
	<pre>
@prefix fhir: &lt;http://hl7.org/fhir/&gt; .
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix obs: &lt;http://hl7.org/fhir/Observation.&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix sct: &lt;http://snomed.info/id/&gt; .

_:b202{
	_:b3 
		prov:generatedAtTime "2015-01-01T12:01:45Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
	    lr:stream lr:streamA12342015-01-01T12:00:00Z .
	_:b3 {
		_:b4 a fhir:Observation ;
			obs:code sct:36407505 ;
			obs:device lr:sensor1234 ;
			obs:effectiveDateTime "2015-01-01T12:01:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
			obs:subject lr:patientA ;
			obs:valueQuantity _:b5 .
	
		_:b5 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
			lr:value "53"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
	}
}
</pre>
 	<p>The third document of the sequence is as follows.
			The named dataset is denoted ND<sub>3, prov</sub>.
</p>
	<pre>
@prefix fhir: &lt;http://hl7.org/fhir/&gt; .
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix obs: &lt;http://hl7.org/fhir/Observation.&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix sct: &lt;http://snomed.info/id/&gt; .

_:b203{
	_:b6 
		prov:generatedAtTime "2015-01-01T12:02:54Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
	    lr:stream lr:streamA12342015-01-01T12:00:00Z .
	_:b6 {
		_:b7 a fhir:Observation ;
			obs:code sct:36407505 ;
			obs:device lr:sensor1234 ;
			obs:effectiveDateTime "2015-01-01T12:02:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
			obs:subject lr:patientA ;
			obs:valueQuantity _:b8 .
	
		_:b8 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
			lr:value "48"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
	}
}
</pre>
	</aside>
	</section>
    <section>
	<h3>Patterns</h3>
     <aside class="example" id="triple-pattern-fhir-heartrate" title="Triple Pattern for FHIR Heartrate Observation">	
	 <p>The triple pattern below demonstrates how solution mappings might be extracted from the above data structures.
	 This triple pattern is denoted TP<sub>FHIR-heartrate</sub>.
	 </p>
	<pre>
?heartRateObservation a fhir:Observation ;
          obs:subject ?patient ;
          obs:effectiveDateTime ?heartRate_time ;
          obs:code &lt;http://snomed.info/sct:36407505&gt; ; # heart rate
          obs:valueQuantity [ fhir:Quantity.value ?heartRate;
                              fhir:Quantity.unit 'bpm'        ] .
</pre>
</aside>	
	</section>
    <section>
	<h3>Queries</h3>
     <aside class="example" id="select-triple-pattern-avg-hr" title="Select Query with Triple Pattern for Average Heartrate">	
	<pre>
	
SELECT (AVG(?heartRate) AS ?heartRate_base)

WHERE {
        ?heartRateObservation a fhir:Observation ;
          obs:subject ?patient ;
          obs:effectiveDateTime ?heartRate_time ;
          obs:code &lt;http://snomed.info/sct:36407505&gt; ; # heart rate
          obs:valueQuantity [ fhir:Quantity.value ?heartRate;
                              fhir:Quantity.unit 'bpm'        ] .
        FILTER ( ?heartRate_time &gt;= (now() - 'P30D'^^xsd:dayTimeDuration) )
    }
</pre>
	</aside>
	</section>
    <section>
	<h3>Abstract Algebra Examples</h3>
     <aside class="example" id="" title="">	
	</aside>
	</section>
</section>


	<p class="ednote">Ignore after this point- the material below is just copied from the RDF stream document for convenient reference.</p> 

	  <section id="def-timestamp-predicate">
          <h4>Timestamp Predicates</h4>
          <p> An RSP timestamp predicate <code>P</code> is an <code>rdf:Property</code> with range <code>rsp:TemporalEntity</code> that is
            associated, through the <code>rdf:Property</code> <code>rsp:hasTimePlenum</code>, with a unique partially-ordered set (poset) of 
            temporal entities <code>T<sub>P</sub> &sube; T<sup>all</sup></code>, called the <em>time plenum</em> of the predicate.
            The time plenum is associated, through the <code>rdf:Property</code> <code>rsp:hasPartialOrder</code>, 
            with a unique partial order <code>&le;<sub>P</sub></code> that is in the class
            <code>rsp:ChronologicalPartialOrder</code>.</p>
          <p>The usual mathematical requirements of a partial order MUST be satisfied by an instance of 
            <code>rsp:ChronologicalPartialOrder</code>. In particular, for all instances <code>X</code>, <code>Y</code>, <code>Z</code> 
            of the time plenum, the following properties hold: 
          </p>
          <dl>
            <dt>Reflexivity</dt>
            <dd><code>X &le;<sub>P</sub> X</code></li></dd>
            <dt>Antisymmetry<dt>
              <dd>  <code>X &le;<sub>P</sub> Y</code> and <code>Y &le;<sub>P</sub> X</code> implies <code>X =
                Y</code></dd>
            <dt>Transitivity</dt>
              <dd><code>X &le;<sub>P</sub> Y</code> and <code>Y &le;<sub>P</sub> Z</code> implies <code>X &le;<sub>P</sub>
                Z</code>. </dd>
          </dl>
          <p>Further, an instance of <code>rsp:ChronologicalPartialOrder</code> MUST respect the natural order of time.</p>
          <p>In
            particular, if every time instant within the closure of temporal entity <code>X</code>
            is earlier than every time instant within the closure of temporal entity <code>Y</code>,
            then <code>X &le;<sub>P</sub> Y</code> (where closure of a time instant <code>t</code> is defined as
            the degenerate interval <code>[t, t]</code>, and closure of an interval is defined in
            the usual way) </p>
          <p class="ednote">The above definition regarding natural order of time depends on the
            concept of time instant as the primitive of temporal entities that is universal amoung
            temporal ontologies and has a particular chronological order associated with them. 
            However, not all temporal
            ontologies have time instants as their primitives, e.g., OMG.
            Further, branching temporal ontologies have a different set of time primitives than linear ones.
            So this needs to be fixed.
          </p>
        </section>
      </section>
      <section id="def-timestamped-graph">
        <h3>Timestamped Graphs</h3>
        <p>A <em>timestamped graph</em> is defined as an RDF Dataset under the RDF Dataset semantics
          that <a
            href="http://www.w3.org/TR/2014/NOTE-RDF11-datasets-20140225/#each-named-graph-defines-its-own-context"
            >"each named graph defines its own context"</a> (see <a href="#semantics"></a>) and
          where a particular triple in the default graph has been designated as the timestamp
          triple, with the following constraints:</p>
        <ol>
          <li>There is exactly one named graph pair <code>&lt;n, G></code> in the RDF Dataset (where
              <code>G</code> is an <a href="http://www.w3.org/TR/RDF11-Concepts/#section-rdf-graph"
              >RDF graph</a>, and <code>n</code> is an IRI or blank node).</li>
          <li>The timestamp triple has the form <code>&lt;n, p, t></code>, where <code>n</code> is
            defined in the previous item, and <code>p</code> is a timestamp predicate that captures
            the relationship between the temporal entity <code>t</code>, called the timestamp, and
            the graph <code>G</code>.</li>
        </ol>
        <p class="note">There may be multiple triples in the default graph, including multiple
          triples using timestamp predicates, but exactly one triple must be designated as the
          timestamp triple. The objects of any triples that use a timestamp predicate but are not the 
          timestamp triple of the timestamped graph are not called timestamps.</p>
        <p class="ednote">Due to the assertion of the timestamp triple, the context referred to in
          "each named graph defines its own context" has a temporal aspect to it. Other aspects of
          this context may be asserted by additional triples in the default graph of the timestamped
          graph. Such triples are not required to have timestamp predicates, and thus may be about
          non-temporal aspects of the context, e.g., the authority or the sensor device. That is, we
          require the context to have a temporal aspect to it, but the context is not limited to
          temporal aspects. Thus, it would be misleading to call it a "temporal context". </p>
        <p class="ednote">The part of the definition above that points to RDF Dataset semantics
          really belongs in the semantics section, not in this definition, which should be purely
          syntax. We need additional informative text that gives the motivation for these
          definitions. </p>
        <p class="ednote">This definition does not permit the timestamp to be omitted, which is one
          of the data structures that is considered to be in-scope by the requirements document. </p>
        <p> A sequence of RDF graphs (or named graph pairs, or RDF datasets) MAY be physically received
          by an RSP engine, which MAY then create an RDF stream from it by adding timestamps, e.g.
          indicating the time of arrival. The original sequence is not itself an RDF stream. </p>
        <p class="note"> This definition allows the timestamp to be an IRI or blanknode. Additional
          information about the timestamp may be provided in the default graph (e.g., through
          properties in the OWL Time Ontology), but this is not required by the definition of
          timestamped graph. </p>
        <p>A <em>literal-timestamped graph</em> is a timestamped graph whose timestamp
            <code>t</code> is an <code>rdfs:Literal</code>.</p>
        <p class="note">Merge and union of RDF streams with non-literal-timestamped graphs may not
          be defined. See <a href="#merge-union"></a>.</p>
        <p class="ednote">The timestamp predicate <code>p</code> may be drawn from a community
          agreed vocabulary (<a href="https://github.com/streamreasoning/RSP-QL/issues/10">Issue
            10</a>). The timestamp predicate may also be user-defined. </p>
        <p> The format in the examples of this document follows <a href="http://www.w3.org/TR/trig/"
            >TriG</a>, although does not imply any specific serialization or formatting; it simply
          shows the data structured according to the RDF stream model. When the default graph of a
          timestamped graph contains only one triple, this must be the timestamp triple, so there is
          no need of an additional format to designate it. In examples of timestamped graphs having
          more than one triple in the default graph, the first triple of the default graph to occur
          in the serialization is assumed to be the timestamp triple. Prefixes (e.g.
            <code>prov:</code>, <code>dul:</code>) are used for readability; their expansions are defined in 
          <a>Table 1</a>.</p>
        <p class="ednote">A non-normative subsection should be created to hold all the information
          about the formatting of examples, and there the expansion of all prefixes that are used in
          examples can be defined.</p>
        <aside class="example" id="literal-TSG" title="Literal-Timestamped Graph">
          <p>The following timestamped graph contains a named graph pair <code>&lt;_:b0, G></code>
            where the graph <code>G</code> contains a triple stating that temperature in Berlin is 12.5 C.
            The timestamp predicate <code>p</code> used in this example is the [[PROV-O]] property <a
              href="https://www.w3.org/TR/prov-o/#generatedAtTime"
              ><code>prov:generatedAtTime</code></a>. The purpose of this timestamp triple is to add
            a temporal aspect to the context of the named graph pair, to the effect that the
            temporal entity <code>"2015-01-01T01:00:00Z"^^xsd:dateTime</code> is the time at which
            the entity <code>_:b0</code> "was completely created and is available for use". The
            details of the semantics of the named graph pair in a timestamped graph are provided in
              <a href="#semantics"></a>
          </p>
          <pre><code class="highlight">_:b0 prov:generatedAtTime "2015-01-01T01:00:00Z"^^xsd:dateTimeStamp .
_:b0 {dbp:Berlin loc:hasPointTempC "12.5"^^xsd:decimal .}</code></pre>
        </aside>
        <p class="ednote">According to the semantics defined in <a href="#semantics"></a>, the
          assertion of the named graph pair means that the graph denoted by <code>:g</code> entails
          the triple <code>dbp:Berlin loc:hasPointTempC "12.5"^^xsd:decimal .</code>,
          under whatever entailment regime is being considered. It does not assert that triple
          directly, nor does it assert that this triple is actually in that graph. Further, it
          does not rule out additional entailments of <code>_:b0</code>. These details are best
          explained in the semantics section itself, although it would probably be helpful to have
          some informative explanation near the beginning to avoid confusion. </p>
        <aside class="example" id="non-literal-TSG" title="Non-Literal-Timestamped Graph">
          <p>The following timestamped graph contains a named graph pair <code>&lt;:g2, G></code>
            where the graph <code>G</code> contains one triple that states that entity
              <code>:axel</code> leaves the <code>:BlueRoom</code>. The timestamp predicate used in
            this example is the DOLCE+DnS Ultralite property <a
              href="http://www.loa-cnr.it/ontologies/DUL.owl#isObservableAt"
                ><code>dul:isObservableAt</code></a>. The purpose of this timestamp triple is to add
            a temporal aspect to the context of the named graph pair, to the effect that there is
            some time <code>_:t2</code> at which an observation could have been (or be) made where
            the results of that observation are described by the entity <code>:g2</code>. There is
            an additional triple in the default graph that compares the observable time to some
            other time <code>_:t1</code> using the Allen interval relation <a
              href="https://www.w3.org/2006/time#after"><code>owl-time:intervalAfter</code></a> from
            [[OWL-TIME]]. </p>
          <pre><code class="highlight">
          :g2 dul:isObservableAt _:t2.
          :g2 {:axel :leave :BlueRoom. }
          _:t2 owl-time:intervalAfter _:t1
        </code></pre>
        </aside>
        <p>Given any two timestamped graphs, <code>TSG1</code> and <code>TSG2</code>, we say that
            <code>TSG2</code>
          <em>covers</em>
          <code>TSG1</code> (denoted <code>TSG1  	&#8818; TSG2</code>) if and only if <code>TSG1</code>
          and <code>TSG2</code> have the same timestamp predicate <code>P</code> and the timestamps,
            <code>t1</code> and <code>t2</code> resp., satisfy <code>t1 &le;<sub>P</sub> t2</code>,
          where <code>&le;<sub>P</sub></code> is the temporal partial order associated with the time plenum
          of the timestamp predicate <code>P</code>. </p>
        <p class="note">The relation <code>&#8818;</code> between timestamped graphs is a preorder (a
          reflexive, transitive binary relation). It is not a partial order because it doesn't have
          the antisymmetry property (<code>a&#8818; b</code> and <code>b&#8818; a</code> implies <code>a
            = b</code>.) </p>
        <aside class="example highlight" title="Distinct Timestamped Graphs That Cover Each Other">
          <p>Two timestamped graphs that have the same timestamp predicate and the same timestamp
            but different named graph pairs cover each other, but are not equal. This demonstrates that
            the "cover" relation is not antisymmetric. </p>
        </aside>
      </section>
      <section id="rdf-stream">
      <h3>RDF Stream</h3>
      <p>An <em>RDF stream</em>
        <code>S</code> consists of a sequence of timestamped graphs, called its <em>elements</em>, such that 
        elements sharing the same
        timestamp predicate are ordered by the partial order associated with this timestamp predicate.
        I.e., if a stream <code>S</code> contains elements <code>S(i)</code> and <code>S(j)</code>
        with <code>i &lt; j</code> and <code>S(i)</code> covers <code>S(j)</code>, then the 
        timestamps of  <code>S(i)</code> and <code>S(j)</code> are equal. </p>
      <p>If a timestamp predicate occurs in the timestamp triple of an element of an RDF stream, we say
         it is a <em>timestamp predicate of the stream.</em></p>
      <p class="ednote">Time-boundedness properties on RDF streams behave better if it is required
        that the set of temporal entities for each timestamp predicate is pairwise bounded. I.e.,
        for each pair of temporal entities in the set, there is a temporal entity in the set that is
        an upper bound of both, as well as a temporal entity in the set that is a lower bound of
        both. This property is not satisfied by branching temporal structures,  but could be a 
        requirement of some profile.</p>
      <p class="ednote">The comparability between any pair of elements of an RDF stream must (or should?) be
        completely determined from the default graphs of those elements, or elements that precede at least one of them. 
        Otherwise the ordering could
        be revealed by a subsequent element, inducing retroactively an ordering requirement on
        a previous pair of stream elements.</p>
      <p>On the following we may refer to RDF stream simply as stream.</p>
      <aside class="example" id="literal-rdf-stream" title="RDF Stream">
        <p>An RDF stream produces data that indicates where a person is at a given time. The
          timestamp predicate <code>p</code> used in this example is the PROV
            `<code>prov:generatedAtTime</code>. In this example the named graph pairs
            (<code>:g1</code>,<code>:g2</code>, etc.) contain the streaming data contents (for
          brevity the contents are represented by the dots <code>...</code>). </p>
        <pre class="highlight"><code>:g1 {...}{:g1,prov:generatedAtTime,t1}
:g2 {...} {:g2, prov:generatedAtTime, t2}
:g3 {...} {:g3, prov:generatedAtTime, t3}
:g4 {...} {:g4, prov:generatedAtTime, t4}
...
</code></pre>
        <p>We can expand the content of the graph component of each named graph pair, which is a set of triples:</p>
        <pre class="highlight"><code>
:g1 {:axel :isIn :RedRoom. :darko :isIn :RedRoom} 
  {:g1, prov:generatedAtTime, "2015-06-18T12:00:00Z"^^xsd:dateTime}
:g2 {:axel :isIn :BlueRoom. }                     
  {:g2, prov:generatedAtTime, "2015-06-18T12:00:35"^^xsd:dateTime}
:g3 {:minh :isIn :RedRoom. }                      
  {:g3, prov:generatedAtTime, "2015-06-18T12:02:07"^^xsd:dateTime}
...
</code></pre>
      </aside>
      <p class="note">There can be multiple graphs with the same timestamp in the stream.</p>
      <p class="ednote">It has been pointed out that this statement might be problematic, as graphs
        could no longer be used for punctuation purposes. Comparatively, we have not found a
        constraint on this in similar models e.g., CQL: <em>there could be zero, one, or multiple
          elements with the same timestamp in a stream</em>.</p>
      </section>
      <section id="isomorpism">
        <h3>Isomorphism</h3>
        <p>Two RDF timestamped graphs TSG1 and TSG2 are <em>isomorphic</em> if and only if there is
          a bijection M between the nodes, triples, graphs and named graph pairs in TSG1 and those in
          TSG2 such that: <ol>
            <li>M maps blank nodes to blank nodes;</li>
            <li>M is the identity map on literals and IRIs;</li>
            <li>For every triple &lt;s p o>, M(&lt;s, p, o>)= &lt;M(s), M(p), M(o)>;</li>
            <li>For every graph G={t1, ..., tn}, M(G)={M(t1), ..., M(tn)};</li>
            <li>For every named graph pair NG=&lt;n, G>, M(NG)=&lt;M(n), M(G)>;</li>
            <li> M(NG1)=NG2, M(DG1)=DG2, and M(TST1)=TST2, where NG1 is the named graph pair of TSG1, DG1
              is the default graph of TSG1, TST1 is the timestamp triple of TSG1, and similarly for
              TSG2.</li>
          </ol>
        </p>
        <p class="note"> The definition of timestamped graphs allows blank nodes to be used as graph
          names, as well as within triples. </p>
        <pre class="example highlight" title="Isomorphic Timestamped Graphs with Blank-Node Graph Names">
<code>:_1 {...} {:_1, prov:generatedAtTime, t1}
</code>

<code>:_2 {...} {:_2, prov:generatedAtTime, t1}
</code>
      
      </pre>
        <p>Two RDF streams are <em>S-isomorphic</em> if and only if they have the same set of
          elements.</p>
        <pre class="example highlight" title="S-Isomorphic Streams with Unique Well-Ordered Timestamp Predicate">
<code>
:g1 {...} {:g1, prov:generatedAtTime, t1}
:g2 {...} {:g2, prov:generatedAtTime, t2}
:g3 {...} {:g3, prov:generatedAtTime, t2}
:g4 {...} {:g4, prov:generatedAtTime, t3}
</code>

<code>
:g1 {...} {:g1, prov:generatedAtTime, t1}
:g3 {...} {:g3, prov:generatedAtTime, t2}
:g2 {...} {:g2, prov:generatedAtTime, t2}
:g4 {...} {:g4, prov:generatedAtTime, t3}
</code>
    </pre>
        <p>Two RDF streams S1 and S2 are <em>B-isomorphic</em> if and only if there is a bijection M
          between the nodes, triples, graphs, named graph pairs, and timestamped graphs in S1 and those
          in S2 such that: <ol>
            <li>M maps blank nodes to blank nodes;</li>
            <li>M is the identity map on literals and IRIs;</li>
            <li>For every triple &lt;s p o>, M(&lt;s, p, o>)= &lt;M(s), M(p), M(o)>;</li>
            <li>For every graph G={t1, ..., tn}, M(G)={M(t1), ..., M(tn)};</li>
            <li>For every named graph pair NG=&lt;n, G>, M(NG)=&lt;M(n), M(G)>;</li>
            <li>For every timestamped graph TSG where NG is the named graph pair and DG is the default
              graph containing the timestamp triple TST, M(TSG) is a timestamped graph TSG2, with
              named graph pair M(NG), default graph M(DG) and timestamp triple M(TST).;</li>
            <li>For every i &ge; 1, M(S1(i))=S2(i), where S1(i) is the i-th element of S1 and S2(i)
              is the i-the element of S2.</li>
          </ol>
        </p>
        <p class="note">An RDF stream is viewed as being on a single "RDF surface"(see [[BLOGIC]]),
          so that blank nodes may be shared between any graphs in the stream. For this reason,
          B-isomorphism is defined in terms of a single mapping M for the entire RDF stream rather
          than, say, separate mappings for each timestamped graph.</p>
        <p>Two RDF streams are <em>isomorphic</em> if there exists an RDF stream that is both
          B-isomorphic to one stream and S-isomorphic to the other stream.</p>
        <p class="note"> RDF streams that are S-isomorphic are isomorphic. </p>
        <p class="note"> RDF streams that are B-isomorphic are isomorphic. </p>
        <p>Isomorphic RDF streams MUST have the same semantics. The semantics of RDF streams is
          affected by the result of applying window functions (<a href="#window-functions"></a>) as
          well as by entailment. Therefore, isomorphic RDF streams SHALL be indistinguishable, up to
          isomorphism, with respect to entailment (in any entailment regime), as well as with
          respect to the application of window functions. </p>
      </section>
      <section id="substreams-and-Windows">
        <h3>Substreams and Windows</h3>
        <p>A <em>substream</em>
          <code>S'</code> of an RDF stream <code>S</code> is an RDF stream that is isomorphic to a
          subsequence of <code>S</code>.</p>
        <p class="note">There are several specializations of the substream relation which are useful
          in practice. These include the <em>filter</em> relation, where stream elements are
          selected based on satisfaction of some criterion, and the <em>window</em> relation, where
          a temporally contiguous portion of the stream is selected. </p>
        <p>A <em>window</em>
          <code>S'</code> of an RDF stream <code>S</code> is a substream of <code>S</code> such that
          if <code>S'(i)</code> and <code>S'(j)</code> are two elements of <code>S'</code> and
          <code>S'(i)  	&#8818; S'(j)</code> (i.e., <code>S'(j)</code> covers <code>S'(i)</code>), and
          further if <code>S'(i)  	&#8818; S(k)  	&#8818; S'(j)</code> for some element <code>S(k)</code> of
            <code>S</code>, then <code>S(k)</code> is an element of <code>S'</code>. </p>
        <p class="note">Informally, a window is a temporally-contiguous selection from the original
          stream, without gaps.</p>
      </section>
    </section>
    <section id="merge-union">
      <h2>Stream Merge and Union Operations</h2>
      <p>In order to combine two RDF streams into one, without loss or gratuitous introduction of
        entailments, we define two relations, <em>RDF stream merge</em> and <em>RDF stream
          union</em>. These relations are inspired by the concepts of similar name for RDF graphs
        defined in [[RDF11-Concepts]]. </p>
      <p>
        <em>RDF stream union</em> is a trinary relation on RDF streams. Let <code>S1</code>,
          <code>S2</code>, and <code>S3</code> be RDF streams. We say <code>S3</code> is an <em>RDF
          stream union</em> of <code>S1</code> and <code>S2</code> if and only if the set of
        elements of <code>S3</code> is equal to the union of the sets of elements of <code>S1</code>
        with <code>S2</code>.</p>
      <p class="note"> Union is the appropriate relation to consider when combining streams that are
        allowed to share blank nodes. Informally, they would be on the same RDF surface. </p>
      <aside class="example" title="Union of RDF Streams that Share Blank Nodes">
        <pre><code class="highlight">
        :g1 {_:1 :isIn :RedRoom. _:2 :isIn :RedRoom}    {:g1, prov:generatedAtTime, t1}
        :g2 {_:1 :isIn :BlueRoom. }                     {:g2, prov:generatedAtTime, t2}
        :g3 {_:3 :isIn :RedRoom. }                      {:g3, prov:generatedAtTime, t3}
        </code></pre>
        is a union of the following two streams: <pre><code class="highlight">
          :g1 {_:1 :isIn :RedRoom. _:2 :isIn :RedRoom}  {:g1, prov:generatedAtTime, t1}
          :g3 {_:3 :isIn :RedRoom. }                    {:g3, prov:generatedAtTime, t3}
        </code></pre>
        <pre><code class="highlight">
          :g2 {_:1 :isIn :BlueRoom. }                   {:g2, prov:generatedAtTime, t2}
        </code></pre>
      </aside>
      <p>
        <em>RDF stream merge</em> is a trinary relation on RDF streams. Let <code>S1</code>,
          <code>S2</code>, and <code>S3</code> be RDF streams. We say <code>S3</code> is an <em>RDF
          stream merge</em> of <code>S1</code> and <code>S2</code> if and only if there exist
        streams <code>S4</code>, <code>S5</code>, and <code>S6</code>, with <code>S4</code>
        isomorphic to <code>S1</code>, <code>S5</code> isomorphic to <code>S2</code>,
          <code>S6</code> isomorphic to <code>S3</code>, such that <code>S4</code> and
          <code>S5</code> have no blank nodes in common, and <code>S6</code> is the union of
          <code>S4</code> with <code>S5</code>.</p>
      <p class="note"> Merge is the appropriate relation to consider when joining streams that are
        considered to not share blank nodes. Informally, they would be on different RDF surfaces. </p>
      <aside class="example" title="Merge of RDF Streams that Do Not Share Blank Nodes">
        <pre><code class="highlight">
        _:1 {:axel :isIn :RedRoom. :darko :isIn :RedRoom} {_:1, prov:generatedAtTime, t1}
        _:2 {:axel :isIn :BlueRoom. }                     {_:2, prov:generatedAtTime, t2}
        _:3 {:minh :isIn :RedRoom. }                      {_:3, prov:generatedAtTime, t3}
        </code></pre>
        is a merge of the following two streams: <pre><code class="highlight">
        _:1 {:axel :isIn :RedRoom. :darko :isIn :RedRoom} {_:1, prov:generatedAtTime, t1}
        _:2 {:minh :isIn :RedRoom. }                      {_:2, prov:generatedAtTime, t3}
        </code></pre>
        <pre><code>
        _:1 {:axel :isIn :BlueRoom. }                     {_:1, prov:generatedAtTime, t2}
        </code></pre>
      </aside>
      <p class="note">Given any three RDF streams, the determination of their satisfaction of the
        union or merge relation is a purely syntactic computation. That is, it is not necessary to
        consider entailments of the RDF streams or to be concerned with the values of literals.</p>
      <aside class="ednote">
        <p>It should not be hard to show that each relation defines a (deterministic) binary partial
          operation on the equivalence classes of isomorphic RDF streams. However, note that this
          operation is not purely syntactic. The construction of a union of two streams,
            <code>S1</code> and <code>S2</code>, can be accomplished, theoretically, by the
          following steps:</p>
        <ol>
          <li>Construct the set of timestamped graphs <code>R3</code> that is the set union of
            elements of <code>S1</code> and <code>S2</code>. </li>
          <li>Construct an RDF stream <code>S3</code> from the set <code>R3</code> by placing this
            set into a sequential form that satisfies the definitional constraint regarding relative order of
            elements, if possible.</li>
        </ol>
        <p> The second step may require consideration of the values of timestamps or entailments
          regarding temporal order in order to <em>sequentialize</em> the set of timestamp graphs.
          The construction of a merge is accomplished by performing stream copying to a common
          surface before union: </p>
        <ol>
          <li>Copy both <code>S1</code> and <code>S2</code> to a new RDF surface by renaming blank
            nodes that occur in both streams to fresh blank nodes (new to both <code>S1</code> and
              <code>S2</code>), producing new streams <code>S1'</code> and <code>S2'</code>. </li>
          <li>Form the union of streams <code>S1'</code> and <code>S2'</code>.</li>
        </ol>
      </aside>
      <p class="ednote"> The continuous operation of union or merge of two semi-infinite streams may
        not be computable, even when the operation is defined. This is because it may not be
        possible to know when an element of one stream might be received that needs to occur before
        the latest elements of the other stream. Computability of these continuous operations may be
        possible when additional information is available about the streams to be combined, e.g., the
        set of timestamp predicates used by each stream and the maximum latency of transmission.
        Within some RDF stream profiles, the union and merge operations may be continuously computable
        due to the profile constraints.
      </p>
    </section>
    <section id="subclasses">
      <h2>RDF Stream Subclasses</h2>
      <p class="ednote">This section may be expanded with more subclasses if a need is identified.</p>
      <h3>Time-bounded RDF Streams</h3>
      <p>A <em>time-bounded-above RDF stream</em> is an RDF Stream where for every timestamp
        predicate of the stream, there is a temporal entity in its range that bounds
        from above (is greater than or equal to) all timestamps of that predicate in the stream. </p>
      <p>A <em>time-bounded-below RDF stream</em> is an RDF Stream where for every timestamp
        predicate of the stream, there is a temporal entity in its range that bounds
        from below (is less than or equal to) all timestamps of that predicate in the stream. </p>
      <p>A <em>time-bounded RDF stream</em> is an RDF Stream that is time-bounded above and
        time-bounded below. </p>
      <p class="ednote"> Consider the class of timestamp predicates whose associated poset of
        temporal entities is <em>pairwise bounded</em>, i.e., has the property that every pair of
        temporal entities is bounded, i.e., there exists a temporal entity in the poset that is
        greater or equal to both of them and another temporal entity that is less or equal to both
        of them. This is a fairly natural property to expect for partial orders on time plenums with "linear topology" (e.g.
        [[OWL-TIME]]). However it does not hold in the case of branching time plenums (see <a
          href="http://www.ihmc.us/users/phayes/docs/timeCatalog.pdf"
          >http://www.ihmc.us/users/phayes/docs/timeCatalog.pdf</a>), because there are incomparable
        branches (called paths), which may be considered, e.g., as different worlds (in the sense of
        Kripke frames), scenarios (planning usecase), or concurrent processes (process modelling).
        It would be a useful property if the class of time-bounded RDF streams were closed under
        stream merger and union, and this property would hold, under the current definition, in the
        case of pairwise-bounded time plenums, because we could find bounds for the temporal entities
        of each timestamp predicate by taking an upper bound of the pair of upper bounds from the
        two streams, and similarly for the lower bounds. However, the closure property does not hold
        in the case of a branching temporal topology under the above definition of time-bounded. To
        achieve the closure property, we would have to allow a number of "upper bounds" for each
        timestamp predicate, rather than requiring just one. For example, <br /><br /> Given two
        sets, A and B, of entities in a partial order, we say that A bounds B from above iff there
        is no element of B that is greater than any element of A, and for every element of B there
        is some element of A that is greater than or equal to it. <br /><br /> A
          <em>time-bounded-above RDF stream</em> is an RDF Stream where for every timestamp
        predicate of the stream, there is a set of temporal entities in its range that
        bounds from above all timestamps of that predicate in the stream. </p>
      <p class="note"> The class of time-bounded RDF streams using timestamp predicates with
        pairwise-bounded time plenums is closed under stream merger and union. </p>
      <h3>Temporal-count-bounded RDF Streams</h3>
      <p>A <em>temporal-count-bounded RDF stream</em> is an RDF stream where for each timestamp
        predicate of the stream, there are a finite number of temporal entities in the
        stream that are timestamps for that predicate.</p>
      <p class="ednote">The qualifier "temporal" has been added to this term to emphasize that it is
        temporal entities that are being counted, not timestamped graphs.</p>
      <p class="ednote"> Given an RDF stream whose set of timestamp predicates with pairwise-bounded
        temporal entities, then if the stream is temporal-count-bounded it is also time-bounded,
        since the upper bound for each timestamp predicate can be taken as an upper bound of the
        (finite) set of its timestamps, and similarly for the lower bound. If the definition of
        "time-bounded" was changed as in the earlier Editor's Note, then every
        temporal-count-bounded RDF stream would be time-bounded. </p>
      <p class="note"> Every temporal-count-bounded RDF stream using only timestamp predicates with
        pairwise-bounded time plenums is time-bounded. </p>
      <p class="note"> The class of temporal-count-bounded RDF streams is closed under stream merger
        and union, since the sets of temporal entities for each timestamp predicate in the resulting
        stream are the union of the corresponding (finite) sets in the original streams, and so are
        finite. </p>
      <pre class="example highlight" title="As a pathological example, an RDF stream representing Zeno's paradox using a continuous time plenum (e.g., Julian date) would be time-bounded but not temporal-count-bounded.">          
        </pre>
      <h3>Finite RDF Stream</h3>
      <p>A <em>finite RDF stream</em> is an RDF stream of finite length, i.e., with a finite number
        of elements in it.</p>
      <p class="note"> Clearly, every finite RDF stream is temporal-count-bounded. </p>
      <p class="ednote"> Every finite RDF stream using only pairwise-bounded time plenums is
        time-bounded. However, this is not the case for branching time plenums. </p>
      <p class="note"> Clearly, the class of finite RDF streams is closed under stream merger and
        union. </p>
      <pre class="example highlight" title="As another pathological example, an RDF stream that contained an infinite set of timestamped graphs all with the same timestamp predicate and  timestamp would be temporal-count-bounded but not be finite.">
        </pre>
      <pre class="example highlight" title="Even more pathological, a temporal-count-bounded stream that uses an infinite set of timestamp predicates would not be finite.">
        </pre>
    </section>
    <section id="window-functions">
      <h2>Window Functions</h2>
      <p>A <em>window function</em> is a partial function from RDF streams to their windows that
        preserves isomorphism. That is, if <code>w</code> is a window function, with isomorphic
        streams <code>S1</code> and <code>S2</code> in its domain, then <code>w(S1)</code> is
        isomorphic to <code>w(S2)</code>. </p>
      <p>A <em>general window function</em> is a window function that is a total function on RDF
        streams. </p>
      <p class="ednote">The term "window operator" is reserved for later use to return a sequence of
        windows.</p>
      <p>The most common types of window functions in practice are time-based and count-based.</p>
      <section id="time-based-window-functions">
        <h3>Time-based Window Functions</h3>
        <p>Because the time plenum for each timestamp predicate is partially ordered, we may define a
            <em>temporal interval of a timestamp predicate</em> to be an interval, in the usual
          sense for partial orders, within its time plenum. </p>
        <p class="note">Recall that intervals need not be bounded and need not be closed, and are
          specified in terms of two, one or zero inequality conditions based on the partial order or
          its induced strict order.</p>
        <p>A <em>general time-based window function</em>
          <code>w</code> is a general window function specified by a finite set
            <code>w<sub>P</sub></code> of timestamp predicates together with temporal intervals
              <code>w<sub>J</sub>(P)</code> of each timestamp predicate <code>P</code> in
              <code>w<sub>P</sub></code>, such that for every stream <code>S</code>, an element
            <code>S(i)</code> of <code>S</code> is an element of <code>w(S)</code> if and only if the
          timestamp predicate <code>P</code> of <code>S(i)</code> is in <code>P</code> and the
          timestamp <code>t</code> of <code>S(i)</code> is contained in
            <code>w<sub>J</sub>(P)</code>. </p>
        <p>A <em>time-based window function</em> is the restriction of a general time-based window
          function to a subset of RDF streams.</p>
        <p class="note">The substream resulting from the application of a time-based window function
          is time-bounded.</p>
      </section>
      <section id="temporal-count-based-window-functions">
        <h3>Temporal-count-based Window Functions</h3>
        <p>A <em>general temporal-count-based window function</em>
          <code>w</code> is a general window function specified by a finite set
            <code>w<sub>P</sub></code> of timestamp predicates together with semi-infinite temporal
          intervals <code>w<sub>J</sub>(P)</code> of each timestamp predicate <code>P</code> in
              <code>w<sub>P</sub></code> with endpoints <code>w<sub>T</sub>(P)</code>, and positive
          integers <code>w<sub>N</sub>(P)</code>, for each timestamp predicate <code>P</code> in
              <code>w<sub>P</sub></code> such that for every stream <code>S</code>, an element
            <code>S(i)</code> of <code>S</code> is an element of <code>w(S)</code> if and only if the
          timestamp predicate <code>P</code> of <code>S(i)</code> is in <code>P</code> and <ul>
            <li><code>t</code> is the timestamp of <code>S(i)</code>,</li>
            <li><code>T(S, P, w)</code> is the set of temporal entities that are timestamps for
              elements of <code>S</code> with timestamp predicate <code>P</code>, belong to
                  <code>w<sub>J</sub>(P)</code>, and belong to the closed interval between
                <code>t</code> and <code>w<sub>T</sub>(P)</code>, and</li>
            <li>the cardinality of <code>T(S, P, w)</code> is less than or equal to
                  <code>w<sub>N</sub>(P)</code></li>

          </ul>
        </p>
        <p>A <em>temporal-count-based window function</em> is the restriction of a general
          temporal-count-based window function to a subset of RDF streams.</p>
        <aside class="note">The above definition of temporal-count-based window function allows the
          following characteristics to be set independently for each timestamp predicate of concern: <ul>
            <li>future-facing or past-facing orientation,</li>
            <li>inclusive or exclusive temporal anchor,</li>
            <li>depth.</li>
          </ul>
        </aside>
        <p class="note">The substream resulting from the application of a temporal-count-based
          window function is temporal-count-bounded.</p>
        <p class="note">Due to the potential for stream elements with incomparable or duplicate
          timestamps, the number of elements in the substream having a particular timestamp
          predicate is not guaranteed to be equal to the depth specified for the predicate by the
          temporal-count-based window. </p>
        <p class="note">Temporal-count-based window functions with future-facing orientation on
          timestamp predicates whose time plenum is not totally-ordered are not computable, because in
          general it is not possible to know, at any point in the reception of the stream, whether
          there are further elements of the stream that would be selected by the
          temporal-count-based window function.</p>
        <p>Applications that require a substream with an exact number <code>N</code> of elements for
          a specified timestamp predicate might apply a temporal-count-based window function with
            <code>N</code> for the count of temporal entities, and then randomly discard extra
          elements, according to some criterion, e.g., extreme elements (maximal or minimal,
          depending on the orientation of the counting). However, this extra step causes the process
          to be nondeterministic, and hence does not correspond to a function. If elements are
          discarded in a nonrandom fashion, e.g., based on their order in the stream sequence, then
          this would be a function, but would not preserve isomorphism, and so would not be a window
          function. The issue of obtaining a window function that returns an exact number of
          elements (for a particular predicate) is handled in the next section, where we define the
          concept of window relation, and use it to define an element-count-based window relation.
          When restricted to certain kinds of RDF streams, element-count-based window relations are
          functional so element-count-based window functions can be defined on such subsets. This
          and similar considerations, of importance to implementations, motivate the <a
            href="#profiles">RDF Stream profiles</a> as subsets of RDF streams. </p>
      </section>
      <section id="window-relations">
        <h3>Window Relations</h3>
        <p>A <em>window relation</em> is a binary relation on RDF streams (a relation having an
          extension which is a set of pairs of RDF streams) such that the second element in the pair
          is a window of the first element and preserves isomorphism. That is, if <code>&lt;S1,
            S2&gt;</code> is a member of the extension of a window relation <code>W</code>, and
            <code>S3</code> and <code>S4</code> are isomorphic to <code>S1</code> and
            <code>S2</code>, resp., then <code>&lt;S3, S4&gt;</code> is also a member of the
          extension of <code>W</code>. </p>
        <p>A <em>length-based window relation</em>
          <code>W</code> is a window relation specified by a set of streams
            <code>W<sub>S</sub></code>, a finite set <code>W<sub>P</sub></code> of timestamp
          predicates together with the following parameters for each timestamp predicate
            <code>p</code> in <code>W<sub>P</sub></code>: <ul>
            <li>a semi-infinite temporal interval <code>W<sub>J</sub>(P)</code> of timestamp
              predicate <code>P</code> (the interval of <code>P</code>);</li>
            <li>a temporal entity <code>W<sub>T</sub>(P)</code> which is the finite endpoint of
                  <code>W<sub>J</sub>(P)</code> (the temporal reference of <code>P</code>);</li>
            <li>a positive integer <code>W<sub>N</sub>(P)</code> (the length for
              <code>P</code>)</li>
          </ul> The extension of a length-based window relation is defined as follows: Let
            <code>S</code> be an RDF stream in <code>W<sub>S</sub></code> with window
            <code>S'</code>. Define <code>T(S, P, W)</code> to be the set of temporal entities that
          are timestamps for elements of <code>S</code> with timestamp predicate <code>P</code> and
          belong to <code>W<sub>J</sub>(P)</code>, and <code>T(S', P)</code> to be the set of
          temporal entities that are timestamps for elements of <code>S'</code> with timestamp
          predicate <code>P</code>. The pair <code>&lt;S, S'&gt;</code> is a member of the extension
          of <code>W</code> if and only if the cardinality of <code>T(S', P)</code> is equal to the
          minimum of <code>W<sub>N</sub>(P)</code> and the cardinality of the set <code>T(S, P,
            W)</code> and for each element <code>S'(i)</code> of <code>S'</code>
          <ul>
            <li>the timestamp predicate <code>P</code> of <code>S'(i)</code> is in
                  <code>W<sub>P</sub></code>, and</li>
            <li> the timestamp <code>t</code> of <code>S'(i)</code> is in
                <code>W<sub>J</sub>(P)</code>.</li>
          </ul>
        </p>
        <p>A <em>length-based window function</em>
          <code>W</code> on domain <code>W<sub>S</sub></code> is a length-based window relation
          where any member <code>&lt;S, S'&gt;</code> of its extension are such that <code>S</code>
          is in <code>W<sub>S</sub></code> and <code>W</code> defines a total function on
              <code>W<sub>S</sub></code>.</p>
        <p class="ednote">The names "element-count-bounded window relation" and "element-count-bounded window function"
        have been proposed, as a replacement of "length-bounded ...". This nomenclature depends on the adoption of 
        "element" as the term for the individual timestamped graphs in a stream.</p>
      </section>
    </section>
    <!--<h2>References:</h2>
    
    <ul>
      <li>EP-SPARQL: a unified language for event processing and stream reasoning.
        Anicic, D., Fodor, P., Rudolph, S., &amp; Stojanovic, N. In WWW (p. 635-644). ACM. 2011.</li>
      <li>C-SPARQL: a Continuous Query Language for RDF Data Streams. 
        Barbieri, D. F., Braga, D., Ceri, S., Della Valle, E., &amp; Grossniklaus, M. Int. J. Semantic Computing, 4(1), 3-25. 2010.</li>
      <li>Enabling query technologies for the semantic sensor web. 
        Calbimonte, J.-P., Jeung, H., Corcho, Ó., &amp; Aberer, K. Int. J. Semantic Web Inf. Syst., 8(1), 43-63. 2012.</li>
      <li>RSP-QL Semantics: a Unifying Query Model to Explain Heterogeneity of RDF Stream Processing Systems. 
        D. Dell’Aglio, E. Della Valle, J.-P. Calbimonte, O. Corcho. Int. J. Semantic Web Inf. Syst, 10(4). (in press). 2015.</li>
      <li>A Native and Adaptive Approach for Unified Processing of Linked Streams and Linked Data.
        Phuoc, D. L., Dao-Tran, M., Parreira, J. X., &amp; Hauswirth, M.In ISWC (Vol. 7031, p. 370-388). Springer. 2011.</li>
      <li>LARS: A Logic-based Framework for Analyzing Reasoning over Streams.
        Beck, H., Dao-Tran, M., Eiter, T., Fink, M. In AAAI. 2015.</li>
      <li>RDF 1.1: On Semantics of RDF Datasets. Zimmerman, Antoine, ed.. 2014.  <a href="http://www.w3.org/TR/2014/NOTE-RDF11-datasets-20140225">http://www.w3.org/TR/2014/NOTE-RDF11-datasets-20140225</a>.</li>
    </ul>-->
    <blockquote>
      <p class="note">this example could be integrated to the main text body</p>
      <h3>Beyond time instants: intervals &amp; more</h3>
      <p>Usign the previously described model, intervals can be specified for a graph in the
        following way: Given p1 and p2 representing start and end time predicates, then
          <code>(g,p1,t1)</code> and <code>(g,p2,t2)</code> denote that g is defined in an interval
        [t1,t2]. As an example:</p>
      <pre class="example highlight" title="RDF Stream with Two Timestamp Predicates for One Named Graph Pair"><code>:g_1, :startsAt, "2015-06-18T12:00:00"^^xsd:dateTime
:g_1, :endsAt, "2015-06-18T13:00:00"^^xsd:dateTime
</code></pre>
      <p>Or even:</p>
      <pre class="example highlight" title="RDF Stream with Interval Timestamp"><code>:g_2 :validBetween
    [:startsAt "2015-06-18T12:00:00"^^xsd:dateTime;
    :endsAt "2015-06-18T13:00:00"^^xsd:dateTime]
</code></pre>
    </blockquote>
    <section id="semantics">
      <h2>Semantics</h2>
      <p> The semantics of a timestamped graph is defined in terms of its semantics as an RDF
        Dataset. In particular, the designation of a particular triple in the default graph as the
        timestamp triple has no effect on its semantics. </p>
      <p>The semantics of timestamped graphs, and consequently of RDF streams, is based on the
        semantics formalized in [[RDF11-Datasets]] in the case that each named graph pair defines its own
        context.</p>
      <p>The following terms are used in the sense of [[RDF11-MT]]: <em>entailment regime,
          E-interpretation, blank node, universe, named graph pair, RDF graph, E-entails, default
          graph</em>.</p>
      <p> An <em>RSP interpretation</em>
        <code>I</code> with respect to an entailment regime <code>E</code> is an E-interpretation
        extended to named graph pairs, timestamped graphs, RDF datasets, and RDF streams as follows: <ul>
          <li>given a mapping <code>A</code> from blank nodes to the universe <code>IR</code> and a
            named graph pair <code>ng</code> = <code>&lt;n, G></code>, <code>[I+A](ng)</code> is
            true if and only if <code>[I+A](n)</code> is an RDF graph that E-entails
            <code>G</code>;</li>
          <li>for a timestamped graph <code>TSG</code> = <code>&lt; ng, DG></code>, where
              <code>DG</code> is the default graph and <code>ng</code> is the named graph pair of
              <code>TSG</code>,and a mapping <code>A</code> from blank nodes to the universe
              <code>IR</code>, <code>[I+A](TSG)</code> is true if and only if <code>[I+A](DG)</code>
            is true and <code>[I+A](ng)</code> is true;</li>
          <li><code>I(TSG)</code> true if and only if there exisits a mapping <code>A</code> from
            blank nodes to the universe <code>IR</code> such that [I+A](TSG) is true.</li>
          <li>for an RDF dataset <code>D</code> = <code>&lt; DG, NG></code>, where <code>DG</code>
            is the default graph and <code>NG</code> is the set of named graph pairs of
              <code>D</code>, <code>I(D)</code> is true if there exists a mapping <code>A</code>
            from blank nodes to the universe <code>IR</code> such that <code>[I+A](DG)</code> is
            true and <code>[I+A](ng)</code> is true for every <code>ng</code> in
            <code>NG</code>;</li>
          <li><code>I(D)</code> is false otherwise.</li>
          <li>for an RDF stream <code>S</code>, I(S) is true if and only if if there exists a
            mapping <code>A</code> from blank nodes to the universe <code>IR</code> such that for
            every element <code>S(i)</code> of <code>S</code>, <code>i &ge; 1</code>,
              <code>[I+A](S(i))</code> is true.</li>
        </ul> We say that an RSP interpretation <code>I</code> E-satisfies a graph, named graph
        pair, timestamped graph, dataset, or stream <code>X</code> (or satisfies <code>A</code>
        w.r.t the E-entailment regime) when <code>I(X)</code> is true. </p>
      <p>Following standard terminology, we say that a graph, named graph pair, timestamped graph,
        dataset, or stream <code>X</code> RSP-E-entails a graph, named graph pair, timestamped graph,
        dataset, or stream <code>Y</code> if and only if for every RSP interpretation <code>I</code>
        with respect to E-entailment, <code>I</code> E-satisfies <code>Y</code> whenever
          <code>I</code> E-satisfies <code>X</code>.</p>
      <p class="note">The "RSP" prefix in "RSP-E-entails" may be dropped when neither antecedent nor
        consequent is an RDF dataset or named graph pair, as in that case there is no possibility of
        alternate dataset semantics.</p>
      <p class="ednote">It should be straightforward to prove that isomorphic RDF streams simply
        entail each other, and hence are logically equivalent under simple entailment. This should
        also hold for other standard entailment regimes, and perhaps should be required for all RSP
        entailment regimes.</p>
      <p class="ednote"> The notion of RSP-E-entailment is defined here so that it may be used to
        support the definition of query semantics under various entailment regimes in the [[RSP-QL
        Queries]] document, including simple entailment. A choice was made regarding the semantics
        of named graph pairs (i.e., <code>[I+A](ng)</code> is true if and only if
          <code>[I+A](n)</code> is an RDF graph that E-entails <code>G</code>) that affects the
        semantics of timestamped graphs, datasets, and streams. It remains to be seen if this choice
        supports query semantics in a manner that meets the needs of the RSP-QL community. In
        particular, it imposes constraints on the denotation of the name of a named graph pair which
        may be inconsistent with the domain of a predicate that might be considered for use as a
        timestamp predicate (e.g., SSN predicate <code>ssn:observationSamplingTime</code>). </p>
    </section>
    <section id="profiles">
      <h2>Profiles</h2>
      <p>It is possible to restrict the abstract syntax for a class of RDF streams. This is often
        done in order to facilitate the efficient implementation of certain operations and queries
        or to promote more efficient representation. Each such restriction constitutes an <em>RDF
          stream profile</em>.</p>
      <p>The profiles defined in this document fall into two categories: time-series profiles and
        linked-list profiles. Within each category there is a least-restrictive profile and a number
        of subprofiles which apply additional restrictions to it.</p>
      <section id="timeseries">
        <h3>RDF Time-Series Profiles</h3>
        <p>This section describes four profiles which are defined through restrictions on the
          default graphs of the stream's timestamped graph elements or on the relation between
          timestamp values: <ul>
            <li>RDF time series</li>
            <li>RDF distinct time series</li>
            <li>RDF regular time series</li>
            <li>RDF distinct regular time series</li>
          </ul></p>
        <section id="time-series-profile">
          <h4>RDF Time-Series Profile</h4>
          <p>The RDF Stream Time-Series profile is designed to support high-volume, low-latency
            window operations and queries that depend on full knowledge of timestamps.</p>
          <p class="ednote">This motivational paragraph needs some work - the above is a place
            holder.</p>
          <p>An <em>RDF time series</em> is an RDF stream that satisfies the following
            properties:</p>
          <ol>
            <li>The stream uses exactly one timestamp predicate.</li>
            <li>The range of this timestamp predicate is xsd:dateTimeStamp.</li>
            <li>No two elements in the time series have the same graph name.</li>
          </ol>
          <aside class="example">
            <p><a href="#literal-rdf-stream">Example 4</a> satisfies the first and third
              requirements of the time-series profile. However, the range of the predicate
                <code>prov:generatedAtTime</code> is not limited to xsd:dateTimeStamp -
              non-timezoned values of xsd:dateTime are also permitted.</p>
          </aside>
          <p class="ednote">If some property (e.g., in the RSP-QL namespace) is defined as a
            subproperty of <code>prov:generatedAtTime</code>, then this can be substituted in <a
              href="#literal-rdf-stream">Example 4</a>, which will then serve as an example of a
            time series. Alternatively, the definition could be changed so that the only requirement
            is that the timestamps that occur in the stream belong to xsd:dateTime. </p>
          <p class="note">The set of RDF time series is closed under RDF stream merge and union on
            time series that use the same timestamp predicate.</p>
        </section>
        <section id="distinct-time-series-profile">
          <h4>RDF Distinct Time-Series Profile</h4>
          <p>An "RDF distinct time series" is an RDF time series such that no two elements in the
            time series have the same timestamp.</p>
          <aside class="example">
            <p> A distinct time series example ...</p>
          </aside>
          <p class="note">An RDF distinct time series has a unique sequential order - it is
            S-isomorphic only to itself.</p>
          <p class="note">The merge or union of RDF distinct time series is always defined but is
            not necessarily an RDF distinct time series, even if the operation is restricted to time
            series that use the same timestamp predicate, due to the possibility of duplication of
            timestamp. However, the merge or union of RDF distinct time series that use the same
            timestamp predicate is always an RDF time series. </p>
          <p class="note">The restriction of element-count-based window relations 
            to distinct time series results in functional
            relations. This enables the definition of element-count-based window functions for distinct time series. 
            Further, the element-count-based window functions are always computable. However, the
            time-based window functions are not necessarily computable unless there is a finite
            precision to the timestamps, as can be seen from the Zeno's paradox example @@@(this
            example hasn't been written up yet, but is mentioned elsewhere). </p>
          <p class="ednote">There is going to be a lot more to talk about for RDF time series
            relative to queries, but in this document we don't have those definitions.</p>
        </section>
        <section id="regular-time-series-profile">
          <h4>RDF Regular Time-Series Profile</h4> A <em>regular</em> RDF time series is a further
          subclass of distinct time series where the duration between timestamps is an integer
          multiple of a specified duration, called the <em>spacing</em> of the time series. <aside
            class="example">
            <p> A regular time series example ...</p>
          </aside>
          <p class="note"> The restriction of timestamps in the regular time series profile allows
            the timestamp to be represented concisely as an integer, provided the spacing is
            provided elsewhere, e.g., in metadata. </p>
        </section>
        <section id="distinct-regular-time-series-profile">
          <h4>RDF Distinct Regular Time-Series Profile</h4> A <em>distinct regular</em> RDF time
          series is a further subclass of distinct time series that is both a distinct time series
          and a regular time series. <aside class="example">
            <p> A distinct regular time series example ...</p>
          </aside>
          <p class="note"> @@@motivate this profile </p>
        </section>
      </section>
      <section id="linked-list-profiles">
        <h3>RDF Linked-List Profiles</h3>
      </section>
      <aside class="ednote"><p>In order to indicate which profile applies to a stream, the
          transmission must include that information in addition to the stream content. For streams
          which are delivered the method is to supply the profile as a <code>profile</code>
          parameter to the media type. The following profiles are defined <ul>
            <li><code>http://www.w3.org/ns/rsp-ql#time-series</code></li>
            <li><code>http://www.w3.org/ns/rsp-ql#distinct-time-series</code></li>
            <li><code>http://www.w3.org/ns/rsp-ql#synchronous-time-series</code></li>
            <li><code>http://www.w3.org/ns/rsp-ql#linked-stream</code></li>
          </ul>
        </p>
        <p>This information shouldn't stay in this document - it probably should go into the Serialization document.</p>
      </aside>
    </section>
    <!-- CONFORMANCE -->
    <section id="conformance"> </section>
  </body>
</html>
