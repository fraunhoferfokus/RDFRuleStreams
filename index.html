<!DOCTYPE html>
<html>
	<head>
		<title>Extension of the SPARQL Abstract Data Model for RDF Stream Processing</title>
		<meta charset="utf-8" />
		<script src="http://www.w3.org/Tools/respec/respec-w3c-common" async="async" class="remove"></script>
		<script class="remove">
      var respecConfig = {
          specStatus: "CG-DRAFT",
          shortName:  "rdf-stream-semantics",
          editors: [
            {
              name:       "Editor 1",
              company:    "Company 1",
              companyURL: "http://example1.com/",
              mailto: "ed1@example1.com"
            },
            {
              name:       "Editor 2",
              company:    "Company 2",
              companyURL: "http://example2.com/",
              mailto: "ed2@example1.com"
            }
          ],
          authors: [
            {
              name:       "Tara Athan",
              company:    "Athan Services",
              companyURL: "http://athant.com",
              mailto: "taraathan@gmail.com"
            },
            {
              name:       "Author 2",
              company:    "Company 2",
              companyURL: "http://example2.com/",
              mailto: "auth2@example1.com"
            }
          ],
          wg:           "RDF Stream Processing Community Group",
          wgURI:        "https://www.w3.org/community/rsp/",
          wgPublicList: "public-rsp",

          localBiblio:  {
            "LINEARCOMPOSITION": {
              title: "Multi-device Linear Composition on the Web: Enabling Multi-device Linear Media with HTMLTimingObject and Shared Motion",
              href: "https://sites.google.com/site/mediasynchronization/Paper4_Arntzen_webComposition_CR.pdf?attredirects=0&amp;d=1",
              authors: [
                "Ingar M. Arntzen",
                "Njål T. Borch",
                "François Daoust",
                "Dominique Hazaël-Massieux"
              ]
            },
            "BLOGIC": {
              title:    "Blogic",
              href:     "http://www.slideshare.net/PatHayes/blogic-iswc-2009-invited-talk",
              authors:  [
                "Pat Hayes"
              ]
            },
            "DUL": {
              title:    "DOLCE+DnS Ultralite (dul)",
              href:     "http://lov.okfn.org/dataset/lov/vocabs/dul",
              authors:  [
                "Aldo Gangemi"
              ]
            },
            "DVB-CSS": {
              title: "ETSI TS 103 256-2 V1.1.1 Digital Video Broadcasting (DVB); Companion Screens and Streams; Part 2: Content Identification and Media Synchronization",
              href: "http://www.etsi.org/modules/mod_StandardSearch/pdf.png"
            },
            "SHAREDMOTION": {
              title: "Shared Motion",
              href: "http://motioncorporation.com"
            },
            "MEDIASYNC":{
              title: "MediaSync",
              href: "https://github.com/webtiming/mediasync"
            },
            "SEQUENCER" : {
              title: "Open-source sequencer library",
              href: "https://github.com/webtiming/sequencer"
            }
          },
          otherLinks: [
            {
              key: "Version history",
              data: [
                {
                  value: "GitHub streamreasoning/RSP-QL/commits",
                  href: "https://github.com/streamreasoning/RSP-QL/commits/"
                }
              ]
            },
            {
              key: "Participate",
              data: [
                {
                  value: "GitHub streamreasoning/RSP-QL",
                  href: "https://github.com/streamreasoning/RSP-QL"
                },
                {
                  value: "File an issue",
                  href: "https://github.com/streamreasoning/RSP-QL/issues/new"
                },
                {
                  value: "Open issues",
                  href: "https://github.com/streamreasoning/RSP-QL/issues/"
                },
                {
                  value: "Mailing-list (public-rsp@w3.org)",
                  href: "https://lists.w3.org/Archives/Public/public-rsp/"
                }
              ]
            }
          ],

          issueBase: "https://github.com/streamreasoning/RSP-QL/issues/",
          githubAPI: "https://api.github.com/repos/streamreasoning/RSP-QL"
      };
    </script>
		<style type="text/css">
			table{
			    border-collapse: collapse;
			    border-style: hidden hidden none hidden;
			}
			table thead,
			table tbody{
			    border-bottom: solid;
			}
			table td,
			table th{
			    border-left: solid;
			    border-right: solid;
			    border-bottom: solid thin;
			    vertical-align: top;
			    padding: 0.2em;
			}</style>
	</head>
	<body>
		<!-- ABSTRACT -->
		<section id="abstract"> </section>
		<!-- STATUS OF DOCUMENT -->
		<section id="sotd">
			<p> The specification is intended for discussion within the RDF Stream Processing
				Community Group. Its content does not yet represent the consensus of the Community
				Group. </p>
			<p class="warning"> This specification is incomplete. </p>
		</section>
		<!-- INTRODUCTION -->
		<section class="informative" id="intro">
			<h2>Introduction</h2>
		</section>
		<section id="basics">
			<h2>The Basics</h2>
			<section id="namespaces-and-prefixes">
				<h3>Namespaces and Prefixes</h3>
				<p class="ednote">Do we need any new namespaces for this document? </p>
				<p>Additional prefixes used in this document are the following: </p>
				<div style="text-align: left;">
					<table class="thinborder" style="margin-left: auto; margin-right: auto;">
						<caption id="namespace-table">Table 1: Prefix and Namespaces used in this
							specification</caption>
						<tbody>
							<tr>
								<td><b>prefix</b></td>
								<td><b>namespace IRI</b></td>
								<td><b>definition</b></td>
							</tr>

							<tr>
								<td>rsp</td>
								<td><code>http://www.w3.org/ns/rsp#</code></td>
								<td>The RSP rnamespace</td>
							</tr>

							<tr>
								<td>rdf</td>
								<td><code>http://www.w3.org/1999/02/22-rdf-syntax-ns#</code></td>
								<td>The RDF namespace [[!RDF-SCHEMA]]</td>
							</tr>

							<tr>
								<td>rdfs</td>
								<td><code>http://www.w3.org/2000/01/rdf-schema#</code></td>
								<td>The RDFS namespace [[!RDF-SCHEMA]]</td>
							</tr>

							<tr>
								<td>xsd</td>
								<td><code>http://www.w3.org/2000/10/XMLSchema#</code></td>
								<td>XML Schema Namespace [[!XMLSCHEMA11-2]]</td>
							</tr>

							<tr>
								<td>owl</td>
								<td><code>http://www.w3.org/2002/07/owl#</code></td>
								<td>The OWL namespace [[!OWL2-OVERVIEW]]</td>
							</tr>

							<tr>
								<td>owl-time</td>
								<td><code>http://www.w3.org/2006/time#</code></td>
								<td>The OWL-TIME namespace [[!OWL-TIME]]</td>
							</tr>

							<tr>
								<td>prov</td>
								<td><code>http://www.w3.org/ns/prov#</code></td>
								<td>The PROV namespace [[!PROV-DM]]</td>
							</tr>

							<tr>
								<td>dul</td>
								<td><code>http://www.ontologydesignpatterns.org/ont/dul/DUL.owl#</code></td>
								<td>The DOLCE+DnS Ultralet namespace
									(http://lov.okfn.org/dataset/lov/vocabs/dul) [[DUL]]</td>
							</tr>

							<tr>
								<td>(others)</td>
								<td>(various)</td>
								<td>All other namespace prefixes are used in examples only. <br />
									In particular, IRIs starting with "http://example.com"
									represent<br /> some application-dependent IRI [[!IRI]]</td>
							</tr>
						</tbody>
					</table>
				</div>
			</section>
			<section id="constraints">
				<h3>Restrictions on SPARQL Query Language</h3> For simplicity, we use here a subset
				of the SPARQL Query language, with the aim to minimize ambiguity of semantics.
				Constraints are: <ol>
					<li>The Sample aggregation operator is not allowed either explicitly or
						implicitly.</li>
				</ol>
			</section>
		</section>
		<section id="data-structures">
			<h2>Data Structures</h2>
			<p>In this section, we review the data structure types defined in other recommendations
				and state the definitions of new data structure types that support the signature
				definitions of new algebra operators.s</p>
			<section id="existing-datastructuretypes">
				<h3>Existing Data Structure Types</h3>
				<p class="ednote">For now, I keep this table here as a summary of definitions and
					links to the recommendations for existing datatypes. In the final document, this
					table will be moved to an appendix, or deleted. </p>
				<p>We make use of the following data structure types from other specifications in
					the RDF and SPARQL family:</p>
				<div style="text-align: left;">
					<table class="thinborder" style="margin-left: auto; margin-right: auto;">
						<caption id="existing-data-structure-types-table"><b>Table A</b>: Existing
							data structure types used in this specification</caption>
						<tbody>
							<tr>
								<td><b>Data Structure Type</b></td>
								<td><b>Reference</b></td>
								<td><b>Definition</b>
								</td>
							</tr>
							<tr>
								<td><dfn>solution mapping</dfn></td>
								<td>[[!SPARQL11-Query]] <a
										href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/#sparqlSolutions"
										>#sparqlSolutions</a></td>
								<td> partial function μ : V -> RDF-T </td>
							</tr>
							<tr>
								<td><dfn>multiset of solution mappings</dfn></td>
								<td>[[!SPARQL11-Query]] <a
										href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/#BasicGraphPattern"
										>#BasicGraphPattern</a></td>
								<td>A multiset is an unordered collection of elements in which each
									element may appear more than once. It is described by a set of
									elements and a cardinality function giving the number of
									occurrences of each element from the set in the multiset. </td>
							</tr>
							<tr>
								<td><dfn>sequence of solution mappings</dfn></td>
								<td>[[!SPARQL11-Query]] <a
										href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/#solutionModifiers"
										>#solutionModifiers</a>
								</td>
								<td>The recommendation does not define the concept of "sequence", so
									it is not normatively limited. The characteristics that are used
									correspond to a totally-ordered set. </td>
							</tr>
							<tr>
								<td><dfn data-lt="RDF triples|triple|triples">RDF triple</dfn></td>
								<td>[[!RDF11-Concepts]] <a
										href="https://www.w3.org/TR/rdf11-concepts/#section-triples"
										>#section-triples</a>
								</td>
								<td>An RDF triple consists of three components: <ul>
										<li>the <dfn id="dfn-subject">subject</dfn>, which is an
												<a><abbr
												title="Internationalized Resource Identifier"
												>IRI</abbr></a> or a <a>blank node</a></li>

										<li>the <dfn id="dfn-predicate">predicate</dfn>, which is an
												<a>IRI</a></li>

										<li>the <dfn id="dfn-object">object</dfn>, which is an
												<a>IRI</a>, a <a>literal</a> or a <a>blank
											node</a></li>
									</ul>
								</td>
							</tr>
							<tr>
								<td><dfn data-lt="RDF Graphs|graph|graphs">RDF Graph</dfn></td>
								<td>[[!RDF11-Concepts]] <a
										href="https://www.w3.org/TR/rdf11-concepts/#section-rdf-graph"
										>#section-rdf-graph</a>
								</td>
								<td>An RDF graph is a set of RDF triples. </td>
							</tr>
							<tr>
								<td><dfn data-lt="RDF Datasets">RDF dataset</dfn></td>
								<td>[[!RDF11-Concepts]] <a
										href="https://www.w3.org/TR/rdf11-concepts/#section-dataset"
										>#section-dataset</a>
								</td>
								<td>An RDF dataset is a collection of <a>RDF graphs</a>, and
									comprises: <ul>
										<li>Exactly one <dfn data-lt="default graphs">default
												graph</dfn>, being an <a>RDF graph</a>. The default
											graph does not have a name and MAY be empty.</li>
										<li>Zero or more <dfn data-lt="named graph">named
												graphs</dfn>. Each named graph is a pair consisting
											of an IRI or a blank node (the <dfn>graph name</dfn>),
											and an <a>RDF graph</a>. Graph names are unique within
											an RDF dataset.</li>
									</ul>
								</td>
							</tr>
							<tr>
								<td><dfn data-lt="RDF Streams">RDF Stream</dfn></td>
								<td><b>Reference</b></td>
								<td><b>Definition</b>
								</td>
							</tr>
							<tr>
								<td><dfn>RDF Surface (sensu Pat Hayes)</dfn></td>
								<td><b>Reference</b></td>
								<td><b>Definition</b>
								</td>
							</tr>
						</tbody>
					</table>
				</div>
				<p class="note">In mathematics, the "cardinality" (typically called "multiplicity")
					mapping of multisets is generally assumed to range over non-negative integers.
					The cardinality of a multiset is, typically, defined as the sum of the
					multiplicities of all its elements. </p>
				<p class="ednote">This information may or may not be relevant: Note that not all
					sequences can be converted (by forgetting the order) to multisets with finite
					cardinality; some sequences would correspond to multisets with countable
					cardinality (where cardinality of an element may be a non-negative integer or
					the cardinal number aleph-nought). All multisets with countable cardinality can
					be expressed as sequences. However, not all multisets with countable cardinality
					can be expressed as ordered sequences. SPARQL implicitly assumes sequences of
					solution mappings may be ordered, and hence we may suppose that the intention is
					to use only sequences with finite replication of any value, corresponding to
					multisets with finite cardinality when order is forgotten. </p>
			</section>
			<section id="def-datasets">
				<h3>RDF Datasets</h3>
				<section id="def-rdf-dataset">
					<h3>RDF Dataset</h3>
					<p>Recall that an <a>RDF dataset</a> is a collection of <a>RDF graphs</a>, and
						comprises:</p>
					<ul>
						<li>Exactly one <a>default graph</a>.</li>
						<li>Zero or more <a>named graphs</a>.</li>
					</ul> We generalize this concept below to accommodate more complex structures. </section>
				<section id="def-kth-order-dataset">
					<h4>k-th order RDF Dataset</h4>
					<p>A <dfn
							data-lt="k-th Order RDF Datasets|k-th Order Dataset|k-th Order Datasets|Dataset|Datasets"
							>k-th Order RDF Dataset</dfn> is defined inductively as follows. </p>
					<p> A Zeroth-Order RDF Dataset is an RDF Graph. </p>
					<p>For k = 0, 1, ... , a (k+1)-th Order RDF Dataset is a collection of k-th
						Order RDF Datasets composed of a distinguished k-th Order RDF Dataset,
						called the <dfn data-lt="default datasets">default dataset</dfn>, and a set
						of <a>Named k-th Order RDF Datasets</a>. The names of the k-th Order
						Datasets are unique within the k+1-th Order Dataset. The set of <a>k-th
							order dataset</a> names of a k+1-th order dataset D is denoted by
						datasetNames(D). </p>
					<p class="note"> A first-Order RDF Dataset is an RDF Dataset according to the
						definition in [[!RDF11-Concepts]]. </p>
					<p class="note"> Hereafter in this document, "dataset", without modification,
						means a <a>k-th Order RDF Dataset</a> where k is not specified. When the
						original concept of RDF Dataset is intended, we use the term "first-order
						dataset" for clarity. </p>
					<p class="ednote">To be clarified in a separate semantics section: just as there
						is not a single recommended semantics for RDF Datasets, it is important to
						be inclusive regarding possible semantics for k-th Order RDF Datasets. An
						important consideration is whether the component Named Datasets lie on a
						single <a>RDF Surface (sensu Pat Hayes)</a>, i.e. do they share blank nodes.
					</p>
				</section>
				<section id="def-named-kth-order-dataset">
					<h4>Named k-th order RDF Dataset</h4>
					<p>A <dfn
							data-lt="Named k-th Order RDF Datasets|Named Dataset|Named Datasets|Named k-th Order Dataset|Named k-th Order Datasets"
							>Named k-th Order RDF Dataset</dfn> is an ordered pair [n, D] of an IRI
						or blank node n that is the <a>k-th Order RDF Dataset</a> name, and <a>k-th
							Order RDF Dataset</a> D. </p>
					<p class="note"> Hereafter in this document, <dfn>unnamed or named
						dataset</dfn>, without modification, means either a <a>k-th Order RDF
							Dataset</a> or a <a>named k-th Order RDF Dataset</a> where k is not
						specified. </p>
				</section>
				<section id="def-data-sequences-structures">
					<h3>RDF Data Sequences and Structures</h3>
					<section id="def-data-sequences">
						<h4>RDF Data Sequences</h4>
						<p>An <dfn data-lt="RDF Data Sequences|Data Sequence|Data Sequences">RDF
								Data Sequence</dfn> is a <a>sequence</a> of <a>unnamed or named data
								structures</a>.</p>
						<p>In this document, we use the term <dfn>sequence</dfn> in the sense of a
							totally-ordered set that can be strictly monotonically mapped onto the
							integers. I.e., it is isomorphic, as a totally-ordered set, to a subset
							of the integers. A sequence may be finite, or infinite. If infinite, the
							set MAY or MAY NOT have a least element, and MAY or MAY NOT have a
							greatest element. If a sequence has neither least nor greatest element,
							then it is said to be <dfn>bi-infinite</dfn>.</p>
						<p>We define a number of syntactic operations on <a>data sequences</a> as
							follows. Let DSQ be a <a>data sequence</a>. </p>
						<ul>
							<li>If DST is an element of DSQ, then <dfn>isMemberOf</dfn>(DST, DSQ) is
								@@@true; otherwise, it is false.</li>
							<li>If DST and DST' are two elements of DSQ, then
								<dfn>previousTo</dfn>(DST, DST'; DSQ) is defined; otherwise it is
								undefined. It has the value @@@true if DST &lt; DST' in the context
								of DSQ, and has the value @@@false if DST &gt;= DST' in the context
								of DSQ. </li>
							<li>If <a>previousTo</a>(DST, DST'; DSQ), then <dfn>follows</dfn>(DST,
								DST'; DSQ) is defined; otherwise it is undefined. It has the value
								@@@true if DST &gt; DST' in the context of DSQ, and has the value
								@@@false if DST &lt;= DST' in the context of DSQ. </li>
							<li>If DSQ has a least element then <dfn>hasLeast</dfn>(DSQ) has the
								value to the Boolean literal <code>'true'^^&lt;@@@&gt;</code>, and
								otherwise <code>'false'^^&lt;@@@&gt;</code>
							</li>
							<li>If DSQ has a greatest element then <dfn>hasGreatest</dfn>(DSQ) has
								the value <code>'true'^^&lt;@@@&gt;</code>, and otherwise
									<code>'false'^^&lt;@@@&gt;</code>. </li>
							<li>If <a>hasLeast</a>(DSQ) is true, then <dfn>least</dfn>(DSQ) is
								defined, and has the value of the least element of DSQ. Otherwise it
								is undefined. </li>
							<li>If <a>hasGreatest</a>(DSQ) is true, then <dfn>greatest</dfn>(DSQ) is
								defined, and has the value of the greatest element of DSQ. Otherwise
								it is undefined. </li>
							<li>If <a>least</a>(DSQ) = DST, then <dfn>isLeastOf</dfn>(DST, DSQ) has
								value @@@true, and otherwise @@@false. </li>
							<li>If <a>greatest</a>(DSQ) = DST, then <dfn>isGreatestOf</dfn>(DST,
								DSQ) has value @@@true, and otherwise @@@false. </li>
							<li>If <a>isLeastOf</a>(DST, DSG) is false, then
								<dfn>predecessor</dfn>(DST; DSQ) is defined, and has the value of
								the unique <a>predecessor</a> of DST in the context of DSQ. I.e., if
								predecessor(DST; DSQ) = DST', then (in the context of DSQ) DST' &lt;
								DST, and for any DST'' where DST'' &lt; DST, it holds that DST''
								&lt;= DST'. </li>
							<li>If <a>isGreatestOf</a>(DST, DSQ) is false, then
								<dfn>successor</dfn>(DST; DSQ) is defined, and has the value of the
								unique <a>successor</a> of DST in the context of DSQ. I.e., if
								successor(DST; DSQ) = DST', then (in the context of DSQ) DST' &gt;
								DST, and for any DST'' where DST'' &gt; DST, it holds that DST''
								&gt;= DST'. </li>
							<li></li>
						</ul>
						<p class="ednote">Add to table of structural inferences: rdfsq:isMemberOf,
							rdfsq:previousTo, rdfsq:follows, rdfsq:hasLeast, rdfsq:hasGreatest,
							rdfsq:least, rdfsq:greatest, rdfsq:isLeastOf, rdfsq:isGreatestOf,
							rdfsq:predecessor, rdfsq:successor, These relationships can be inferred,
							but only within the context of a particular sequence. The order is not
							an inherent relationship between the two data structures. That means
							that if this information is materialized, it must be done, in a number
							of cases, as a quad, to indicate the context in which it holds. The
							graph name in the quad may or may not "denote" the sequence, depending
							on the semantics that is in play. The sequence does not need to be named
							explicitly, but can be indicated by a blank node, (again, provided that
							makes sense within the active semantics). If the sequence is a named
							sequence, then whether it is appropriate to use that name to materialize
							such statements depends on the active semantics.</p>
					</section>
					<section id="def-data-structures">
						<h4>RDF Data Structures</h4>
						<p>An <dfn>RDF Data Structure</dfn> is a collection of <a>unnamed or named
								data structures</a> comprised of: </p>
						<ul>
							<li>an optional distinguished <a>data structure</a>, called the
									<dfn>unordered component</dfn> of the data structure.</li>
							<li>an optional <a>data sequence</a>, called the <dfn>ordered
									component</dfn> of the data structure. </li>
						</ul>
						<p>The function <dfn>Unordered</dfn> is defined on all <a>data
								structures</a> that have an unordered component, and has the value
							of the unordered component; otherwise, it is undefined. If Unordered(DS)
							= DS, then we say the data structure is an <dfn>unordered data
								structure</dfn>. </p>
						<p>The function <dfn>Ordered</dfn> is defined on all <a>data structures</a>
							that have an ordered component, and its value is the ordered component;
							otherwise it is undefined. If Ordered(DSO = DS, then we say the data
							structure is a <dfn>fully-ordered data structure</dfn>.</p>
					</section>
					<section id="def-k-th-data-structure">
						<h4>k-th Order RDF Data Structure</h4>
						<p>A <dfn
								data-lt="k-th Order RDF Data Structures|k-th Order Data Structure|k-th Order Data Structures|Order Data Structure|Order Data Structures"
								>k-th Order RDF Data Structure</dfn> is a <a>data structure</a> DS
							satisfying the following. Let DS be the k-th order data structure. </p>
						<ul>
							<li>If DS has an unordered component, it is a collection of (k-1)-th
									<a>order data structures</a> composed of: <ul>
									<li> a distinguished (k-1)-th <a>order data structure</a>,
										called the <dfn>default component</dfn> of the data
										structure, denoted <dfn>Default</dfn>(DS) or equivalently
										Default(Unordered(DS)). </li>
									<li> zero or many named (k-1)-th <a>order data structures</a>.
										The set of names is denoted <dfn>structureNames</dfn>(DS) or
										equivalently structureNames(Unordered(DS)) </li>
								</ul>
							</li>
							<li>The elements of its ordered component, Ordered(DS), are k-th order
									<a>data structures</a>.</li>
						</ul>
					</section>
				</section>
				<section id="def-repositories">
					<h3>RDF Repositories</h3>
					<section id="def-kth-order-rdf-repository">
						<h4>k-th Order RDF Repositories</h4>
						<p>A <dfn
								data-lt="k-th Order RDF Repositories|k-th Order Repository|k-th Order Repositories|RDF Repository|RDF Repositories|repository|repositories"
								>k-th Order RDF Repository</dfn> is an <a>RDF data structure</a>
							that is defined by cases: </p>
						<ol>
							<li>a zeroth Order RDF Repository has an RDF Graph for its unordered
								component and the empty sequence for its ordered component. </li>
							<li>k+1-th Order RDF Repository, k = 0, 1, ..., has a <a>k-th order
									dataset</a> for its unordered component and a <a>data
									sequence</a> of <a>named k-th order datasets</a> for its ordered
								component. </li>
						</ol>
						<p class="note">A k-th Order RDF Repository may be viewed as a k-th Order
							RDF Dataset with the additional structure of a total order on its
							component set of <a>named datasets</a>.</p>
						<p>Each element of the ordered component is called a <dfn>version</dfn> of
							the repository. </p>
						<p>We say a version U is a <dfn>previous version</dfn> of version V if U
							&lt; V in the context of the repository, and V is called a
								<dfn>following version</dfn> of U.</p>
						<p>If U is the <a>predecessor</a> of version V then U is called the
								<dfn>preceding version</dfn> of V (V is the <dfn>succeeding
								version</dfn> of U) in the context of the repository.</p>
						<p>An <dfn>initial version</dfn> is a version that does not have a previous
							version. The ordered component of a repository MAY have an initial
							version.</p>
						<p class="note">If the ordered component of a repositoriy has an initial
							version, then it is unique, due to the order being total; further, it is
							a minimal element. </p>
						<p>Every version that is not the initial version has a unique preceding
							version, and we say that each such version <dfn>revises</dfn> its
							preceding version. </p>
						<p>A <dfn>terminal version</dfn> is a version that does not have a following
							version. The ordered component of a repository MAY have a terminal
							version.</p>
						<p class="note">If the ordered component of a repositoriy has a terminal
							version, it is unique, due to the order being total, and is a maximal
							element. </p>
						<p class="note">Every version that is not the terminal version has a unique
							succeding version. </p>
						<p class="note">Because of the requirement for total order in the ordered
							component, it is not possible to have version structures which
							branch.</p>
						<p> The ordered component MAY be finite, semi-infinite, or bi-infinite.</p>
						<p>The set of version names of a repository R is denoted by
								<dfn>versionNames</dfn>(R). </p>
						<p class="ednote">To be clarified in the section on the extension of Eval to
							repositories: there is an entailment regime, called e.g. "provenance
							entailment regime", that has additional entailments beyond those of the
							simple entailment regimes in that each non-initial version is related to
							its previous version by the <a
								href="https://www.w3.org/TR/2012/WD-prov-o-20120503/#wasRevisionOf"
								>prov:wasRevisionOf</a> property. </p>
						<p class="ednote">To be clarified in the section on the extension of Eval to
							repositories: An objective for the provenance entailment regime is that
							it be monotonic with respect to repository updates. From this
							perspective, it would also be ok to include entailment regarding the
							initial version. Therefore, we define a new property
								<code>repo:isInitialVersion</code>. </p>
						<p class="ednote">This is not yet a sufficient analysis of the similarities
							and differences between a second-order repository and a Graph Store.
							Move to Examples section? A second-order <a>RDF Repository</a> is
							similar to a Graph Store, but with the additional structure that every
							version is named (albeit the name may be a blank node). </p>
						<p class="ednote">Needs more explanation, examples: Where Update Operations
							are conceived as functional specifications for transitions of the state
							of a Graph Store, for a RDF Repository these operations play the role of
							helper functions used in the "lazy" construction of the RDF Repository. </p>
						<p class="ednote">Needs more explanation, examples: To use a second-order
								<a>repository</a> to emulate a <a>Graph Store</a>, the input for
							query and update operations would have to be limited to some "current
								<a>version</a>", and the <a>unordered dataset</a> of the repository
							would be empty. The execution of an <a>Update operation</a> would move
							the "current <a>version</a> pointer" to the <a>succeeding version</a>.
						</p>
					</section>
					<section id="def-named-kth-order-repository">
						<h4>Named k-th Order RDF Repository</h4>
						<p>A <dfn
								data-lt="Named k-th Order RDF Repositories|Named RDF Repository|Named RDF Repositories|Named Repository|Named Repositories"
								>Named k-th Order RDF Repository</dfn> is an ordered pair [n, R] of
							an IRI or blank node n that is the <a>RDF Repository</a> name, and
								<a>k-th Order RDF Repository</a> R. </p>
						<p class="ednote">To be clarified in the section on the extension of Eval on
							named repositories: Under the provenance entailment regime a <a>named
								RDF repository</a> has additional entailments that each version is
							related to the parent RDF Repository by the <a
								href="https://www.w3.org/TR/2012/WD-prov-o-20120503/#specializationOf"
								>prov:specializationOf</a> property. </p>
						<p class="ednote">To be clarified in the section on the extension of Eval to
							named repositories: For a named repository, e.g. within a store, the
							provenance entailment regime also includes an additional property,
								<code>repo:isInitialVersionOf</code>, with object being the name of
							the repository. A new namespace (prefix "repo") needs to be defined.
							These properties can be used to define operations on <a>data
								sequences</a> and <a>data structures</a> that extract parts of the
							sequence (e.g. foot(initial version), and subsequences related to that,
							such as the first 10 versions) that are monotonic relative to update
							operations. </p>
						<p class="ednote">To be clarified in the section on the extension of Eval to
							repositories and named repositories: a "repository structure entailment
							regime" could be defined as an extension of the monotonic provenance
							entailment regime, relaxing the requirement for monotonicity with
							respect to update operations. This regime could then include entailments
							regarding the terminal version, using
								<code>repo:isTerminalVersion</code> and
								<code>repo:isTerminalVersionOf</code>. These properties could be
							used as the basis for basic operations on <a>data sequences</a> and
								<a>data structures</a> that extract parts of the sequence, e.g.
							head, tail and subsequences related to these) that are nonmonotonic
							relative to update operations). </p>
					</section>
				</section>
				<section id="def-stores">
					<h3>Stores</h3>
					<section id="def-kth-order-store">
						<h4>k-th Order RDF Store</h4>
						<p>A <dfn
								data-lt="k-th Order RDF Stores|k-th Order Store|k-th Order Stores|RDF Store|RDF Stores|Store|Stores"
								>k-th Order RDF Store</dfn> is defined by cases: </p>
						<ol>
							<li>A zeroth Order RDF Store is an RDF Graph. </li>
							<li>For k=0, 1, ..., a k+1-th Order RDF Store is a k+1-th Order RDF
								Repository whose component k-th Order RDF Datasets have the
								additionl structure of RDF Repositories. The history of each member
								k-th Order RDF Repository is a restriction of the RDF Store history
								to that RDF Repository. </li>
						</ol>
						<p class="ednote">We need to define a restriction of the history of a k+1-th
							Order RDF Repository to a member k-th Order RDF Repository. In
							particular, we need to take care to remove adjacent versions that have
							identical content. </p>
						<p class="ednote">The intended semantics, under any entailment regime, of a
							k+1-th Order <a>RDF Store</a> is derived from that of the corresponding
							k+1-th Order RDF Repository, supplemented by that of its member k-th
							Order Repositories, possibly augmented (e.g. in the provenance regime)
							by entailments that link the two. </p>
					</section>
					<section id="def-kth-order-rdf-recursive-store">
						<h4>k-th Order RDF Recursive Store</h4>
						<p>A <dfn
								data-lt="k-th Order RDF Recursive Stores|k-th Order Recursive Store|k-th Order Recursive Stores|RDF Recursive Store|RDF Recursive Stores|Recursive Store|Recursive Stores"
								>k-th Order RDF Recursive Store</dfn> is defined by cases: </p>
						<ol>
							<li>A zeroth Order RDF Recursive Store is an RDF Graph. </li>
							<li>For k=0, 1, ..., a k+1-th Order RDF Recursive Store is a k+1-th
								Order RDF Repository whose component k-th Order RDF Datasets have
								the additionl structure of RDF Recursive Stores. </li>
						</ol>
						<p class="ednote">The intended semantics, under any entailment regime, of a
							k+1-th Order <a>RDF Recursive Store</a> is derived from that of the
							corresponding k+1-th Order RDF Store, supplemented by that of its member
							k-th Order RDF Store, possibly augmented (e.g. in the provenance regime)
							by entailments that link the two. </p>
					</section>
					<section id="def-named-rdf-recursive-store">
						<h4>Named RDF Recursive Store</h4>
						<p>A <dfn
								data-lt="Named RDF Recursive Stores|Named Recursive Store|Named Recursive Stores"
								>Named RDF Recursive Store</dfn> is an ordered pair [n, S] of an IRI
							n that is the <a>Store</a> name, and <a>RDF Recursive Store</a> S. </p>
						<p class="ednote">We do need this type for the components of Recursive
							Stores. Also at the top level, a Recursive Store, or even the
							non-recursive version, may be named. The effect is that in the
							provenance entailment regime there are entailments of the property
							prov:isSpecializationOf, which require the parent Store to have an
							identifier. </p>
					</section>
				</section>
				<section id="def-streams">
					<h3>Streams</h3>
				</section>
				<!--
		<section id="def-term-segment">
          <h4>Term Segment</h4>
		  <p>The <dfn data-lt="Term Segments">Term Segment</dfn> (of a Store) is the mutable (write-only) set of 
		       the RDF Terms known at a given time to appear 
				or have appeared in the Named Repositories Store 
				</a>.
		  </p>	
		  <p class="ednote">The Term Segment includes terms that might not currently appear in any statement in the current Revision of any 
				RDF Repository in the Store.
		  </p>
		  <p class="ednote">Do we need this term?it appears to be connected with the iterator over the history of a Repository,
		  which does not appear to be relevant for the abstract data model.
		  </p>		  
		</section>-->
				<!--
		<section id="def-term">
          <h4>The Term</h4>
		  <p>A <dfn data-lt="the terms">the term</dfn> is a (supertype) where (specialization)</a>.
		  </p>	
		  <p class="ednote">The intended semantics of a <a>the term</a> is (semantics)
		  </p>
		</section>
		<section id="def-property">
          <h4>the property</h4>
		  <p>A <dfn data-lt="the properties">the property</dfn> (of a domain type) is a (range type) where (requirements)</a>.
		  </p>	
		  <p class="ednote">This property is a subproperty of @@@
		  </p>
		</section>		-->
				<div style="text-align: left;">
					<table class="thinborder" style="margin-left: auto; margin-right: auto;">
						<caption id="new-datatypes-discarded-table"><b>Table B</b>: New datatypes
							considered for this specification but probably not needed</caption>
						<tbody>
							<tr>
								<td><b>Name</b></td>
								<td><b>Definition</b>
								</td>
							</tr>
							<!-- <tr>
			    <td>Store</td>
				<td>structure on a fixed RDF Surface (sensu Pat Hayes).
				This Surface contains all the RDF Terms that (ever) appear in the Term Segment. 
				The structure is as a set R of Named Repositories and a default RDF Repository. 
				Every Store has an open world assumption in that at any point in time we may not know all the 
				Repositories in it and all the Terms in it.
				</td>
			  </tr>              
			  <tr>
			    <td>Named RDF Recursive Store</td>
				<td>ordered pair [n, S] of an IRI n that is the Store name, and Store S.
				</td>
			  </tr>   -->
							<tr>
								<td>Term Segment (of a Store)</td>
								<td>mutable (write-only) set of the RDF Terms known at a given time
									to appear or have appeared in the Named Repositories of the
									Store (i.e includes terms that might not currently appear in any
									statement in the current Revision of any RDF Repository
									currently in the Store) </td>
							</tr>
							<tr>
								<td>version (of a RDF Repository)</td>
								<td>relationship between the RDF Repository and each of the elements
									in its sequence of Named RDF Datsets. A subproperty of
									prov:specializationOf. The "versionOf" relationship can be
									inferred from a prov:specializationOf, together with
									participation in a prov:wasRevisionOf relationship with another
									specialization of the same RDF Repository. Therefore it is not
									necessary to mint this subproperty. </td>
							</tr>
							<tr>
								<td>RDF Databag</td>
								<td>structure consisting of an RDF Dataset D and a cardinality
									mapping from the set of triples and quads in D into positive
									integers. For an RDF Databag B, the dataset is data[B] and the
									cardinality mapping is card[B]. </td>
							</tr>
							<tr>
								<td>Named RDF Databag</td>
								<td>ordered pair [n, B] of an IRI that is the name of the RDF
									Databag, and RDF Databag B. </td>
							</tr>
							<tr>
								<td>Multirepository</td>
								<td>like a Repository, but with RDF Databags instead of RDF
									Datasets. (move to a note) Explanation: The purpose of the
									additional types which incorporate cardinality is to accommodate
									a representation of an arbitrary RDF Stream with a "cumulative"
									strategy (just making this name up on the spot). This strategy
									is what I am using in the current experimental repository, where
									each new timestamped graph of the stream is inserted into the
									repository that holds its representation, and never deleted. The
									cardinality is needed for the case when the same triple, or
									quad, appears in more than one element of the stream, which is
									allowed in the most general RDF stream. This strategy was the
									best match to the visibility mapping where the default is
									persistence (if no value is given for run length, then it is
									assumed to be present from then on, and never deleted). An
									alternative strategy would be to use a "noncumulataive"
									strategy, where the repository is "cleared" (without deleting
									the history) when a new stream element arrives, and the new
									stream element becomes the repository content. The visibility
									map that is the best match to this case is where the default is
									a runlength of 1. This strategy would not need the additional
									types with cardinality, making the abstract data model more
									parsimonious. </td>
							</tr>
							<tr>
								<td>Named Multirepository</td>
								<td>ordered pair [n, M] of an IRI n that is the Multirepository
									name, and Multirepository M. </td>
							</tr>
							<tr>
								<td>Multistore</td>
								<td>like a Store, but with Multirepositories instead of
									Repositories. </td>
							</tr>
						</tbody>
					</table>
				</div>
			</section>
		</section>
		<section id="abstract-syntactic-categories">
			<h2>Abstract Syntactic Categories</h2>
			<p>In this section, we review the abstract syntactic categories defined in other
				recommendations and state the definitions of new abstract syntactic categories that
				support the signature definitions of new algebra operators.</p>
			<section id="existing-abstract-syntactic-categories">
				<h3>Existing Abstract Syntactic Categories</h3>
				<p class="ednote">For now, I keep this table here as a summary of definitions and
					links to the recommendations for existing datatypes. In the final document, this
					table will be moved to an appendix, or deleted. </p>
				<p>We make use of the following data structure types from other specifications in
					the RDF and SPARQL family:</p>
				<div style="text-align: left;">
					<table class="thinborder" style="margin-left: auto; margin-right: auto;">
						<caption id="existing-abstract-syntactic-categories-table"><b>Table C</b>:
							Existing abstract syntactic categories used in this specification,
							excluding data structures</caption>
						<tbody>
							<tr>
								<td><b>Syntactic Category</b></td>
								<td><b>Reference</b></td>
								<td><b>Definition</b>
								</td>
							</tr>
							<tr>
								<td><dfn data-lt="RDF Terms">RDF-T</dfn></td>
								<td>[[!SPARQL11-Query]] <a
										href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/#sparqlBasicTerms"
										>#sparqlBasicTerms</a></td>
								<td> Let <dfn>I</dfn> be the set of all <a>IRIs</a>. Let
										<dfn>RDF-L</dfn> be the set of all <a>RDF Literals</a> Let
										<dfn>RDF-B</dfn> be the set of all <a>blank nodes</a> in
										<a>RDF graphs</a>. The set of RDF Terms, RDF-T, is I ∪ RDF-L
									∪ RDF-B. </td>
							</tr>
							<tr>
								<td><dfn>V</dfn></td>
								<td>[[!SPARQL11-Query]] <a
										href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/#sparqlQueryVariables"
										>#sparqlQueryVariables</a></td>
								<td>A query variable is a member of the set V where V is infinite
									and disjoint from RDF-T. </td>
							</tr>
							<tr>
								<td><dfn>expression</dfn></td>
								<td><b>Reference</b></td>
								<td><b>Definition</b>
								</td>
							</tr>
							<tr>
								<td><dfn>Graph Pattern</dfn></td>
								<td><b>Reference</b></td>
								<td><b>Definition</b>
								</td>
							</tr>
							<tr>
								<td><dfn>Quadpattern</dfn> or <dfn>GraphGraphPattern</dfn></td>
								<td> [[!SPARQL11-Update]] [[!SPARQL11-Query]] <a
										href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/#sparqlTranslateGraphPatterns"
										>#sparqlTranslateGraphPatterns</a>
								</td>
								<td><b>Definition</b>
								</td>
							</tr>
							<tr>
								<td><dfn>Query</dfn></td>
								<td><b>Reference</b></td>
								<td><b>Definition</b>
								</td>
							</tr>
						</tbody>
					</table>
				</div>
			</section>
			<section id="patterns">
				<h3>Patterns</h3>
				<section id="kth-order-pattern">
					<h4>k-th Order Pattern</h4>
					<p>A <dfn data-lt="k-th Order patterns|pattern|patterns">k-th Order
							pattern</dfn> is @@@ </p>
				</section>
			</section>
		</section>
		<section id="abstract-algebra">
			<h2>Abstract Algebra Operations</h2>
			<section id="def-existing-query">
				<h3>SPARQL Query Abstract Algebra</h3>
				<section id="def-existing-query-eval">
					<h4>Eval</h4>
					<p>We define an extension of the signature, and the corresponding semantics, of
						the "eval" operation of [[!SPARQL11-Query]] that accommodates two
						styles:</p>
					<ul>
						<li>A syntactic style that uses reserved words to modify data structures,
							e.g. by casting to different types, extracting substructures or applying
							window operations.</li>
						<li>A semantic style that uses naming conventions within the IRI, e.g. with
							query strings, to indicate such modifications of the data
							structures.</li>
					</ul>
					<p> In either case, it is necessary that the transformation to the abstract
						algebra does not depend on any details of the IRI; i.e., IRIs are still
						opaque. </p>
					<p>However, it is acceptable, even typical, that the <em>semantics</em> of the
						abstract operations depend on the datatype of the arguments. </p>
					<p> Thus, we extend the binary signature of the eval function from its
						definition in [[!SPARQL11-Query]] of RDF Dataset x expression, to take a
						data structure of the various types defined in Section <a
							href="#data-structures"></a>, as first argument and an extended pattern,
						as defined in Section <a href="#patterns"></a> as second argument. </p>
					<p>All of the newly-defined data structures can be considered as specializations
						of an abstract data structure having an unordered (set-based) component and
						a totally-ordered (sequence-based) component. This same abstraction holds
						for:</p>
					<ul>
						<li>data strucutures with triples as the fundamental type (datasets,
							repositories, stores, streams)</li>
						<li>data structures with solution mappings as the fundamental type
							(structured results)</li>
					</ul>
					<p> The semantics of eval are based on matching the pattern to the ordered
						component of the data structure, with the unordered component unioned into
						every element, to obtain the ordered component of the structured result, and
						also matching the same pattern to the unordered component of the data
						structure to get the unordered component of the structured result. </p>
					<p>In the semantic approach, the implementer defines the naming convention of
						the query string to denote substructures of the parent data structure, e.g
						just the unordered component, just the ordered component, just the terminal
						element of the ordered component, or structures derived from the parent
						structure, e.g. the ordered component with the unordered component unioned
						into every element, or the result of a window operation, where subsequences
						from the ordered component are unioned to obtain a new sequence. </p>
					<p>In the syntactic approach, abstract algebra operations are employed to
						express these transformations and associate them with variables or names,
						with a syntactic transformation to and from the concrete syntax.
						<!--The motivation for the syntactic approach is to allow the same capability for transformation as the semantic approach in regard to external and legacy sources.-->
					</p>
				</section>
			</section>
			<section id="def-existing-basic">
				<h3>Basic Operations</h3>
				<section id="def-existing-basic-dataset-union">
					<h4>Dataset-UNION</h4>
					<p><dfn>Dataset-UNION</dfn> is defined in [[!SPARQL11-Update]] as a basic
						operation with the following signature:</p>
					<ul>
						<li>On the domain RDF Dataset x RDF Dataset, the range is RDF Dataset.</li>
					</ul>
					<p>The semantics of <a>Dataset-UNION</a> is defined in [[!SPARQL11-Update]] as
						follows.</p>
					<p> Let DS={DG} union {(iri<sub>i</sub>, G<sub>i</sub>) | 1 ≤ i ≤ n} and DS' =
						{DG'} union {(iri'<sub>j</sub>, G'<sub>j</sub>) | 1 ≤ j ≤ m} be two RDF
						Datasets. Let further graphNames(DS) = { iri<sub>i</sub> | 1 ≤ i ≤ n} and
						graphNames(DS') = {iri'<sub>j</sub> | 1 ≤ j ≤ m}. The Dataset-UNION between
						DS and DS' is defined as follows: </p>
					<p>Dataset-UNION(DS, DS') = {DG union DG'} union {(iri, G) | iri in
						graphNames(DS) union graphNames(DS')}</p>
					<p>and G defined as </p>
					<ul>
						<li>G<sub>i</sub> for iri = iri<sub>i</sub> such that iri<sub>i</sub> in
							graphNames(DS) minus graphNames(DS')</li>
						<li>G<sub>j</sub> for iri = iri'<sub>j</sub> such that iri<sub>j</sub> in
							graphNames(DS') minus graphNames(DS)</li>
						<li>G<sub>i</sub> union G<sub>j</sub> for iri = iri<sub>i</sub> =
								iri'<sub>j</sub> in graphNames(DS) intersect graphNames(DS')</li>
					</ul>
					<p>where union between <a>graphs</a> is defined as set-union of <a>triples</a>
						in those <a>graphs</a>.</p>
					<p>The Dataset-UNION operation constructs a new (first-order) <a>RDF Dataset</a>
						from two existing (first-order) <a>RDF Datasets</a> by forming the union of
						the <a>default graphs</a>, and taking the union of the sets of <a>named
							graphs</a>, with the exception that if the sets of <a>graph names</a> of
						the two (first-order) <a>RDF datasets</a> have a non-empty intersection, the
							<a>graphs</a> with names in that intersection must be combined by set
						union. </p>
					<p> We extend the Dataset-UNION operation to <a>k-th Order RDF Datasets</a>.
						Also, the names are extended from IRIs to also allow <a>blank node</a>
						names. </p>
					<ul>
						<li>On the domain <a>k-th order dataset</a> x k'-th order <a>dataset</a>,
							the range is max(k, k')-th order <a>dataset</a>.</li>
					</ul>
					<p>The extended semantics of <a>Dataset-UNION</a> is defined by cases as
						follows.</p>
					<p>In the case that both arguments are zeroth-order <a>datasets</a>, i.e.,
							<a>RDF Graphs</a>, then the result is the union of the sets of
							<a>triples</a> in the two <a>graphs</a>. </p>
					<p>In the case that both arguments, DSP and DSP', are k+1-th order
							<a>datasets</a>, let DSP = {DSC} ∪ {(name<sub>i</sub>, DSC<sub>i</sub>)
						| 1 ≤ i ≤ n} and datasetNames(DSP) = { name<sub>i</sub> | 1 ≤ i ≤ n}, and
						similarly for DSP', where DSC and DSC<sub>i</sub> are <a>k-th Order RDF
							Datasets</a>, and name and name<sub>i</sub> are IRIs or blank nodes (I ∪
						RDF-B). Then</p>
					<p> Dataset-UNION(DSP, DSP') = Dataset-UNION(DSC , DSC') ∪ {(iri, DS) | iri in
						datasetNames(DSP) ∪ datasetNames(DSP')}</p>
					<p>and DS defined as </p>
					<ul>
						<li>DSC<sub>i</sub> for iri = iri<sub>i</sub> such that iri<sub>i</sub> in
							datasetNames(DSP) minus datasetNames(DSP')</li>
						<li>DSC'<sub>j</sub> for iri = iri'<sub>j</sub> such that iri<sub>j</sub> in
							datasetNames(DSP') minus datasetNames(DSP)</li>
						<li>Dataset-UNION(DSC<sub>i</sub> , DSC'<sub>j</sub>) for iri =
								iri<sub>i</sub> = iri'<sub>j</sub> in datasetNames(DSP) intersect
							datasetNames(DSP')</li>
					</ul>
					<p>In the case that k and k' are different, let k'' = max(k, k'). Then
						Dataset-UNION(DS, DS') = Dataset-UNION(Dataset-ELEVATE(DS, k''),
						Dataset-ELEVATE(DS', k'')). </p>
					<p class="ednote">Define the basic operation Dataset-ELEVATE to lift a k'-th
						order <a>dataset</a> to a k-th order <a>dataset</a>, with k' &lt;= k. In the
						case that k' &lt; k, then it will be embedded in the <a>default dataset</a>.
						The definition can be recursive, with base case of k = k' + 1, and the input
							<a>dataset</a> is the <a>default dataset</a> of the output, and the
						output contains no <a>named datasets</a>. When k=k', then the operation
						returns the input <a>dataset</a>. </p>
					<p class="ednote"> It is possible to informally apply the Dataset-UNION
						operation to pairs of <a>repositories</a> or <a>stores</a> by first applying
						a forgetful functor that retains all the versions but forgets the order
						among them. However, the result is not a <a>repository</a> or <a>store</a>,
						but is just a <a>dataset</a>. To truly extend the UNION operation to
							<a>repositories</a> and <a>stores</a> to generate new
							<a>repositories</a> and <a>stores</a>, it would be necessary to mint new
						version names and to specify how versions are ordered and combined when
						necessary e.g. according to timestamps, but timestamps are not mandatory in
						our definitions There is no precedent for such definitions, since the Update
						spec does not define a union operation on Graph Stores. </p>
				</section>
				<section>
					<h4>Other Basic Operations</h4>
					<p class="ednote">Similarly, the basic operations of Dataset-DIFF and Dataset
						will be extended to <a>k-th order datasets</a>. The Dataset operation will
						also be extended to <a>k-th order patterns</a>. </p>
				</section>
			</section>
			<section id="def-existing-update">
				<h3>SPARQL Update Operations</h3>
				<section id="def-existing-update-insert-data">
					<h4>Insert Data</h4>
					<p><dfn>Insert Data</dfn> is defined in [[!SPARQL11-Update]] as an Update
						Operation with the following signature:</p>
					<ul>
						<li>On the domain RDF Dataset x IRI, the range is RDF Dataset.</li>
					</ul>
					<p>The semantics of <a>Insert Data</a> is defined in [[!SPARQL11-Update]] as
						follows.</p>
					<p>OpInsertData(GS, <em>QuadPattern</em>) = Dataset-UNION(GS,
							Dataset(<em>QuadPattern</em>,{},GS,GS))</p>
					<p>where {} is the empty solution mapping,</p>
					<p> GS is an (first-order) <a>RDF dataset</a>, and <em>QuadPattern</em> is a
						ground quad pattern. </p>
					<p>The Insert Data operation constructs a new RDF Dataset from an existing RDF
						Dataset by adding triples and quads to it as expressed in the ground
							<em>QuadPattern</em>. Note that Insert Data is functional in that it
						does not actually "change" the input RDF Dataset. Update operations may be
						used to define a Graph Store as a state machine, where the state of a Graph
						Store is an RDF Dataset, and Update Operations take the current state of a
						Graph Store as one of their input arguments, generating a new RDF Dataset
						which then is taken as the new state of the Graph Store. </p>
					<p> We extend the Insert Data operation to <a>k-th Order RDF Datasets</a>. </p>
					<ul>
						<li>On the domain <a>k-th Order RDF Datasets</a> x Ground <a>k-th Order
								Patterns</a>, the range is <a>k-th Order RDF Datasets</a>.</li>
					</ul>
					<p>The extended semantics of <a>Insert Data</a> is defined as follows.</p>
					<p>OpInsertData(DS, <em>k-Pattern</em>) = Dataset-UNION(DS,
							Dataset(<em>k-Pattern</em>,{},DS,DS))</p>
					<p>where {} is the empty solution mapping,</p>
				</section>
				<section id="def-existing-update-opcreate">
					<h4>OpCreate</h4>
					<p><dfn>OpCreate</dfn> is defined in [[!SPARQL11-Update]] as an Update Operation
						with the following signature:</p>
					<ul>
						<li>On the domain RDF Dataset x IRI, the range is RDF Dataset.</li>
					</ul>
					<p>The semantics of <a>OpCreate</a> is defined in [[!SPARQL11-Update]] as
						follows.</p>
					<p>Let GS = {DG} union {(iri<sub>i</sub>, G<sub>i</sub>) | 1 ≤ i ≤ n} and
						graphNames(GS) = { iri<sub>i</sub> | 1 ≤ i ≤ n}, then</p>
					<p>OpCreate(GS, iri) = GS union {(iri, {})} if iri not in graphNames(GS);
						otherwise, OpCreate(GS, iri) = GS</p>
					<p> where DG and G<sub>i</sub> are graphs, iri and iri<sub>i</sub> are IRIs. </p>
					<p>The OpCreate operation constructs a new RDF Dataset from an existing RDF
						Dataset by adding an empty named graph to it with a specified IRI as name,
						provided it does not already contain a named graph of the specified name.
						Note that OpCreate is functional in that it does not actually "change" the
						input RDF Dataset. Update operations may be used to define a Graph Store as
						a state machine, where the state of a Graph Store is an RDF Dataset, and
						Update Operations take the current state of a Graph Store as input,
						generating a new RDF Dataset which then is taken as the new state of the
						Graph Store. </p>
					<p> We extend the OpCreate operation to <a>k-th Order RDF Datasets</a>. Also,
						the names are extended from IRIs to also allow blank node names. </p>
					<ul>
						<li>On the domain <a>k-th Order RDF Dataset</a> x I ∪ RDF-B, the range is
								<a>k-th Order RDF Dataset</a>.</li>
					</ul>
					<p>The extended semantics of <a>OpCreate</a> is defined as follows.</p>
					<p>Let DSP = {DSC} ∪ {(name<sub>i</sub>, DSC<sub>i</sub>) | 1 ≤ i ≤ n} and
						datasetNames(DSP) = { name<sub>i</sub> | 1 ≤ i ≤ n}, then</p>
					<p>OpCreate(DSP, name) = DSP ∪ {(name, {})} if name not in datasetNames(DSP);
						otherwise, OpCreate(DSP, name) = DSP</p>
					<p> where DSC and DSC are <a>k-th Order RDF Datasets</a>, and name and
							name<sub>i</sub> are IRIs or blank nodes (I ∪ RDF-B). </p>
				</section>
				<section>
					<h4>Other Update Operations</h4>
					<p class="ednote">Similarly, the Update Operations of SPARQL Update can be
						extended to <a>k-th order datasets</a>, making use of the extended basic
						operations. </p>
				</section>
			</section>
			<section id="def-new-apply-operation">
				<h4>Apply Operation</h4>
				<p class="ednote">The above extensions only allow management, e.g. creation of a new
					"slot", at the top level of a <a>k-th order dataset</a>. However, it is also
					useful to be able to apply operations at a deeper level within a <a>k-th order
						dataset</a>. This capability can be enabled with a generic operation, called
						<dfn>Apply</dfn>, which takes a <a>k-th order dataset</a> as first argument,
					an Operation specification (Operation name and tuple of arguments with a
					placehoder in one position) as second argument, and an optional third argument
					that is an IRI or blank node. The result would be the result of applying that
					Operation with the appropriate (named or default) (k-1)-th order dataset
					replacing the placeholder, and then substituting for the (k-1)-th order dataset
					with the operation result within the original <a>k-th order dataset</a>. If the
					name is not in the set of database names, then the result would be the original
					dataset. </p>
			</section>
			<section id="def-new-revise-operation">
				<h4>Revise Operation</h4>
				<p>Update Operations as defined in SPARQL Update, and here extended, are not
					directly applicable to <a>repositories</a> and <a>stores</a>. In order to enable
					them, we define a Revise operation, that takes as input a <a>repository</a>
					(with a terminal version) as first argument, an Update Operation specification
					as second argument and a name (IRI or blank node) as third argument. The result
					is a new <a>repository</a> that has an additional <a>version</a>, that being the
					result of applying the Update Operation to the terminal <a>version</a> of the
					input repository, and making that the new terminal version, using the specified
					name as the version name. </p>
				<p>The Revise Operation may be extended to <a>stores</a> in the obvious fashion.
					However, this only allows manipulation of the version sequence of the store. To
					enable direct manipulation of the version sequence of repositories within the
					store, we extend the <a>Apply</a> Operation, defined above for <a>k-th order
						datasets</a> and Update Operation specifications, to <a>stores</a> and
					Revise Operation specifications on repositories (only for unnested Revise). </p>
				<p>The Revise Operation may be further extended to <a>recursive stores</a> and
					nested Revise Operation Specifications. </p>
				<p>Application of a Revise operation to a structure without a terminal version is an
					error. </p>
			</section>
			<!--
		  <section>
		  <p class="ednote">Options for extending Update Operations to the Store/Repository model are:
		  </p>
		  <ol>
		    <li>Follow the functional approach by defining functions that act on the "state" of a Store, 
			(defined in terms of the Repositories, named or default, that it owns), 
			producing a new state.
			This state then presumably becomes the current state of the mutable Store, just like in the SPARQL UPDATE spec,
			but the mutability of a Store is not addressed explicitly.
			However, the mutability of Repositories is explicitly addressed, 
			these being the objects that make up the state of a Store.
			</li>
			<li>Handle the mutability of Stores explicitly by defining mutator methods on Store "objects".
			</li>
			<li>Take a full-blown functional approach by hiding all mutability.
			The "state" of a Store is then not expressed in terms of Repositories, but in terms of the "states"
			of its Repositories.
			The state of a Repository is an RDF Dataset, in parallel to the state of a Graph Store being an RDF graph.
			</li>
		  </ol>
		  
		  <p>Here is what the first option would look like:
          </p>
		  <p>
		  We further extend the OpCreate operation to <a>k-th Order Stores</a>.
		  </p>
		  <ul>
		    <li>On the domain <a>k-th Order Store</a> x I ∪ RDF-B, the range is <a>k-th Order Store</a>.</li>
		  </ul>
		  <p>The extended semantics of <a>OpCreate</a> is defined as follows.</p> 
		  <p>Let S<sup>k</sup> = {R<sup>k</sup>} ∪ 
		  {(name<sub>i</sub>, R<sup>k</sup><sub>i</sub>) | 1 ≤ i ≤ n} and 
		  datasetNames(R<sup>k</sup>) = { name<sub>i</sub> | 1 ≤ i ≤ n}, then</p>
		  <p>OpCreate(S<sup>k</sup>, name) = S<sup>k</sup> ∪ {(name, R<sup>k</sup><sub>0</sub>)} 
		  if name not in datasetNames(S<sup>k</sup>); 
		  otherwise, OpCreate(S<sup>k</sup>, name) = S<sup>k</sup></p>
		  <p>
		  where S<sup>k</sup> is a <a>k-th Order Store</a>, 
		  R<sup>k</sup><sub>i</sub> are <a>k-th Order Repositories</a>, 
		  name and name<sub>i</sub> are IRIs or blank nodes (I ∪ RDF-B)
		  and  R<sup>k</sup><sub>0</sub> is the empty <a>k-th Order Repository</a>.
		  </p>
		  -->
			<!--
		  <p>We extend the syntax of OpCreate to include the following additional signature:</p>
		  <ul>
		    <li>On the domain StoreState x IRI, the range is StoreState.</li>
		  </ul>
		  <p>The semantics of <a>OpCreate</a> on the domain extension is defined as follows.</p> 
		  <p>Let SS = {DR} union {NR<sub>i</sub> | 1 ≤ i ≤ n} and repositoryNames(SS) = { name(NR<sub>i</sub>) | 1 ≤ i ≤ n}, then</p>
		  <p>OpCreate(SS, iri) = SS union { NamedRepository(iri, R0)} if iri not in repositoryNames(SS); 
		  otherwise, OpCreate(SS, iri) = SS</p>
		  <p>
		  where DR is a repository, NR<sub>i</sub> are named repositories, iri is an IRI, and 
		  R0 is the "empty repository".
		  </p>		  
		  <p>Here is what the second option would look like:
          </p>
		  <p>
		  The corresponding "create" operation for <a>Stores</a> is a mutator operation on the mutable Store object.
		  </p>
		  <p><dfn>Store.create(self, iri)</dfn> has domain Store x IRI and is void.
		  </p>
		  <p class="ednote">The semantics of Store.create is to have the side-effect of 
             constructing a new Named Repository object with name iri whose initial content is empty, 
			 that is owned by the Store self.		  
		  </p>
		  <p class="ednote">We must defined the meaning of the "initial content of a (Named) Repository is empty" somewhere,
		  probably as a note to the definition of (Named) Repository.
		  </p>
		  <p class="ednote">Is "owned" the property term to use for the relationship between a Named Repository and
		  its parent Store?
		  </p>
		  <p>Here is what the third option would look like:
          </p>
		  <p>We extend the syntax of OpCreate to include the following additional signature:</p>
		  <ul>
		    <li>On the domain StoreState x IRI, the range is StoreState.</li>
		  </ul>
		  <p>The semantics of <a>OpCreate</a> on the domain extension is defined as follows.</p> 
		  <p>Let SS = {DD} union {(iri<sub>i</sub>, D<sub>i</sub>) | 1 ≤ i ≤ n} and 
		  repositoryNames(SS) = { iri<sub>i</sub> | 1 ≤ i ≤ n}, then</p>
		  <p>OpCreate(SS, iri) = SS union { (iri, {})} if iri not in repositoryNames(SS); 
		  otherwise, OpCreate(SS, iri) = SS</p>
		  <p>
		  where DD and D<sub>i</sub> are RDF Datasets, iri and iri<sub>i</sub> are IRIs.
		  </p>
         </section>
		-->
			<!--
		<section id="def-existing-update-operator">
          <h4>Operator</h4>
		  <p><dfn>Operator</dfn> is defined in [[!SPARQL11-Update]] as an Update Operation with the following signatures:</p>
		  <ul>
		    <li>On the domain X x Y, the range is Z.</li>
			<li>On the domain X x U, the range is V.</li>
		  </ul>	
		  <p class="ednote">The semantics of <a>Operator</a> is defined in [[!SPARQL11-Update]] to be (semantics)
		  </p>
    	</section>
		<section id="def-extension-of-existing-update-operator">
          <h4>Operator</h4>
		  <p><dfn>Operator</dfn> is defined in [[!SPARQL11-Update]] as a (supertype) with the following signatures:</p>
		  <ul>
		    <li>On the domain X x Y, the range is Z.</li>
			<li>On the domain X x U, the range is V.</li>
		  </ul>
		  <p>We extend the syntax of Operator to include the following additional signatures:</p>
		  <ul>
		    <li>On the domain X x Y, the range is Z.</li>
			<li>On the domain X x U, the range is V.</li>
		  </ul>
		  <p class="ednote">The semantics of <a>Operator</a> is defined in [[!SPARQL11-Update]] to be (semantics)
		  </p>
		  <p class="ednote">The intended semantics of <a>Operator</a> on the extended signature is (semantics)
		  </p>
		</section>
		<section id="def-new-operator">
          <h4>Operator</h4>
		  <p>A <dfn>Operator</dfn> is a (supertype) where (specialization)</a>.
		  </p>	
		  <p class="ednote">The intended semantics of <a>Operator</a> is (semantics)
		  </p>
		</section>
		-->
		</section>
		<section id="examples">
			<h2>Examples</h2>
			<section id="fhir-stream-example">
				<h3>FHIR RDF Stream Example</h3>
				<section id="fhir-graph-example">
					<aside class="example" id="fhir-graph" title="FHIR Statements as RDF Graph"> The
						following is a representation of an RDF graph in Trig. These statements have
						a dynamic aspect that is implicitly expressed in timestamps of the events of
						type fhir:Observations. Although the statements appear in a particular order
						within the graph representation, this order is not significant to inference.
						In particualr, the temporal order is not directly available through simple
						entailment. We will show how this same data could be handled within some of
						the new datatypes so that the dynamic nature of the statements is accessible
						through simple entailment. <p>The following graph is denoted
								G<sub>FHIR</sub>.</p>
						<pre>
@prefix fhir: &lt;http://hl7.org/fhir/&gt; .
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix obs: &lt;http://hl7.org/fhir/Observation.&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix sct: &lt;http://snomed.info/id/&gt; .

_:b1 a fhir:Observation ;
		obs:code sct:36407505 ;
		obs:device lr:sensor1234 ;
		obs:effectiveDateTime "2015-01-01T12:00:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		obs:subject lr:patientA ;
		obs:valueQuantity _:b2 .
	
_:b2 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		lr:value "50"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .

_:b4 a fhir:Observation ;
		obs:code sct:36407505 ;
		obs:device lr:sensor1234 ;
		obs:effectiveDateTime "2015-01-01T12:01:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		obs:subject lr:patientA ;
		obs:valueQuantity _:b5 .
	
_:b5 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		lr:value "53"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .

_:b7 a fhir:Observation ;
		obs:code sct:36407505 ;
		obs:device lr:sensor1234 ;
		obs:effectiveDateTime "2015-01-01T12:02:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		obs:subject lr:patientA ;
		obs:valueQuantity _:b8 .
	
_:b8 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		lr:value "48"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
</pre>
						<p>G<sub>FHIR</sub> is the union of three graphs, each fully describing an
							observation. We write:</p>
						<p>G<sub>FHIR</sub> = Dataset-UNION(Dataset-UNION(G<sub>1, FHIR</sub>,
								G<sub>2, FHIR</sub>), G<sub>3, FHIR</sub>)</p>
						<p>where G<sub>1, FHIR</sub> is: </p>
						<pre>
@prefix fhir: &lt;http://hl7.org/fhir/&gt; .
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix obs: &lt;http://hl7.org/fhir/Observation.&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix sct: &lt;http://snomed.info/id/&gt; .

_:b1 a fhir:Observation ;
		obs:effectiveDateTime "2015-01-01T12:00:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		obs:code sct:36407505 ;
		obs:device lr:sensor1234 ;
		obs:subject lr:patientA ;
		obs:valueQuantity _:b2 .
	
_:b2 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		lr:value "50"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
</pre>
						<p>G<sub>2, FHIR</sub> is:. </p>
						<pre>
@prefix fhir: &lt;http://hl7.org/fhir/&gt; .
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix obs: &lt;http://hl7.org/fhir/Observation.&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix sct: &lt;http://snomed.info/id/&gt; .


_:b4 a fhir:Observation ;
		obs:effectiveDateTime "2015-01-01T12:01:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		obs:code sct:36407505 ;
		obs:device lr:sensor1234 ;
		obs:subject lr:patientA ;
		obs:valueQuantity _:b5 .
	
_:b5 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		lr:value "53"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
</pre>
						<p>G<sub>3, FHIR</sub> is: </p>
						<pre>
@prefix fhir: &lt;http://hl7.org/fhir/&gt; .
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix obs: &lt;http://hl7.org/fhir/Observation.&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix sct: &lt;http://snomed.info/id/&gt; .

_:b7 a fhir:Observation ;
		obs:effectiveDateTime "2015-01-01T12:02:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		obs:code sct:36407505 ;
		obs:device lr:sensor1234 ;
		obs:subject lr:patientA ;
		obs:valueQuantity _:b8 .
	
_:b8 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		lr:value "48"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
</pre>
					</aside>
				</section>
				<aside class="example" id="fhir-stream" title="FHIR Statements as RDF Stream">
					Recall that an <a>RDF Stream</a> is a sequence of <a>timestamped graphs</a>
					which are composed of a default graph and a named graph, where the default graph
					contains a distinguished triple called the timestamp triple. <p>We will
						construct an RDF Stream for the FHIR observation data, which as a whole is
						denoted S<sub>FHIR</sub>.</p>
					<p>Let G'<sub>1, FHIR</sub> be </p>
					<pre>
@prefix fhir: &lt;http://hl7.org/fhir/&gt; .
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix obs: &lt;http://hl7.org/fhir/Observation.&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix sct: &lt;http://snomed.info/id/&gt; .

_:b1 a fhir:Observation ;
		obs:code sct:36407505 ;
		obs:device lr:sensor1234 ;
		obs:subject lr:patientA ;
		obs:valueQuantity _:b2 .
	
_:b2 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		lr:value "50"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
</pre>
					<p>which is like G<sub>1, FHIR</sub> except it is missing the timestamp triple,
						and TS<sub>1, FHIR</sub> be the timestamp triple</p>
					<pre>
_:b1 obs:effectiveDateTime "2015-01-01T12:00:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt;
</pre>
					<p>We construct a timestamped graph with the basic operation <a>TSGraph</a></p>
					<p>TSG<sub>1, FHIR</sub> = TSGraph(TS<sub>1, FHIR</sub>, G'<sub>1,
						FHIR</sub>)</p>
					<p>Similarly,</p>
					<p>TSG<sub>i, FHIR</sub> = TSGraph(TS<sub>i, FHIR</sub>, G'<sub>i,
						FHIR</sub>)</p>
					<p>The RDF stream can be abstractly constructed from its component timestamped
						graphs using basic operations:</p>
					<p>S<sub>FHIR</sub> = Cons(TSG<sub>3, FHIR</sub>, Cons(TSG<sub>2, FHIR</sub>,
							Cons(TSG<sub>1, FHIR</sub>, S<sub>0</sub>))) </p>
					<p>where S<sub>0</sub> is the null sequence.</p>
					<p>One possible way that this RDF Stream would be transmitted is as a sequence
						of RDF documents, each one containing an RDF graph corresponding to one
						element of the stream, where the timestamp triple is the first triple in the
						document. </p>
				</aside>
				<aside class="example" id="fhir-dataset" title="FHIR Statements as RDF Dataset"> The
					information about FHIR observations can also be represented using the <a>RDF
						Dataset</a> datatype. The named graph structure allows the statements of
					each stream element to be grouped together, and also allows us to materialize,
					in the default graph, statements giving the order among these elements. <p>Let
							D<sub>0, FHIR</sub> be</p>
					<pre>
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .

_:b102 prov:wasRevisionOf _:b101.
_:b103 prov:wasRevisionOf _:b102.
</pre>
					<p>We write</p>
					<p>D<sub>FHIR</sub> = Dataset-UNION(Dataset-UNION(Dataset-UNION(D<sub>0,
							FHIR</sub>, D<sub>1, FHIR</sub>), D<sub>2, FHIR</sub>), D<sub>3,
							FHIR</sub>)</p>
					<p>where</p>
					<p>D<sub>i, FHIR</sub> = Graph(b<sub>i, FHIR</sub>, G<sub>i, FHIR</sub>) and</p>
					<p>b<sub>1, FHIR</sub>= _:b101, b<sub>2, FHIR</sub>= _:b102, b<sub>3,
						FHIR</sub>= _:b103 .</p>
					<p>The resulting dataset D<sub>FHIR</sub> is as follows.</p>
					<pre>
@prefix fhir: &lt;http://hl7.org/fhir/&gt; .
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix obs: &lt;http://hl7.org/fhir/Observation.&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix sct: &lt;http://snomed.info/id/&gt; .

_:b102 prov:wasRevisionOf _:b101.
_:b103 prov:wasRevisionOf _:b102.
_:b101{
	    _:b1 a fhir:Observation ;
		    obs:code sct:36407505 ;
		    obs:device lr:sensor1234 ;
		    obs:effectiveDateTime "2015-01-01T12:00:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		    obs:subject lr:patientA ;
		    obs:valueQuantity _:b2 .
	
	    _:b2 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		    lr:value "50"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
}	

_:b102{
		_:b4 a fhir:Observation ;
			obs:code sct:36407505 ;
			obs:device lr:sensor1234 ;
			obs:effectiveDateTime "2015-01-01T12:01:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
			obs:subject lr:patientA ;
			obs:valueQuantity _:b5 .
	
		_:b5 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
			lr:value "53"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
}
_:b103{
		_:b7 a fhir:Observation ;
			obs:code sct:36407505 ;
			obs:device lr:sensor1234 ;
			obs:effectiveDateTime "2015-01-01T12:02:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
			obs:subject lr:patientA ;
			obs:valueQuantity _:b8 .
	
		_:b8 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
			lr:value "48"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
}
</pre>
				</aside>
				<aside class="example" id="fhir-repository" title="FHIR Statements as a Repository">
					The dataset above is a static structure representing the entailment, under the
					provenance regime, of a first-order <a>Repository</a>. We denote this repository
					as R<sub>FHIR</sub>. In this example, the default dataset of the repository is
					empty. <p>We write</p>
					<p>R<sub>FHIR</sub> = [{}, [D<sub>i,FHIR</sub>]<sub>i=1, 2, 3</sub>]</p>
					<p>The repository can be abstractly constructed from its component datasets
						using basic operations:</p>
					<p>R<sub>FHIR</sub> = Revise(D<sub>3,FHIR</sub>, Revise(D<sub>2,FHIR</sub>,
							Revise(D<sub>1,FHIR</sub>, S<sub>0</sub>))) </p>
					<p> Since the ordered component of a repository may be lazily contructed, the
						repository could be transmitted as a sequence of documents, one for the
						unordered dataset and one for each of the elements of the ordered component
						of the repository, transmitted as they become available. </p>
				</aside>
			</section>
			<section id="general-stream-example">
				<h3>General RDF Stream Example</h3>
				<aside class="example" id="prov-dataset" title="FHIR Provenance in an RDF Dataset">
					We consider a more general RDF Stream example, showing a typical case when sets
					of statements have multiple temporal characteristics. We construct a set of
					named graphs <p>NG<sub>i, FHIR</sub> = Graph(nb<sub>i</sub>, G<sub>i,
						FHIR</sub>)</p> where <p>nb<sub>1</sub> = _:b0, nb<sub>2</sub> = _:b3,
							nb<sub>3</sub> = _:b6 </p> and we express some provenance metadata using
					these graph names. Let G<sub>1, prov</sub> be
					<pre>
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .

_:b0 
	prov:generatedAtTime "2015-01-01T12:00:33Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
    lr:stream lr:streamA12342015-01-01T12:00:00Z .
</pre>
					Let G<sub>2, prov</sub> be
					<pre>
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .

_:b3 
	prov:generatedAtTime "2015-01-01T12:01:45Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
    lr:stream lr:streamA12342015-01-01T12:00:00Z .
</pre>
					Let G<sub>3, prov</sub> be
					<pre>
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .

_:b6 
	prov:generatedAtTime "2015-01-01T12:02:54Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
    lr:stream lr:streamA12342015-01-01T12:00:00Z .
</pre>
					In this example, we have the time of observation as one temporal
					characteristics, and the time the observation report was generated as the second
					temporal characteristic, The entire dataset is denoted D<sub>prov</sub> and is
					constructed as follows <p>D<sub>prov</sub> = Dataset-UNION(
							Dataset-UNION(D<sub>1,prov</sub>, D<sub>2,prov</sub>),
							D<sub>3,prov</sub> ) </p> where <p> D<sub>i,prov</sub> =
							Dataset-UNION(G<sub>i, prov</sub>, NG<sub>i, FHIR</sub>) </p> The
					following is a representation of D<sub>prov</sub> in Trig. In this
					representation, the dytnamic aspects are implicitly expressed in timestamps, but
					the temporal order is not directly available through simple entailment.
					<pre>
@prefix fhir: &lt;http://hl7.org/fhir/&gt; .
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix obs: &lt;http://hl7.org/fhir/Observation.&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix sct: &lt;http://snomed.info/id/&gt; .

_:b0 
	prov:generatedAtTime "2015-01-01T12:00:33Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
    lr:stream lr:streamA12342015-01-01T12:00:00Z .
_:b0 {
	_:b1 a fhir:Observation ;
		obs:code sct:36407505 ;
		obs:device lr:sensor1234 ;
		obs:effectiveDateTime "2015-01-01T12:00:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		obs:subject lr:patientA ;
		obs:valueQuantity _:b2 .
	
	_:b2 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		lr:value "50"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
}

_:b3 
	prov:generatedAtTime "2015-01-01T12:01:45Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
    lr:stream lr:streamA12342015-01-01T12:00:00Z .
_:b3 {
	_:b4 a fhir:Observation ;
		obs:code sct:36407505 ;
		obs:device lr:sensor1234 ;
		obs:effectiveDateTime "2015-01-01T12:01:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		obs:subject lr:patientA ;
		obs:valueQuantity _:b5 .
	
	_:b5 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		lr:value "53"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
}

_:b6 
	prov:generatedAtTime "2015-01-01T12:02:54Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
    lr:stream lr:streamA12342015-01-01T12:00:00Z .
_:b6 {
	_:b7 a fhir:Observation ;
		obs:code sct:36407505 ;
		obs:device lr:sensor1234 ;
		obs:effectiveDateTime "2015-01-01T12:02:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		obs:subject lr:patientA ;
		obs:valueQuantity _:b8 .
	
	_:b8 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		lr:value "48"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
}
</pre>
				</aside>
				<aside class="example" id="prov-stream" title="FHIR Provenance in an RDF Stream">
					<p>The representation of this same information as an RDF Stream takes the form
						of the following sequence. The stream as a whole is denoted
						S<sub>prov</sub>. </p>
					<p>We write</p>
					<p>S<sub>prov</sub> = [TSG<sub>i,prov</sub>]<sub>i=1, 2, 3</sub></p>
					<p>A representation of the first element of the stream is a timestamped graph as
						follows.</p>
					<p>TSG<sub>i,prov</sub> = TSGraph(, D'<sub>i, prov</sub>)</p> where D'<sub>i,
						prov</sub> is like D<sub>i, prov</sub> except it is missing the timestamp
					triple. <p>D'<sub>i, prov</sub> = Dataset-DIFF(D<sub>i, prov</sub>, TS<sub>i,
							prov</sub>) </p> where TS<sub>1, prov</sub> is <pre>
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .

_:b0 
	prov:generatedAtTime "2015-01-01T12:00:33Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
</pre>
					<p>and so on.</p>
					<p>One possible way that this RDF Stream would be transmitted is as a sequence
						of RDF documents, each one containing an RDF Dataset corresponding to one
						element of the stream.</p>
					<p>In this example, each element of the stream contains a triple in its default
						graph that provides an identifier for the stream as a whole
							(<code>lr:streamA12342015-01-01T12:00:00Z</code>). This is optional for
						RDF streams.</p>
				</aside>
				<aside class="example" id="prov-dataset-2"
					title="FHIR Provenance in a Second-Order Dataset"> This data can also be
					represented using the second-order <a>Dataset</a> datatype. For this
					representation, we use an extension of the Trig syntax to "quints". This
					additional name allows the statements of each stream element to be grouped
					together, and also allows us to materialize, in the default (first-order)
					dataset, statements giving the order among these elements. The second-order
					dataset is denoted D2<sub>prov</sub>. <pre>
@prefix fhir: &lt;http://hl7.org/fhir/&gt; .
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix obs: &lt;http://hl7.org/fhir/Observation.&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix sct: &lt;http://snomed.info/id/&gt; .

_:b202 prov:wasRevisionOf _:b201.
_:b203 prov:wasRevisionOf _:b202.

_:b201{
    DEFAULT {
		_:b0 
			prov:generatedAtTime "2015-01-01T12:00:33Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
			lr:stream lr:streamA12342015-01-01T12:00:00Zm .
	}
    _:b0 {
	    _:b1 a fhir:Observation ;
		    obs:code sct:36407505 ;
		    obs:device lr:sensor1234 ;
		    obs:effectiveDateTime "2015-01-01T12:00:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		    obs:subject lr:patientA ;
		    obs:valueQuantity _:b2 .
	
	    _:b2 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		    lr:value "50"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
    }
}	

_:b202{
    DEFAULT {
		_:b3 
			prov:generatedAtTime "2015-01-01T12:01:45Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
			lr:stream lr:streamA12342015-01-01T12:00:00Z .
	}
	_:b3 {
		_:b4 a fhir:Observation ;
			obs:code sct:36407505 ;
			obs:device lr:sensor1234 ;
			obs:effectiveDateTime "2015-01-01T12:01:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
			obs:subject lr:patientA ;
			obs:valueQuantity _:b5 .
	
		_:b5 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
			lr:value "53"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
	}
}
_:b203{
    DEFAULT {
		_:b6 
			prov:generatedAtTime "2015-01-01T12:02:54Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
			lr:stream lr:streamA12342015-01-01T12:00:00Z .
	}
	_:b6 {
		_:b7 a fhir:Observation ;
			obs:code sct:36407505 ;
			obs:device lr:sensor1234 ;
			obs:effectiveDateTime "2015-01-01T12:02:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
			obs:subject lr:patientA ;
			obs:valueQuantity _:b8 .
	
		_:b8 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
			lr:value "48"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
	}
}
</pre>
					<p>We write</p>
					<p>D2<sub>prov</sub> = Dataset-UNION( Dataset-UNION( Dataset-UNION(
							Dataset-UNION(D<sub>0, prov</sub>,ND<sub>1, prov</sub>),ND<sub>2,
							prov</sub>),ND<sub>3, prov</sub>) </p>
					<p>where</p>
					<p>ND<sub>i, prov</sub> = Dataset( b<sub>i, prov</sub>, D<sub>i, prov</sub>)</p>
					<p>b<sub>1, prov</sub>= _:b201, b<sub>2, prov</sub>= _:b202, b<sub>3,
						prov</sub>= _:b203 .</p>
					<p>and</p>
					<p>D<sub>0, prov</sub> is</p>
					<pre>
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .

_:b202 prov:wasRevisionOf _:b201.
_:b203 prov:wasRevisionOf _:b202.
</pre>
				</aside>
				<aside class="example" id="psrov-repository"
					title="FHIR Provenance as a Second-Order Repository">
					<p> The second-order dataset above is a static structure representing the
						entailment, under the provenance regime, of a second-order
						<a>Repository</a>. The repository as a whole is denoted R<sub>prov</sub>. In
						this example, the default dataset of the repository is empty. </p>
					<p>We write</p>
					<p>R<sub>prov</sub> = {} ∪ [ND<sub>i,prov</sub>]<sub>i=1, 2, 3</sub></p>
					<p> For this representation, we use the same extension of the Trig syntax to
						"quints" as in the previous example. Since repositories are dynamic
						entities, one form of the transmission of the repository would be a sequence
						of documents representing named first-order datasets, as shown below. </p>
					<p>The first document of the sequence is as follows. The named dataset is
						denoted ND<sub>1, prov</sub>. </p>
					<pre>
@prefix fhir: &lt;http://hl7.org/fhir/&gt; .
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix obs: &lt;http://hl7.org/fhir/Observation.&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix sct: &lt;http://snomed.info/id/&gt; .

_:b201{
    DEFAULT {
		_:b0 
			prov:generatedAtTime "2015-01-01T12:00:33Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
			lr:stream lr:streamA12342015-01-01T12:00:00Zm .
	}
    _:b0 {
	    _:b1 a fhir:Observation ;
		    obs:code sct:36407505 ;
		    obs:device lr:sensor1234 ;
		    obs:effectiveDateTime "2015-01-01T12:00:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
		    obs:subject lr:patientA ;
		    obs:valueQuantity _:b2 .
	
	    _:b2 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
		    lr:value "50"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
    }
}	
</pre>
					<p>The second document of the sequence is as follows. The named dataset is
						denoted ND<sub>2, prov</sub>. </p>
					<pre>
@prefix fhir: &lt;http://hl7.org/fhir/&gt; .
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix obs: &lt;http://hl7.org/fhir/Observation.&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix sct: &lt;http://snomed.info/id/&gt; .

_:b202{
    DEFAULT {
		_:b3 
			prov:generatedAtTime "2015-01-01T12:01:45Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
			lr:stream lr:streamA12342015-01-01T12:00:00Z .
	}
	_:b3 {
		_:b4 a fhir:Observation ;
			obs:code sct:36407505 ;
			obs:device lr:sensor1234 ;
			obs:effectiveDateTime "2015-01-01T12:01:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
			obs:subject lr:patientA ;
			obs:valueQuantity _:b5 .
	
		_:b5 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
			lr:value "53"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
	}
}
</pre>
					<p>The third document of the sequence is as follows. The named dataset is
						denoted ND<sub>3, prov</sub>. </p>
					<pre>
@prefix fhir: &lt;http://hl7.org/fhir/&gt; .
@prefix lr: &lt;http://localhost/local-records#&gt; .
@prefix obs: &lt;http://hl7.org/fhir/Observation.&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix sct: &lt;http://snomed.info/id/&gt; .

_:b203{
    DEFAULT {
		_:b6 
			prov:generatedAtTime "2015-01-01T12:02:54Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
			lr:stream lr:streamA12342015-01-01T12:00:00Z .
	}
	_:b6 {
		_:b7 a fhir:Observation ;
			obs:code sct:36407505 ;
			obs:device lr:sensor1234 ;
			obs:effectiveDateTime "2015-01-01T12:02:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt; ;
			obs:subject lr:patientA ;
			obs:valueQuantity _:b8 .
	
		_:b8 lr:unit "bpm"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
			lr:value "48"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt; .
	}
}
</pre>
				</aside>
			</section>
			<section id="pattern-examples">
				<h3>Pattern Examples</h3>
				<aside class="example" id="bgp-fhir-heartrate"
					title="Basic Graph Pattern for FHIR Heartrate Observation">
					<p>The basic graph pattern (BGP) below will be used to demonstrate how solution
						mappings might be extracted from the above data structures. This BGP is
						denoted TP<sub>FHIR-heartrate</sub>. </p>
					<pre>
?heartRateObservation a fhir:Observation ;
          obs:subject ?patient ;
          obs:effectiveDateTime ?heartRate_time ;
          obs:code &lt;http://snomed.info/sct:36407505&gt; ; # heart rate
          obs:valueQuantity [ fhir:Quantity.value ?heartRate;
                              fhir:Quantity.unit 'bpm'        ] .
</pre>
				</aside>
				<aside class="example" id="bgp-pattern-prov-revise"
					title="Basic Graph Pattern for Revision Statements">
					<p>The basic graph pattern (BGP) below will be used to demonstrate how solution
						mappings about the relative order of versions of a repository might be
						extracted from the above data structures using the provenance entailment
						regime. This BGP is denoted TP<sub>revise</sub>. </p>
					<pre>
?version prov:wasRevisionOf ?previous.
</pre>
				</aside>
				<aside class="example" id="bgp-pattern-prov-initial"
					title="Basic Graph Pattern for Initial Version Statements">
					<p>The basic graph pattern (BGP) below will be used to demonstrate how solution
						mappings describing the initial version of a repository might be extracted
						from the above data structures using the provenance entailment regime. This
						BGP is denoted TP<sub>initialVersion</sub>. </p>
					<pre>
?version repo:isInitialVersion "true"^^&lt;http://www.w3.org/2001/XMLSchema#boolean&gt; .
</pre>
				</aside>
				<aside class="example" id="bgp-pattern-prov-initial-of"
					title="Basic Graph Pattern for Initial Version Statements for Named Repositories">
					<p>The basic graph pattern (BGP) below will be used to demonstrate how solution
						mappings describing the initial version of a named repository might be
						extracted from the above data structures using the provenance entailment
						regime. This BGP is denoted TP<sub>initialVersionOf</sub>. </p>
					<pre>
?version repo:isInitialVersionOf ?repo .
</pre>
				</aside>
				<aside class="example" id="bgp-pattern-prov-terminal"
					title="Basic Graph Pattern for Terminal Version Statements">
					<p>The basic graph pattern (BGP) below will be used to demonstrate how solution
						mappings describing the terminal version of a repository might be extracted
						from the above data structures using the repository structure entailment
						regime. This BGP is denoted TP<sub>terminalVersion</sub>. </p>
					<pre>
?version repo:isTerminalVersion "true"^^&lt;http://www.w3.org/2001/XMLSchema#boolean&gt; .
</pre>
				</aside>
				<aside class="example" id="bgp-pattern-prov-terminal-of"
					title="Basic Graph Pattern for Terminal Version Statements for Named Repositories">
					<p>The basic graph pattern (BGP) below will be used to demonstrate how solution
						mappings describing the terminal version of a named repository might be
						extracted from the above data structures using the repository structure
						entailment regime. This BGP is denoted TP<sub>terminalVersionOf</sub>. </p>
					<pre>
?version repo:isTerminalVersionOf ?repo .
</pre>
				</aside>
				<aside class="example" id="quad-fhir-heartrate"
					title="QuadPattern for FHIR Heartrate Observation">
					<p>The <a>Quadpattern</a> below will be used to demonstrate how solution
						mappings might be extracted from the above data structures. This
							<a>Quadpattern</a> is denoted QP<sub>FHIR-heartrate</sub>. </p>
					<pre>
GRAPH ?g {
	?heartRateObservation a fhir:Observation ;
          obs:subject ?patient ;
          obs:effectiveDateTime ?heartRate_time ;
          obs:code &lt;http://snomed.info/sct:36407505&gt; ; # heart rate
          obs:valueQuantity [ fhir:Quantity.value ?heartRate;
                              fhir:Quantity.unit 'bpm'        ] .
}
</pre>
					<p>We write</p>
					<p>QP<sub>FHIR-heartrate</sub> = Graph( ?g, TP<sub>FHIR-heartrate</sub>) </p>
				</aside>
				<aside class="example" id="id-pattern-0" title="Identity Pattern for Zeroth Order">
					This BGP is denoted ID<sub>0</sub>. <pre>
{?s ?p ?o.}
</pre> The abstract
					algebra expression is <p> ID<sub>0</sub> = BGP(?s ?p ?o) </p>
				</aside>
				<aside class="example" id="id-pattern-1" title="Identity Pattern for First Order">
					This GraphPattern is denoted ID<sub>1</sub>.
					<pre>
{ graph ?g {?s ?p ?o} } union {?s ?p ?o}
</pre> The abstract algebra
					expression is <p> ID<sub>1</sub> = Union( Graph( ?g, ID<sub>0</sub> ),
							ID<sub>0</sub>) </p>
				</aside>
				<aside class="example" id="id-pattern-2" title="Identity Pattern for Second Order">
					This DatasetPattern is denoted ID<sub>2</sub>.
					<pre>
	{ dataset ?d { { graph ?g {?s ?p ?o} } union {?s ?p ?o} } }    
	 union
	{ graph ?g {?s ?p ?o} } union {?s ?p ?o}
</pre>
					The abstract algebra expression is <p> ID<sub>2</sub> = Union( Dataset( ?d,
							ID<sub>1</sub> ), ID<sub>1</sub>) </p> and in general <p>
							ID<sub>k+1</sub> = Union( Dataset( ?d, ID<sub>k</sub> ), ID<sub>k</sub>)
					</p>
				</aside>
			</section>
			<section id="query-examples">
				<h3>Query Examples</h3>
				<aside class="example" id="select-triple-pattern-avg-hr"
					title="Select Query with Triple Pattern for Average Heartrate">
					<pre>
	
SELECT (AVG(?heartRate) AS ?heartRate_base)

WHERE {
        ?heartRateObservation a fhir:Observation ;
          obs:subject ?patient ;
          obs:effectiveDateTime ?heartRate_time ;
          obs:code &lt;http://snomed.info/sct:36407505&gt; ; # heart rate
          obs:valueQuantity [ fhir:Quantity.value ?heartRate;
                              fhir:Quantity.unit 'bpm'        ] .
        FILTER ( ?heartRate_time &gt;= (now() - 'P30D'^^xsd:dayTimeDuration) )
    }
</pre>
				</aside>
			</section>
			<section id="abstract-algebra-examples">
				<h3>Abstract Algebra Examples</h3>
				<section id="eval-examples">
					<h4>Eval</h4>
					<aside class="example" id="match-bgp-dataset"
						title="Matching a BGP Against a k-th Order Dataset">
						<p>If k=0, then application of eval to a dataset and BGP corresponds to
							matching a BGP against an RDF Graph.</p>
						<p>If k=1, then application of eval to a dataset and BGP corresponds to
							matching a BGP against an RDF Dataset, so that the default graph is
							active.</p>
						<p>If k=2, then application of eval to a dataset and BGP corresponds to
							matching a BGP against the RDF Dataset that is the default dataset of
							the second-order dataset. For example,</p>
						<p> eval(<a href="#prov-dataset-2">D2<sub>prov</sub></a>, <a
								href="#bgp-pattern-prov-revise">TP<sub>rev</sub></a>) =
								Ω<sub>rev</sub>
						</p>
						<p> where Ω<sub>rev</sub> is shown in the following table:</p>
						<table id="omega-rev">
							<tr>
								<th>?version</th>
								<th>?previous</th>
							</tr>
							<tr>
								<td>_:b202</td>
								<td>_:b201</td>
							</tr>
							<tr>
								<td>_:b203</td>
								<td>_:b202</td>
							</tr>
						</table>
					</aside>
					<aside class="example" id="match-bgp-repository"
						title="Matching a BGP Against a k-th Order Repository">
						<p>If k=0, then application of eval to a repository and BGP corresponds to
							matching the BGP against an RDF Graph, as defined in [[!SPARQL11-Query]]
							.</p>
						<p>If k=1, then application of eval to a repository and BGP corresponds to
							matching the BGP against the RDF Graph that is the unordered component
							of the first-order repository.</p>
						<p>For example,</p>
						<p> eval(<a href="#prov-dataset-2">D2<sub>prov</sub></a>, <a
								href="#bgp-pattern-prov-revise">TP<sub>rev</sub></a>) = <a
								href="#omega-rev">Ω<sub>rev</sub></a>
						</p>
						<p>If k=2, then application of eval to a repository and BGP corresponds to
							matching the BGP against the RDF Dataset that is the unordered component
							of the second-order repository.</p>
						<p>In general, application of eval to a (k+1)-th repository and BGP
							corresponds to matching the BGP against the k-th order RDF Dataset that
							is the unordered component of the input repository. The result is a
							multiset of solution mappings.</p>
					</aside>
					<aside class="example" id="match-quad-dataset"
						title="Matching a QuadPattern Against a k-th Order Dataset">
						<p>Recall that a <a>QuadPattern</a>, also called a GraphGraphPattern, can
							match withing named and unnamed graphs, depending on the nature of the
							pattern.</p>
						<p>If k=0, then application of eval to a dataset and QuadPattern corresponds
							to matching a QuadPattern against an RDF Graph, as defined in
							[[!SPARQL11-Query]].</p>
						<p>If k=1, then application of eval to a dataset and QuadPattern corresponds
							to matching a QuadPattern against an RDF Dataset, also as defined in
							[[!SPARQL11-Query]]. Such an operation can extract information from both
							the default and named components of the dataset. </p>
						<p>If k>=2, then application of eval to a dataset and QuadPattern
							corresponds to matching the QuadPattern against the default (k-1)-th
							order RDF Dataset of the input dataset, which may be defined by
							induction over the order. As shown in the next example, a second-order
							pattern (containing "quint" patterns) is needed to access the
							information in the (non-default) named components of a second-order
							dataset. </p>
					</aside>
					<aside class="example" id="match-quint-second-order-dataset"
						title="Matching a Second-Order Pattern Against a Second-Order Dataset">
						<p> eval(<a href="#prov-dataset-2">D2<sub>prov</sub></a>, <a
								href="#id-pattern-2">ID<sub>2</sub></a>) = Ω<sub>D2-prov</sub>
						</p>
						<p> where Ω<sub>D2-prov</sub> is partially shown in the following table:</p>
						<table id="omega-d2-prov">
							<tr>
								<th>?s</th>
								<th>?p</th>
								<th>?o</th>
								<th>?g</th>
								<th>?d</th>
							</tr>
							<tr>
								<td>_:b202</td>
								<td>prov:wasRevisionOf</td>
								<td>_:b201</td>
								<td></td>
								<td></td>
							</tr>
							<tr>
								<td>_:b0</td>
								<td>prov:generatedAtTime</td>
								<td>"2015-01-01T12:00:33Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt;</td>
								<td></td>
								<td>_:b201</td>
							</tr>
							<tr>
								<td>_:b1</td>
								<td>rdf:type</td>
								<td>fhir:Observation</td>
								<td>_:b0</td>
								<td>_:b201</td>
							</tr>
						</table>
					</aside>
					<aside class="example" id="match-quad-first-order-repository"
						title="Matching a First-Order Pattern Against a First-Order Repository">
						<p> eval(<a href="#fhir-repository">R<sub>FHIR</sub></a>, <a
								href="#id-pattern-1">ID<sub>1</sub></a>) = eval(Dataset-UNION(G0,
								[D<sub>i, FHIR</sub>]<sub>i=1,2,3</sub>), <a href="#id-pattern-1"
									>ID<sub>1</sub></a>) = [{} , [Ω<sub>i,
								R-FHIR</sub>]<sub>i=1,2,3</sub>] </p>
						<p> where Ω<sub>i, R-FHIR</sub> = eval(Dataset-UNION(G0, D<sub>i,
							FHIR</sub>), ID<sub>1</sub>) </p>
						<!--is partially shown in the following table:</p>
		<table id="omega-d2-prov">
			<tr>
				<th>?s</th>
				<th>?p</th>
				<th>?o</th>
				<th>?g</th>
				<th>?d</th>
			</tr>
			<tr>
				<td>_:b202</td>
				<td>prov:wasRevisionOf</td>
				<td>_:b201</td>
				<td></td>
				<td></td>
			</tr>
			<tr>
				<td>_:b0</td>
				<td>prov:generatedAtTime</td>
				<td>"2015-01-01T12:00:33Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt;</td>
				<td></td>
				<td>_:b201</td>
			</tr>
			<tr>
				<td>_:b1</td>
				<td>rdf:type</td>
				<td>fhir:Observation</td>
				<td>_:b0</td>
				<td>_:b201</td>
			</tr>
		</table>-->
					</aside>
					<!--
	 The Eval function has been extended to apply to higher-order datasets, repositories and stores.
     If the pattern is a BGP, then matching will be performed only within the default component 
	 (dataset, repository or store, as the case may be.)
	 <p>The multiset of solution mappings that is obtained by applying the BGP 
	 <a href='#bgp-pattern-prov-revise'>TP<sub>rev</sub></a> to the second-order dataset <a href='#prov-dataset-2'>D2<sub>prov</sub></a> is
	 shown in the following table, and is denoted Ω<sub>rev</sub>.
	 </p>
	 Application of the same BGP to the second-order <a>repository</a> <a href="#fhir-repository">R<sub>prov</sub></a>
	 under the simple entailment regime
	 results in an empty set of solution mappings.
	 <p>Eval(R<sub>prov</sub>, TP<sub>rev</sub>) = {}</p>
	 However, the solutions above are obtained under the provenance entailment regime
	 <p>Eval-Prov(R<sub>prov</sub>, TP<sub>rev</sub>) = Ω<sub>rev</sub></p>
	</aside>
	<p class="ednote">OK, the extension of the eval function has not actually been defined yet.</p>
     <aside class="example" id="match-quad-pattern" title="Matching a QuadPattern">
     If the pattern is a quad pattern, then matching will be performed according to two cases.
	 <ul>
	   <li>If the data structure  (dataset, repository, or store) is of first-order, then the quad pattern
	   will be applied to named components.
	   </li>
	   <li>If the data structure  (dataset, repository, or store) is of order higher than first, 
	   then the quad pattern will be applied to the default component.
	   </li>
	 </ul>
	 <p>The solutions that are obtained by applying the QuadPattern 
	 <a href='#quad-fhir-heartrate'>QP<sub>FHIR-heartrate</sub></a> to 
	 the first-order dataset <a href='#fhir-dataset'>D<sub>FHIR</sub></a> are
	 shown in the following table, and denoted by Ω<sub>hr</sub>.
	 </p>
	 <table>
		<tr>
			<th>?g</th>
			<th>?heartRateObservation</th>
			<th>?patient</th>
			<th>?heartRate_time</th>
			<th>?heartRate</th>
		</tr>
		<tr>
			<td>_:b101</td>
			<td>_:b1</td>
			<td>lr:patientA</td>
			<td>"2015-01-01T12:00:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt;</td>
			<td>50"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt;</td>
		</tr>
		<tr>
			<td>_:b102</td>
			<td>_:b4</td>
			<td>lr:patientA</td>
			<td>"2015-01-01T12:01:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt;</td>
			<td>53"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt;</td>
		</tr>
		<tr>
			<td>_:b103</td>
			<td>_:b7</td>
			<td>lr:patientA</td>
			<td>"2015-01-01T12:02:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt;</td>
			<td>48"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt;</td>
		</tr>
		
	 </table>
We write
	 <p>Eval(D<sub>FHIR</sub>, QP<sub>FHIR-heartrate</sub>) = Ω<sub>hr</sub></p>

	 When the same QuadPattern is applied to the first-order <a>repository</a> <a href="#fhir-repository">R<sub>FHIR</sub></a>
	 we also obtain the solutions above.
	 This is because the eval function corresponds to simple entailment,
	 and simple entailment on a repository is defined as simple entailment
	 on the dataset obtained by forgetting the sequential order of versions.
	 <p>Eval(R<sub>FHIR</sub>, QP<sub>FHIR-heartrate</sub>) = Ω<sub>hr</sub></p>
	</aside>
	</section>
	<section id="ceval-examples">
	<h4>Ceval</h4>
	<p>
     A new function (Ceval) has also been defined for performing repeated (also called "continuous") matching
	 of patterns against dynamic data structures.
	 Continuous matching is "push-style" when the transmission is streamed;
	 a match event is triggered by the arrival of a stream element.
	 </p>
	<aside class="example" id="continuous-match-bgp" title="Continuous Matching with a BGP">
	</aside>
	 <p  class="ednote">To be moved to the section where Ceval is defined:
	 Continuous matching is applicable to
	  <a>Repositories</a>, <a>Stores</a>, <a>Recursive Stores</a>,
	   sequences of <a>k-th Order RDF Datasets</a>,
	   and <a>k-th Order Sequential RDF Datasets</a>, 
	   which are structures consisting of a sequence of <a>k-th Order RDF Datasets</a>
	   and a distinguished <a>k-th Order RDF Datasets</a> called the default dataset.
     </p>
	 <p class="ednote">To define the semantics of Ceval, we need a new datatype for the result, called
	 <dfn>Structured Result</dfn>, comprised of a distinguished multiset of solution mappings, possibly empty, 
	 the default component of the structured result, and a
	 totally-ordered set of solutions (possibly empty, finite or infinite), the sequential component of the structured result.
	 </p>
	 <p class="ednote">
	 In the case that the first argument is a <a>Repository</a>, <a>Store</a>, or <a>Recursive Store</a>,
	 the Ceval function takes a pattern as second argument.
	 </p>
	 <p class="ednote">
	 The pattern is matched to the default component of the input data structure, 
	 to generate the default component of the <a>structured result</a>.
	 </p>
	 <p class="ednote">
	 The pattern is also matched against the union of an element of the sequence
	 together with the default component,
	 to generate the sequential part of the <a>structured result</a>.
     </p>
	 <p>The solution sequence that is obtained by matching the QuadPattern 
	 <a href='#quad-fhir-heartrate'>QP<sub>FHIR-heartrate</sub></a> continuously to 
	 the first-order repository <a href="#fhir-repository">R<sub>FHIR</sub></a> is
	 shown in the following sequence of tables, denoted by [Ω<sub>i, hr</sub>]<sub>i = 1, 2, 3</sub>.
	 </p>
	 <table>
		<tr>
			<th>?g</th>
			<th>?heartRateObservation</th>
			<th>?patient</th>
			<th>?heartRate_time</th>
			<th>?heartRate</th>
		</tr>
		<tr>
			<td>_:b101</td>
			<td>_:b1</td>
			<td>lr:patientA</td>
			<td>"2015-01-01T12:00:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt;</td>
			<td>50"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt;</td>
		</tr>
	 </table>
	 <br/><br/>
	 <table>
		<tr>
			<th>?g</th>
			<th>?heartRateObservation</th>
			<th>?patient</th>
			<th>?heartRate_time</th>
			<th>?heartRate</th>
		</tr>
		<tr>
			<td>_:b102</td>
			<td>_:b4</td>
			<td>lr:patientA</td>
			<td>"2015-01-01T12:01:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt;</td>
			<td>53"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt;</td>
		</tr>
	 </table>
	 <br/><br/>
		<table>
		<tr>
			<th>?g</th>
			<th>?heartRateObservation</th>
			<th>?patient</th>
			<th>?heartRate_time</th>
			<th>?heartRate</th>
		</tr>
		<tr>
			<td>_:b103</td>
			<td>_:b7</td>
			<td>lr:patientA</td>
			<td>"2015-01-01T12:02:00Z"^^&lt;http://www.w3.org/2001/XMLSchema#dateTimeStamp&gt;</td>
			<td>48"^^&lt;http://www.w3.org/2001/XMLSchema#integer&gt;</td>
		</tr>
		
	 </table>
	 <p>The Quadpattern QP<sub>FHIR-heartrate</sub> will yield no solutions
	 when matched to an RDF graph. Therefore the default component of the result is empty, no matter what
	 is in the default dataset R<sub>FHIR</sub>.
	 </p>
	 <p>We write</p>
	 <p>Ceval(R<sub>FHIR</sub>, QP<sub>FHIR-heartrate</sub>) = {} ∪ [Ω<sub>i,hr</sub>]<sub>i=1, 2, 3</sub></p>
	 as well as
	 <p>Ceval-prov(R<sub>FHIR</sub>, QP<sub>FHIR-heartrate</sub>) = {} ∪ [Ω<sub>i,hr</sub>]<sub>i=1, 2, 3</sub></p>
	 <p class="ednote">RDF Stream remarks:
	 Recall that inference on an RDF stream is supposed to be oblivious to which triple is the timestamp triple.
	 The special status of timestamp triple is supposed to only be used in windowing operations. 
	 So, a continuous query on a stream should not be able to determine which triple is the timestamp triple.
	 However, a second-order repository that emulates an RDF stream needs to keep this information somewhere,
	 for windowing purposes.
	 The default dataset of the repository is the logical place for this information to be stored. 
	 With a property "repo:hasTimestampTriple", and the RDF reification vocabulary, this
	 information can be materialized provided there is an identifier for each stream element (and this
	 identifier is an inherent part of the second-order repository).
	 Queries of the repository that are supposed to emulate queries of the stream MUST not access that information.
	 This works out OK as long as a repository is used to emulate the stream - the pattern to match against stream elements
	 must have a second-order name (i.e. it is a quint pattern) and so will give no solutions when matched against the
	 default first-order dataset.
	 </p>
	 <p class="ednote">Restrictions on the arguments of Ceval in order to emulate an unwindowed query of an RDF stream:
	 </p>
	 <ol>
	 <li>It is necessary to include the (second-order) dataset name as a variable in the query, so that 
	     the query matching is scoped to one stream element at a time. However, there needs to be a
		 Projection applied to the solution so that the dataset name is forgotten for the <a>structured result</a>
	 </li>
	 <li>Since an RDF Stream does not have a default dataset, there should be no additional entailments
	     such as those arising from the structural information of the stream, with property <code>repo:hasTimestampTriple</code>.
		 On the other hand, we want to enable queries of RDF streams jointly with static datasets.
		 The union of a static and dynamic data structure puts the static statements into the default component of the
		 dynamic structure, so the union of a static dataset and an RDF stream will have a default component (except for the trivial
		 case when the static structure is empty). For proper querying in this case, it would be helpful if in the "union" the
		 elements of sequential components are named with blank nodes, so that the result of the union is a repository.
		 On the other hand, the union of a stream with an empty static dataset causes a change of type, which is undesirable.
	</li>	 
	 </ol>
	 <p class="ednote">
	 	 Note that windowing is supposed to be performed prior to pattern matching, so if windowing is done on an RDF stream,
	 the result is a stream (sequence) of RDF Datasets, and this is not (in general) an RDF Stream.
	 So we also need to define Ceval on a sequence of (unnamed) RDF Datasets, and further, show how to
	 restrict the arguments of Ceval so that a second-order repository can be used to emulate a stream of RDF Datasets.
	 </p>
	<p class="ednote">
	 In the case that the first argument of Ceval is a <a>sequential dataset</a>,
	 the Ceval function takes a <a>k-th Order Pattern</a>, P<sub>seq</sub>, as second argument,
	 and an optional <a>k-th Order Pattern</a>, P<sub>def</sub> as third argument.
	 The pattern P<sub>seq</sub> is matched continuously against
	 the union of each element of the sequence with the default dataset,
	 to generate the sequential part of the <a>structured result</a>,
	 while the default component of the <a>structured result</a> is obtained
	 from a one-time match of P<sub>def</sub> against the default dataset.
	 </p>
	 <p class="ednote">
	 The case that the first argument is an <a>RDF Stream</a>
     is a specialization of the case that the first argument is a sequence of 
	 <a>k-th Order RDF Datasets</a>, with k = 1, with empty default dataset.
	 Therefore, the second argument P<sub>seq</sub> MUST be a Quadpattern.
	 If a third argument is present, it has no effect since the default dataset is empty.
	 </p>
	 <p class="ednote">
	 The case that the first argument is the merger or union of an <a>RDF Stream</a>
	 with a static first-order dataset
     is also a specialization of the case that the first argument is a sequence of 
	 <a>k-th Order RDF Datasets</a>, with k = 1, with non-empty default dataset.
	 Again, the second argument P<sub>seq</sub> MUST be a Quadpattern.
	 If a third argument is present, it does have an effect since the default dataset is non-empty.
	 </p>
	 <p class="ednote">This formulation of the Ceval function that takes two pattern arguments is
	 not very satisfactory because it breaks the SPARQL syntactic convention of processing just one pattern at a time.
	 It might be better to split off the matching of the pattern on the default structure to another
	 Eval spinoff, e.g. Seval - for Static evaluation on a dynamic data structure, where the pattern is matched
	 only against the default component, and the result is a single multiset of solution mappings.
	 </p>
	<p class="ednote">OK, the continuous eval function Ceval has not actually been defined yet either.
	The description above will form the basis of that definition.
	The problem is that we have proposed three different "evals" on dynamic structures, Eval, Ceval and Seval.
	To express this in the concrete syntax, we need to provide a hint as to which "eval" function is intended,
	given
	</p>
	<pre>
	SELECT *
	FROM &lt;dynamic data structure IRI&gt;
	WHERE {some pattern}
	</pre>
	<p>
	There is a choice for indicating the different kinds of eval syntactically (with reserved words) or semantically,
	with e.g. query strings.
	In the former case, the syntactic hints can be applied to construct subqueries without binding them to an IRI.
	In the latter case, IRIs become non-opaque, since the query string would be used to determine the transformation
	from concrete to abstract syntax.
	</p>
	-->
				</section>
				<section id="dataset-union-examples">
					<h4>Dataset-UNION</h4>
					<p>The Dataset-UNION basic operation has been extended to apply to arbitrary
						pairs of higher-order datasets. It has also been extended to apply to pairs
						where one of the elements is a higher-order dataset and the other is a
						dynamic data structure (repository, store, recursive store, sequential
						datasets ). </p>
					<p> In all cases, the first step of evaluation is to elevate a static input
						argument as needed: to the same order (in the case of two higher-order
						datasets), or in the case of one static and one dynamic structure, to the
						requisite orders so that the default component of the dynamic structure is
						of the same order as the static structure. Then it is sufficient to consider
						input arguments where these orders are already compatible. The type of the
						result is either a dynamic structure, of the same type and order as the
						dynamic input, or a static, higher-order dataset, of the order of the two
						inputs. </p>
				</section>
			</section>
		</section>
		<!--
	  <section id="def-timestamp-predicate">
          <h4>Timestamp Predicates</h4>
          <p> An RSP timestamp predicate <code>P</code> is an <code>rdf:Property</code> with range <code>rsp:TemporalEntity</code> that is
            associated, through the <code>rdf:Property</code> <code>rsp:hasTimePlenum</code>, with a unique partially-ordered set (poset) of 
            temporal entities <code>T<sub>P</sub> &sube; T<sup>all</sup></code>, called the <em>time plenum</em> of the predicate.
            The time plenum is associated, through the <code>rdf:Property</code> <code>rsp:hasPartialOrder</code>, 
            with a unique partial order <code>&lt;=<sub>P</sub></code> that is in the class
            <code>rsp:ChronologicalPartialOrder</code>.</p>
          <p>The usual mathematical requirements of a partial order MUST be satisfied by an instance of 
            <code>rsp:ChronologicalPartialOrder</code>. In particular, for all instances <code>X</code>, <code>Y</code>, <code>Z</code> 
            of the time plenum, the following properties hold: 
          </p>
          <dl>
            <dt>Reflexivity</dt>
            <dd><code>X &lt;=<sub>P</sub> X</code></dd>
            <dt>Antisymmetry<dt>
              <dd>  <code>X &lt;=<sub>P</sub> Y</code> and <code>Y &lt;=<sub>P</sub> X</code> implies <code>X =
                Y</code></dd>
            <dt>Transitivity</dt>
              <dd><code>X &lt;=<sub>P</sub> Y</code> and <code>Y &lt;=<sub>P</sub> Z</code> implies <code>X &lt;=<sub>P</sub>
                Z</code>. </dd>
          </dl>
          <p>Further, an instance of <code>rsp:ChronologicalPartialOrder</code> MUST respect the natural order of time.</p>
          <p>In
            particular, if every time instant within the closure of temporal entity <code>X</code>
            is earlier than every time instant within the closure of temporal entity <code>Y</code>,
            then <code>X &lt;=<sub>P</sub> Y</code> (where closure of a time instant <code>t</code> is defined as
            the degenerate interval <code>[t, t]</code>, and closure of an interval is defined in
            the usual way) </p>
          <p class="ednote">The above definition regarding natural order of time depends on the
            concept of time instant as the primitive of temporal entities that is universal amoung
            temporal ontologies and has a particular chronological order associated with them. 
            However, not all temporal
            ontologies have time instants as their primitives, e.g., OMG.
            Further, branching temporal ontologies have a different set of time primitives than linear ones.
            So this needs to be fixed.
          </p>
        </section>
      </section>
      <section id="def-timestamped-graph">
        <h3>Timestamped Graphs</h3>
        <p>A <em>timestamped graph</em> is defined as an RDF Dataset under the RDF Dataset semantics
          that <a
            href="http://www.w3.org/TR/2014/NOTE-RDF11-datasets-20140225/#each-named-graph-defines-its-own-context"
            >"each named graph defines its own context"</a> (see <a href="#semantics"></a>) and
          where a particular triple in the default graph has been designated as the timestamp
          triple, with the following constraints:</p>
        <ol>
          <li>There is exactly one named graph pair <code>&lt;n, G></code> in the RDF Dataset (where
              <code>G</code> is an <a href="http://www.w3.org/TR/RDF11-Concepts/#section-rdf-graph"
              >RDF graph</a>, and <code>n</code> is an IRI or blank node).</li>
          <li>The timestamp triple has the form <code>&lt;n, p, t></code>, where <code>n</code> is
            defined in the previous item, and <code>p</code> is a timestamp predicate that captures
            the relationship between the temporal entity <code>t</code>, called the timestamp, and
            the graph <code>G</code>.</li>
        </ol>
        <p class="note">There may be multiple triples in the default graph, including multiple
          triples using timestamp predicates, but exactly one triple must be designated as the
          timestamp triple. The objects of any triples that use a timestamp predicate but are not the 
          timestamp triple of the timestamped graph are not called timestamps.</p>
        <p class="ednote">Due to the assertion of the timestamp triple, the context referred to in
          "each named graph defines its own context" has a temporal aspect to it. Other aspects of
          this context may be asserted by additional triples in the default graph of the timestamped
          graph. Such triples are not required to have timestamp predicates, and thus may be about
          non-temporal aspects of the context, e.g., the authority or the sensor device. That is, we
          require the context to have a temporal aspect to it, but the context is not limited to
          temporal aspects. Thus, it would be misleading to call it a "temporal context". </p>
        <p class="ednote">The part of the definition above that points to RDF Dataset semantics
          really belongs in the semantics section, not in this definition, which should be purely
          syntax. We need additional informative text that gives the motivation for these
          definitions. </p>
        <p class="ednote">This definition does not permit the timestamp to be omitted, which is one
          of the data structures that is considered to be in-scope by the requirements document. </p>
        <p> A sequence of RDF graphs (or named graph pairs, or RDF datasets) MAY be physically received
          by an RSP engine, which MAY then create an RDF stream from it by adding timestamps, e.g.
          indicating the time of arrival. The original sequence is not itself an RDF stream. </p>
        <p class="note"> This definition allows the timestamp to be an IRI or blanknode. Additional
          information about the timestamp may be provided in the default graph (e.g., through
          properties in the OWL Time Ontology), but this is not required by the definition of
          timestamped graph. </p>
        <p>A <em>literal-timestamped graph</em> is a timestamped graph whose timestamp
            <code>t</code> is an <code>rdfs:Literal</code>.</p>
        <p class="note">Merge and union of RDF streams with non-literal-timestamped graphs may not
          be defined. See <a href="#merge-union"></a>.</p>
        <p class="ednote">The timestamp predicate <code>p</code> may be drawn from a community
          agreed vocabulary (<a href="https://github.com/streamreasoning/RSP-QL/issues/10">Issue
            10</a>). The timestamp predicate may also be user-defined. </p>
        <p> The format in the examples of this document follows <a href="http://www.w3.org/TR/trig/"
            >TriG</a>, although does not imply any specific serialization or formatting; it simply
          shows the data structured according to the RDF stream model. When the default graph of a
          timestamped graph contains only one triple, this must be the timestamp triple, so there is
          no need of an additional format to designate it. In examples of timestamped graphs having
          more than one triple in the default graph, the first triple of the default graph to occur
          in the serialization is assumed to be the timestamp triple. Prefixes (e.g.
            <code>prov:</code>, <code>dul:</code>) are used for readability; their expansions are defined in 
          <a href="#namespace-table">Table 1</a>.</p>
        <p class="ednote">A non-normative subsection should be created to hold all the information
          about the formatting of examples, and there the expansion of all prefixes that are used in
          examples can be defined.</p>
        <aside class="example" id="literal-TSG" title="Literal-Timestamped Graph">
          <p>The following timestamped graph contains a named graph pair <code>&lt;_:b0, G></code>
            where the graph <code>G</code> contains a triple stating that temperature in Berlin is 12.5 C.
            The timestamp predicate <code>p</code> used in this example is the [[PROV-O]] property <a
              href="https://www.w3.org/TR/prov-o/#generatedAtTime"
              ><code>prov:generatedAtTime</code></a>. The purpose of this timestamp triple is to add
            a temporal aspect to the context of the named graph pair, to the effect that the
            temporal entity <code>"2015-01-01T01:00:00Z"^^xsd:dateTime</code> is the time at which
            the entity <code>_:b0</code> "was completely created and is available for use". The
            details of the semantics of the named graph pair in a timestamped graph are provided in
              <a href="#semantics"></a>
          </p>
          <pre><code class="highlight">_:b0 prov:generatedAtTime "2015-01-01T01:00:00Z"^^xsd:dateTimeStamp .
_:b0 {dbp:Berlin loc:hasPointTempC "12.5"^^xsd:decimal .}</code></pre>
        </aside>
        <p class="ednote">According to the semantics defined in <a href="#semantics"></a>, the
          assertion of the named graph pair means that the graph denoted by <code>:g</code> entails
          the triple <code>dbp:Berlin loc:hasPointTempC "12.5"^^xsd:decimal .</code>,
          under whatever entailment regime is being considered. It does not assert that triple
          directly, nor does it assert that this triple is actually in that graph. Further, it
          does not rule out additional entailments of <code>_:b0</code>. These details are best
          explained in the semantics section itself, although it would probably be helpful to have
          some informative explanation near the beginning to avoid confusion. </p>
        <aside class="example" id="non-literal-TSG" title="Non-Literal-Timestamped Graph">
          <p>The following timestamped graph contains a named graph pair <code>&lt;:g2, G></code>
            where the graph <code>G</code> contains one triple that states that entity
              <code>:axel</code> leaves the <code>:BlueRoom</code>. The timestamp predicate used in
            this example is the DOLCE+DnS Ultralite property <a
              href="http://www.loa-cnr.it/ontologies/DUL.owl#isObservableAt"
                ><code>dul:isObservableAt</code></a>. The purpose of this timestamp triple is to add
            a temporal aspect to the context of the named graph pair, to the effect that there is
            some time <code>_:t2</code> at which an observation could have been (or be) made where
            the results of that observation are described by the entity <code>:g2</code>. There is
            an additional triple in the default graph that compares the observable time to some
            other time <code>_:t1</code> using the Allen interval relation <a
              href="https://www.w3.org/2006/time#after"><code>owl-time:intervalAfter</code></a> from
            [[OWL-TIME]]. </p>
          <pre><code class="highlight">
          :g2 dul:isObservableAt _:t2.
          :g2 {:axel :leave :BlueRoom. }
          _:t2 owl-time:intervalAfter _:t1
        </code></pre>
        </aside>
        <p>Given any two timestamped graphs, <code>TSG1</code> and <code>TSG2</code>, we say that
            <code>TSG2</code>
          <em>covers</em>
          <code>TSG1</code> (denoted <code>TSG1  	&#8818; TSG2</code>) if and only if <code>TSG1</code>
          and <code>TSG2</code> have the same timestamp predicate <code>P</code> and the timestamps,
            <code>t1</code> and <code>t2</code> resp., satisfy <code>t1 &lt;=<sub>P</sub> t2</code>,
          where <code>&lt;=<sub>P</sub></code> is the temporal partial order associated with the time plenum
          of the timestamp predicate <code>P</code>. </p>
        <p class="note">The relation <code>&#8818;</code> between timestamped graphs is a preorder (a
          reflexive, transitive binary relation). It is not a partial order because it doesn't have
          the antisymmetry property (<code>a&#8818; b</code> and <code>b&#8818; a</code> implies <code>a
            = b</code>.) </p>
        <aside class="example highlight" title="Distinct Timestamped Graphs That Cover Each Other">
          <p>Two timestamped graphs that have the same timestamp predicate and the same timestamp
            but different named graph pairs cover each other, but are not equal. This demonstrates that
            the "cover" relation is not antisymmetric. </p>
        </aside>
      </section>
      <section id="rdf-stream">
      <h3>RDF Stream</h3>
      <p>An <em>RDF stream</em>
        <code>S</code> consists of a sequence of timestamped graphs, called its <em>elements</em>, such that 
        elements sharing the same
        timestamp predicate are ordered by the partial order associated with this timestamp predicate.
        I.e., if a stream <code>S</code> contains elements <code>S(i)</code> and <code>S(j)</code>
        with <code>i &lt; j</code> and <code>S(i)</code> covers <code>S(j)</code>, then the 
        timestamps of  <code>S(i)</code> and <code>S(j)</code> are equal. </p>
      <p>If a timestamp predicate occurs in the timestamp triple of an element of an RDF stream, we say
         it is a <em>timestamp predicate of the stream.</em></p>
      <p class="ednote">Time-boundedness properties on RDF streams behave better if it is required
        that the set of temporal entities for each timestamp predicate is pairwise bounded. I.e.,
        for each pair of temporal entities in the set, there is a temporal entity in the set that is
        an upper bound of both, as well as a temporal entity in the set that is a lower bound of
        both. This property is not satisfied by branching temporal structures,  but could be a 
        requirement of some profile.</p>
      <p class="ednote">The comparability between any pair of elements of an RDF stream must (or should?) be
        completely determined from the default graphs of those elements, or elements that precede at least one of them. 
        Otherwise the ordering could
        be revealed by a subsequent element, inducing retroactively an ordering requirement on
        a previous pair of stream elements.</p>
      <p>On the following we may refer to RDF stream simply as stream.</p>
      <aside class="example" id="literal-rdf-stream" title="RDF Stream">
        <p>An RDF stream produces data that indicates where a person is at a given time. The
          timestamp predicate <code>p</code> used in this example is the PROV
            `<code>prov:generatedAtTime</code>. In this example the named graph pairs
            (<code>:g1</code>,<code>:g2</code>, etc.) contain the streaming data contents (for
          brevity the contents are represented by the dots <code>...</code>). </p>
        <pre class="highlight"><code>:g1 {...}{:g1,prov:generatedAtTime,t1}
:g2 {...} {:g2, prov:generatedAtTime, t2}
:g3 {...} {:g3, prov:generatedAtTime, t3}
:g4 {...} {:g4, prov:generatedAtTime, t4}
...
</code></pre>
        <p>We can expand the content of the graph component of each named graph pair, which is a set of triples:</p>
        <pre class="highlight"><code>
:g1 {:axel :isIn :RedRoom. :darko :isIn :RedRoom} 
  {:g1, prov:generatedAtTime, "2015-06-18T12:00:00Z"^^xsd:dateTime}
:g2 {:axel :isIn :BlueRoom. }                     
  {:g2, prov:generatedAtTime, "2015-06-18T12:00:35"^^xsd:dateTime}
:g3 {:minh :isIn :RedRoom. }                      
  {:g3, prov:generatedAtTime, "2015-06-18T12:02:07"^^xsd:dateTime}
...
</code></pre>
      </aside>
      <p class="note">There can be multiple graphs with the same timestamp in the stream.</p>
      <p class="ednote">It has been pointed out that this statement might be problematic, as graphs
        could no longer be used for punctuation purposes. Comparatively, we have not found a
        constraint on this in similar models e.g., CQL: <em>there could be zero, one, or multiple
          elements with the same timestamp in a stream</em>.</p>
      </section>
      <section id="isomorpism">
        <h3>Isomorphism</h3>
        <p>Two RDF timestamped graphs TSG1 and TSG2 are <em>isomorphic</em> if and only if there is
          a bijection M between the nodes, triples, graphs and named graph pairs in TSG1 and those in
          TSG2 such that: <ol>
            <li>M maps blank nodes to blank nodes;</li>
            <li>M is the identity map on literals and IRIs;</li>
            <li>For every triple &lt;s p o>, M(&lt;s, p, o>)= &lt;M(s), M(p), M(o)>;</li>
            <li>For every graph G={t1, ..., tn}, M(G)={M(t1), ..., M(tn)};</li>
            <li>For every named graph pair NG=&lt;n, G>, M(NG)=&lt;M(n), M(G)>;</li>
            <li> M(NG1)=NG2, M(DG1)=DG2, and M(TST1)=TST2, where NG1 is the named graph pair of TSG1, DG1
              is the default graph of TSG1, TST1 is the timestamp triple of TSG1, and similarly for
              TSG2.</li>
          </ol>
        </p>
        <p class="note"> The definition of timestamped graphs allows blank nodes to be used as graph
          names, as well as within triples. </p>
        <pre class="example highlight" title="Isomorphic Timestamped Graphs with Blank-Node Graph Names">
<code>:_1 {...} {:_1, prov:generatedAtTime, t1}
</code>

<code>:_2 {...} {:_2, prov:generatedAtTime, t1}
</code>
      
      </pre>
        <p>Two RDF streams are <em>S-isomorphic</em> if and only if they have the same set of
          elements.</p>
        <pre class="example highlight" title="S-Isomorphic Streams with Unique Well-Ordered Timestamp Predicate">
<code>
:g1 {...} {:g1, prov:generatedAtTime, t1}
:g2 {...} {:g2, prov:generatedAtTime, t2}
:g3 {...} {:g3, prov:generatedAtTime, t2}
:g4 {...} {:g4, prov:generatedAtTime, t3}
</code>

<code>
:g1 {...} {:g1, prov:generatedAtTime, t1}
:g3 {...} {:g3, prov:generatedAtTime, t2}
:g2 {...} {:g2, prov:generatedAtTime, t2}
:g4 {...} {:g4, prov:generatedAtTime, t3}
</code>
    </pre>
        <p>Two RDF streams S1 and S2 are <em>B-isomorphic</em> if and only if there is a bijection M
          between the nodes, triples, graphs, named graph pairs, and timestamped graphs in S1 and those
          in S2 such that: <ol>
            <li>M maps blank nodes to blank nodes;</li>
            <li>M is the identity map on literals and IRIs;</li>
            <li>For every triple &lt;s p o>, M(&lt;s, p, o>)= &lt;M(s), M(p), M(o)>;</li>
            <li>For every graph G={t1, ..., tn}, M(G)={M(t1), ..., M(tn)};</li>
            <li>For every named graph pair NG=&lt;n, G>, M(NG)=&lt;M(n), M(G)>;</li>
            <li>For every timestamped graph TSG where NG is the named graph pair and DG is the default
              graph containing the timestamp triple TST, M(TSG) is a timestamped graph TSG2, with
              named graph pair M(NG), default graph M(DG) and timestamp triple M(TST).;</li>
            <li>For every i &gt;= 1, M(S1(i))=S2(i), where S1(i) is the i-th element of S1 and S2(i)
              is the i-the element of S2.</li>
          </ol>
        </p>
        <p class="note">An RDF stream is viewed as being on a single "RDF surface"(see [[BLOGIC]]),
          so that blank nodes may be shared between any graphs in the stream. For this reason,
          B-isomorphism is defined in terms of a single mapping M for the entire RDF stream rather
          than, say, separate mappings for each timestamped graph.</p>
        <p>Two RDF streams are <em>isomorphic</em> if there exists an RDF stream that is both
          B-isomorphic to one stream and S-isomorphic to the other stream.</p>
        <p class="note"> RDF streams that are S-isomorphic are isomorphic. </p>
        <p class="note"> RDF streams that are B-isomorphic are isomorphic. </p>
        <p>Isomorphic RDF streams MUST have the same semantics. The semantics of RDF streams is
          affected by the result of applying window functions (<a href="#window-functions"></a>) as
          well as by entailment. Therefore, isomorphic RDF streams SHALL be indistinguishable, up to
          isomorphism, with respect to entailment (in any entailment regime), as well as with
          respect to the application of window functions. </p>
      </section>
      <section id="substreams-and-Windows">
        <h3>Substreams and Windows</h3>
        <p>A <em>substream</em>
          <code>S'</code> of an RDF stream <code>S</code> is an RDF stream that is isomorphic to a
          subsequence of <code>S</code>.</p>
        <p class="note">There are several specializations of the substream relation which are useful
          in practice. These include the <em>filter</em> relation, where stream elements are
          selected based on satisfaction of some criterion, and the <em>window</em> relation, where
          a temporally contiguous portion of the stream is selected. </p>
        <p>A <em>window</em>
          <code>S'</code> of an RDF stream <code>S</code> is a substream of <code>S</code> such that
          if <code>S'(i)</code> and <code>S'(j)</code> are two elements of <code>S'</code> and
          <code>S'(i)  	&#8818; S'(j)</code> (i.e., <code>S'(j)</code> covers <code>S'(i)</code>), and
          further if <code>S'(i)  	&#8818; S(k)  	&#8818; S'(j)</code> for some element <code>S(k)</code> of
            <code>S</code>, then <code>S(k)</code> is an element of <code>S'</code>. </p>
        <p class="note">Informally, a window is a temporally-contiguous selection from the original
          stream, without gaps.</p>
      </section>
    </section>
    <section id="merge-union">
      <h2>Stream Merge and Union Operations</h2>
      <p>In order to combine two RDF streams into one, without loss or gratuitous introduction of
        entailments, we define two relations, <em>RDF stream merge</em> and <em>RDF stream
          union</em>. These relations are inspired by the concepts of similar name for RDF graphs
        defined in [[RDF11-Concepts]]. </p>
      <p>
        <em>RDF stream union</em> is a trinary relation on RDF streams. Let <code>S1</code>,
          <code>S2</code>, and <code>S3</code> be RDF streams. We say <code>S3</code> is an <em>RDF
          stream union</em> of <code>S1</code> and <code>S2</code> if and only if the set of
        elements of <code>S3</code> is equal to the union of the sets of elements of <code>S1</code>
        with <code>S2</code>.</p>
      <p class="note"> Union is the appropriate relation to consider when combining streams that are
        allowed to share blank nodes. Informally, they would be on the same RDF surface. </p>
      <aside class="example" title="Union of RDF Streams that Share Blank Nodes">
        <pre><code class="highlight">
        :g1 {_:1 :isIn :RedRoom. _:2 :isIn :RedRoom}    {:g1, prov:generatedAtTime, t1}
        :g2 {_:1 :isIn :BlueRoom. }                     {:g2, prov:generatedAtTime, t2}
        :g3 {_:3 :isIn :RedRoom. }                      {:g3, prov:generatedAtTime, t3}
        </code></pre>
        is a union of the following two streams: <pre><code class="highlight">
          :g1 {_:1 :isIn :RedRoom. _:2 :isIn :RedRoom}  {:g1, prov:generatedAtTime, t1}
          :g3 {_:3 :isIn :RedRoom. }                    {:g3, prov:generatedAtTime, t3}
        </code></pre>
        <pre><code class="highlight">
          :g2 {_:1 :isIn :BlueRoom. }                   {:g2, prov:generatedAtTime, t2}
        </code></pre>
      </aside>
      <p>
        <em>RDF stream merge</em> is a trinary relation on RDF streams. Let <code>S1</code>,
          <code>S2</code>, and <code>S3</code> be RDF streams. We say <code>S3</code> is an <em>RDF
          stream merge</em> of <code>S1</code> and <code>S2</code> if and only if there exist
        streams <code>S4</code>, <code>S5</code>, and <code>S6</code>, with <code>S4</code>
        isomorphic to <code>S1</code>, <code>S5</code> isomorphic to <code>S2</code>,
          <code>S6</code> isomorphic to <code>S3</code>, such that <code>S4</code> and
          <code>S5</code> have no blank nodes in common, and <code>S6</code> is the union of
          <code>S4</code> with <code>S5</code>.</p>
      <p class="note"> Merge is the appropriate relation to consider when joining streams that are
        considered to not share blank nodes. Informally, they would be on different RDF surfaces. </p>
      <aside class="example" title="Merge of RDF Streams that Do Not Share Blank Nodes">
        <pre><code class="highlight">
        _:1 {:axel :isIn :RedRoom. :darko :isIn :RedRoom} {_:1, prov:generatedAtTime, t1}
        _:2 {:axel :isIn :BlueRoom. }                     {_:2, prov:generatedAtTime, t2}
        _:3 {:minh :isIn :RedRoom. }                      {_:3, prov:generatedAtTime, t3}
        </code></pre>
        is a merge of the following two streams: <pre><code class="highlight">
        _:1 {:axel :isIn :RedRoom. :darko :isIn :RedRoom} {_:1, prov:generatedAtTime, t1}
        _:2 {:minh :isIn :RedRoom. }                      {_:2, prov:generatedAtTime, t3}
        </code></pre>
        <pre><code>
        _:1 {:axel :isIn :BlueRoom. }                     {_:1, prov:generatedAtTime, t2}
        </code></pre>
      </aside>
      <p class="note">Given any three RDF streams, the determination of their satisfaction of the
        union or merge relation is a purely syntactic computation. That is, it is not necessary to
        consider entailments of the RDF streams or to be concerned with the values of literals.</p>
      <aside class="ednote">
        <p>It should not be hard to show that each relation defines a (deterministic) binary partial
          operation on the equivalence classes of isomorphic RDF streams. However, note that this
          operation is not purely syntactic. The construction of a union of two streams,
            <code>S1</code> and <code>S2</code>, can be accomplished, theoretically, by the
          following steps:</p>
        <ol>
          <li>Construct the set of timestamped graphs <code>R3</code> that is the set union of
            elements of <code>S1</code> and <code>S2</code>. </li>
          <li>Construct an RDF stream <code>S3</code> from the set <code>R3</code> by placing this
            set into a sequential form that satisfies the definitional constraint regarding relative order of
            elements, if possible.</li>
        </ol>
        <p> The second step may require consideration of the values of timestamps or entailments
          regarding temporal order in order to <em>sequentialize</em> the set of timestamp graphs.
          The construction of a merge is accomplished by performing stream copying to a common
          surface before union: </p>
        <ol>
          <li>Copy both <code>S1</code> and <code>S2</code> to a new RDF surface by renaming blank
            nodes that occur in both streams to fresh blank nodes (new to both <code>S1</code> and
              <code>S2</code>), producing new streams <code>S1'</code> and <code>S2'</code>. </li>
          <li>Form the union of streams <code>S1'</code> and <code>S2'</code>.</li>
        </ol>
      </aside>
      <p class="ednote"> The continuous operation of union or merge of two semi-infinite streams may
        not be computable, even when the operation is defined. This is because it may not be
        possible to know when an element of one stream might be received that needs to occur before
        the latest elements of the other stream. Computability of these continuous operations may be
        possible when additional information is available about the streams to be combined, e.g., the
        set of timestamp predicates used by each stream and the maximum latency of transmission.
        Within some RDF stream profiles, the union and merge operations may be continuously computable
        due to the profile constraints.
      </p>
    </section>
    <section id="subclasses">
      <h2>RDF Stream Subclasses</h2>
      <p class="ednote">This section may be expanded with more subclasses if a need is identified.</p>
      <h3>Time-bounded RDF Streams</h3>
      <p>A <em>time-bounded-above RDF stream</em> is an RDF Stream where for every timestamp
        predicate of the stream, there is a temporal entity in its range that bounds
        from above (is greater than or equal to) all timestamps of that predicate in the stream. </p>
      <p>A <em>time-bounded-below RDF stream</em> is an RDF Stream where for every timestamp
        predicate of the stream, there is a temporal entity in its range that bounds
        from below (is less than or equal to) all timestamps of that predicate in the stream. </p>
      <p>A <em>time-bounded RDF stream</em> is an RDF Stream that is time-bounded above and
        time-bounded below. </p>
      <p class="ednote"> Consider the class of timestamp predicates whose associated poset of
        temporal entities is <em>pairwise bounded</em>, i.e., has the property that every pair of
        temporal entities is bounded, i.e., there exists a temporal entity in the poset that is
        greater or equal to both of them and another temporal entity that is less or equal to both
        of them. This is a fairly natural property to expect for partial orders on time plenums with "linear topology" (e.g.
        [[OWL-TIME]]). However it does not hold in the case of branching time plenums (see <a
          href="http://www.ihmc.us/users/phayes/docs/timeCatalog.pdf"
          >http://www.ihmc.us/users/phayes/docs/timeCatalog.pdf</a>), because there are incomparable
        branches (called paths), which may be considered, e.g., as different worlds (in the sense of
        Kripke frames), scenarios (planning usecase), or concurrent processes (process modelling).
        It would be a useful property if the class of time-bounded RDF streams were closed under
        stream merger and union, and this property would hold, under the current definition, in the
        case of pairwise-bounded time plenums, because we could find bounds for the temporal entities
        of each timestamp predicate by taking an upper bound of the pair of upper bounds from the
        two streams, and similarly for the lower bounds. However, the closure property does not hold
        in the case of a branching temporal topology under the above definition of time-bounded. To
        achieve the closure property, we would have to allow a number of "upper bounds" for each
        timestamp predicate, rather than requiring just one. For example, <br /><br /> Given two
        sets, A and B, of entities in a partial order, we say that A bounds B from above iff there
        is no element of B that is greater than any element of A, and for every element of B there
        is some element of A that is greater than or equal to it. <br /><br /> A
          <em>time-bounded-above RDF stream</em> is an RDF Stream where for every timestamp
        predicate of the stream, there is a set of temporal entities in its range that
        bounds from above all timestamps of that predicate in the stream. </p>
      <p class="note"> The class of time-bounded RDF streams using timestamp predicates with
        pairwise-bounded time plenums is closed under stream merger and union. </p>
      <h3>Temporal-count-bounded RDF Streams</h3>
      <p>A <em>temporal-count-bounded RDF stream</em> is an RDF stream where for each timestamp
        predicate of the stream, there are a finite number of temporal entities in the
        stream that are timestamps for that predicate.</p>
      <p class="ednote">The qualifier "temporal" has been added to this term to emphasize that it is
        temporal entities that are being counted, not timestamped graphs.</p>
      <p class="ednote"> Given an RDF stream whose set of timestamp predicates with pairwise-bounded
        temporal entities, then if the stream is temporal-count-bounded it is also time-bounded,
        since the upper bound for each timestamp predicate can be taken as an upper bound of the
        (finite) set of its timestamps, and similarly for the lower bound. If the definition of
        "time-bounded" was changed as in the earlier Editor's Note, then every
        temporal-count-bounded RDF stream would be time-bounded. </p>
      <p class="note"> Every temporal-count-bounded RDF stream using only timestamp predicates with
        pairwise-bounded time plenums is time-bounded. </p>
      <p class="note"> The class of temporal-count-bounded RDF streams is closed under stream merger
        and union, since the sets of temporal entities for each timestamp predicate in the resulting
        stream are the union of the corresponding (finite) sets in the original streams, and so are
        finite. </p>
      <pre class="example highlight" title="As a pathological example, an RDF stream representing Zeno's paradox using a continuous time plenum (e.g., Julian date) would be time-bounded but not temporal-count-bounded.">          
        </pre>
      <h3>Finite RDF Stream</h3>
      <p>A <em>finite RDF stream</em> is an RDF stream of finite length, i.e., with a finite number
        of elements in it.</p>
      <p class="note"> Clearly, every finite RDF stream is temporal-count-bounded. </p>
      <p class="ednote"> Every finite RDF stream using only pairwise-bounded time plenums is
        time-bounded. However, this is not the case for branching time plenums. </p>
      <p class="note"> Clearly, the class of finite RDF streams is closed under stream merger and
        union. </p>
      <pre class="example highlight" title="As another pathological example, an RDF stream that contained an infinite set of timestamped graphs all with the same timestamp predicate and  timestamp would be temporal-count-bounded but not be finite.">
        </pre>
      <pre class="example highlight" title="Even more pathological, a temporal-count-bounded stream that uses an infinite set of timestamp predicates would not be finite.">
        </pre>
    </section>
    <section id="window-functions">
      <h2>Window Functions</h2>
      <p>A <em>window function</em> is a partial function from RDF streams to their windows that
        preserves isomorphism. That is, if <code>w</code> is a window function, with isomorphic
        streams <code>S1</code> and <code>S2</code> in its domain, then <code>w(S1)</code> is
        isomorphic to <code>w(S2)</code>. </p>
      <p>A <em>general window function</em> is a window function that is a total function on RDF
        streams. </p>
      <p class="ednote">The term "window operator" is reserved for later use to return a sequence of
        windows.</p>
      <p>The most common types of window functions in practice are time-based and count-based.</p>
      <section id="time-based-window-functions">
        <h3>Time-based Window Functions</h3>
        <p>Because the time plenum for each timestamp predicate is partially ordered, we may define a
            <em>temporal interval of a timestamp predicate</em> to be an interval, in the usual
          sense for partial orders, within its time plenum. </p>
        <p class="note">Recall that intervals need not be bounded and need not be closed, and are
          specified in terms of two, one or zero inequality conditions based on the partial order or
          its induced strict order.</p>
        <p>A <em>general time-based window function</em>
          <code>w</code> is a general window function specified by a finite set
            <code>w<sub>P</sub></code> of timestamp predicates together with temporal intervals
              <code>w<sub>J</sub>(P)</code> of each timestamp predicate <code>P</code> in
              <code>w<sub>P</sub></code>, such that for every stream <code>S</code>, an element
            <code>S(i)</code> of <code>S</code> is an element of <code>w(S)</code> if and only if the
          timestamp predicate <code>P</code> of <code>S(i)</code> is in <code>P</code> and the
          timestamp <code>t</code> of <code>S(i)</code> is contained in
            <code>w<sub>J</sub>(P)</code>. </p>
        <p>A <em>time-based window function</em> is the restriction of a general time-based window
          function to a subset of RDF streams.</p>
        <p class="note">The substream resulting from the application of a time-based window function
          is time-bounded.</p>
      </section>
      <section id="temporal-count-based-window-functions">
        <h3>Temporal-count-based Window Functions</h3>
        <p>A <em>general temporal-count-based window function</em>
          <code>w</code> is a general window function specified by a finite set
            <code>w<sub>P</sub></code> of timestamp predicates together with semi-infinite temporal
          intervals <code>w<sub>J</sub>(P)</code> of each timestamp predicate <code>P</code> in
              <code>w<sub>P</sub></code> with endpoints <code>w<sub>T</sub>(P)</code>, and positive
          integers <code>w<sub>N</sub>(P)</code>, for each timestamp predicate <code>P</code> in
              <code>w<sub>P</sub></code> such that for every stream <code>S</code>, an element
            <code>S(i)</code> of <code>S</code> is an element of <code>w(S)</code> if and only if the
          timestamp predicate <code>P</code> of <code>S(i)</code> is in <code>P</code> and <ul>
            <li><code>t</code> is the timestamp of <code>S(i)</code>,</li>
            <li><code>T(S, P, w)</code> is the set of temporal entities that are timestamps for
              elements of <code>S</code> with timestamp predicate <code>P</code>, belong to
                  <code>w<sub>J</sub>(P)</code>, and belong to the closed interval between
                <code>t</code> and <code>w<sub>T</sub>(P)</code>, and</li>
            <li>the cardinality of <code>T(S, P, w)</code> is less than or equal to
                  <code>w<sub>N</sub>(P)</code></li>

          </ul>
        </p>
        <p>A <em>temporal-count-based window function</em> is the restriction of a general
          temporal-count-based window function to a subset of RDF streams.</p>
        <aside class="note">The above definition of temporal-count-based window function allows the
          following characteristics to be set independently for each timestamp predicate of concern: <ul>
            <li>future-facing or past-facing orientation,</li>
            <li>inclusive or exclusive temporal anchor,</li>
            <li>depth.</li>
          </ul>
        </aside>
        <p class="note">The substream resulting from the application of a temporal-count-based
          window function is temporal-count-bounded.</p>
        <p class="note">Due to the potential for stream elements with incomparable or duplicate
          timestamps, the number of elements in the substream having a particular timestamp
          predicate is not guaranteed to be equal to the depth specified for the predicate by the
          temporal-count-based window. </p>
        <p class="note">Temporal-count-based window functions with future-facing orientation on
          timestamp predicates whose time plenum is not totally-ordered are not computable, because in
          general it is not possible to know, at any point in the reception of the stream, whether
          there are further elements of the stream that would be selected by the
          temporal-count-based window function.</p>
        <p>Applications that require a substream with an exact number <code>N</code> of elements for
          a specified timestamp predicate might apply a temporal-count-based window function with
            <code>N</code> for the count of temporal entities, and then randomly discard extra
          elements, according to some criterion, e.g., extreme elements (maximal or minimal,
          depending on the orientation of the counting). However, this extra step causes the process
          to be nondeterministic, and hence does not correspond to a function. If elements are
          discarded in a nonrandom fashion, e.g., based on their order in the stream sequence, then
          this would be a function, but would not preserve isomorphism, and so would not be a window
          function. The issue of obtaining a window function that returns an exact number of
          elements (for a particular predicate) is handled in the next section, where we define the
          concept of window relation, and use it to define an element-count-based window relation.
          When restricted to certain kinds of RDF streams, element-count-based window relations are
          functional so element-count-based window functions can be defined on such subsets. This
          and similar considerations, of importance to implementations, motivate the <a
            href="#profiles">RDF Stream profiles</a> as subsets of RDF streams. </p>
      </section>
      <section id="window-relations">
        <h3>Window Relations</h3>
        <p>A <em>window relation</em> is a binary relation on RDF streams (a relation having an
          extension which is a set of pairs of RDF streams) such that the second element in the pair
          is a window of the first element and preserves isomorphism. That is, if <code>&lt;S1,
            S2&gt;</code> is a member of the extension of a window relation <code>W</code>, and
            <code>S3</code> and <code>S4</code> are isomorphic to <code>S1</code> and
            <code>S2</code>, resp., then <code>&lt;S3, S4&gt;</code> is also a member of the
          extension of <code>W</code>. </p>
        <p>A <em>length-based window relation</em>
          <code>W</code> is a window relation specified by a set of streams
            <code>W<sub>S</sub></code>, a finite set <code>W<sub>P</sub></code> of timestamp
          predicates together with the following parameters for each timestamp predicate
            <code>p</code> in <code>W<sub>P</sub></code>: <ul>
            <li>a semi-infinite temporal interval <code>W<sub>J</sub>(P)</code> of timestamp
              predicate <code>P</code> (the interval of <code>P</code>);</li>
            <li>a temporal entity <code>W<sub>T</sub>(P)</code> which is the finite endpoint of
                  <code>W<sub>J</sub>(P)</code> (the temporal reference of <code>P</code>);</li>
            <li>a positive integer <code>W<sub>N</sub>(P)</code> (the length for
              <code>P</code>)</li>
          </ul> The extension of a length-based window relation is defined as follows: Let
            <code>S</code> be an RDF stream in <code>W<sub>S</sub></code> with window
            <code>S'</code>. Define <code>T(S, P, W)</code> to be the set of temporal entities that
          are timestamps for elements of <code>S</code> with timestamp predicate <code>P</code> and
          belong to <code>W<sub>J</sub>(P)</code>, and <code>T(S', P)</code> to be the set of
          temporal entities that are timestamps for elements of <code>S'</code> with timestamp
          predicate <code>P</code>. The pair <code>&lt;S, S'&gt;</code> is a member of the extension
          of <code>W</code> if and only if the cardinality of <code>T(S', P)</code> is equal to the
          minimum of <code>W<sub>N</sub>(P)</code> and the cardinality of the set <code>T(S, P,
            W)</code> and for each element <code>S'(i)</code> of <code>S'</code>
          <ul>
            <li>the timestamp predicate <code>P</code> of <code>S'(i)</code> is in
                  <code>W<sub>P</sub></code>, and</li>
            <li> the timestamp <code>t</code> of <code>S'(i)</code> is in
                <code>W<sub>J</sub>(P)</code>.</li>
          </ul>
        </p>
        <p>A <em>length-based window function</em>
          <code>W</code> on domain <code>W<sub>S</sub></code> is a length-based window relation
          where any member <code>&lt;S, S'&gt;</code> of its extension are such that <code>S</code>
          is in <code>W<sub>S</sub></code> and <code>W</code> defines a total function on
              <code>W<sub>S</sub></code>.</p>
        <p class="ednote">The names "element-count-bounded window relation" and "element-count-bounded window function"
        have been proposed, as a replacement of "length-bounded ...". This nomenclature depends on the adoption of 
        "element" as the term for the individual timestamped graphs in a stream.</p>
      </section>
    </section>
	-->
		<!--<h2>References:</h2>
    
    <ul>
      <li>EP-SPARQL: a unified language for event processing and stream reasoning.
        Anicic, D., Fodor, P., Rudolph, S., &amp; Stojanovic, N. In WWW (p. 635-644). ACM. 2011.</li>
      <li>C-SPARQL: a Continuous Query Language for RDF Data Streams. 
        Barbieri, D. F., Braga, D., Ceri, S., Della Valle, E., &amp; Grossniklaus, M. Int. J. Semantic Computing, 4(1), 3-25. 2010.</li>
      <li>Enabling query technologies for the semantic sensor web. 
        Calbimonte, J.-P., Jeung, H., Corcho, Ó., &amp; Aberer, K. Int. J. Semantic Web Inf. Syst., 8(1), 43-63. 2012.</li>
      <li>RSP-QL Semantics: a Unifying Query Model to Explain Heterogeneity of RDF Stream Processing Systems. 
        D. Dell’Aglio, E. Della Valle, J.-P. Calbimonte, O. Corcho. Int. J. Semantic Web Inf. Syst, 10(4). (in press). 2015.</li>
      <li>A Native and Adaptive Approach for Unified Processing of Linked Streams and Linked Data.
        Phuoc, D. L., Dao-Tran, M., Parreira, J. X., &amp; Hauswirth, M.In ISWC (Vol. 7031, p. 370-388). Springer. 2011.</li>
      <li>LARS: A Logic-based Framework for Analyzing Reasoning over Streams.
        Beck, H., Dao-Tran, M., Eiter, T., Fink, M. In AAAI. 2015.</li>
      <li>RDF 1.1: On Semantics of RDF Datasets. Zimmerman, Antoine, ed.. 2014.  <a href="http://www.w3.org/TR/2014/NOTE-RDF11-datasets-20140225">http://www.w3.org/TR/2014/NOTE-RDF11-datasets-20140225</a>.</li>
    </ul>-->
		<!--
    <blockquote>
      <p class="note">this example could be integrated to the main text body</p>
      <h3>Beyond time instants: intervals &amp; more</h3>
      <p>Usign the previously described model, intervals can be specified for a graph in the
        following way: Given p1 and p2 representing start and end time predicates, then
          <code>(g,p1,t1)</code> and <code>(g,p2,t2)</code> denote that g is defined in an interval
        [t1,t2]. As an example:</p>
      <pre class="example highlight" title="RDF Stream with Two Timestamp Predicates for One Named Graph Pair"><code>:g_1, :startsAt, "2015-06-18T12:00:00"^^xsd:dateTime
:g_1, :endsAt, "2015-06-18T13:00:00"^^xsd:dateTime
</code></pre>
      <p>Or even:</p>
      <pre class="example highlight" title="RDF Stream with Interval Timestamp"><code>:g_2 :validBetween
    [:startsAt "2015-06-18T12:00:00"^^xsd:dateTime;
    :endsAt "2015-06-18T13:00:00"^^xsd:dateTime]
</code></pre>
    </blockquote>
    <section id="semantics">
      <h2>Semantics</h2>
      <p> The semantics of a timestamped graph is defined in terms of its semantics as an RDF
        Dataset. In particular, the designation of a particular triple in the default graph as the
        timestamp triple has no effect on its semantics. </p>
      <p>The semantics of timestamped graphs, and consequently of RDF streams, is based on the
        semantics formalized in [[RDF11-Datasets]] in the case that each named graph pair defines its own
        context.</p>
      <p>The following terms are used in the sense of [[RDF11-MT]]: <em>entailment regime,
          E-interpretation, blank node, universe, named graph pair, RDF graph, E-entails, default
          graph</em>.</p>
      <p> An <em>RSP interpretation</em>
        <code>I</code> with respect to an entailment regime <code>E</code> is an E-interpretation
        extended to named graph pairs, timestamped graphs, RDF datasets, and RDF streams as follows: <ul>
          <li>given a mapping <code>A</code> from blank nodes to the universe <code>IR</code> and a
            named graph pair <code>ng</code> = <code>&lt;n, G></code>, <code>[I+A](ng)</code> is
            true if and only if <code>[I+A](n)</code> is an RDF graph that E-entails
            <code>G</code>;</li>
          <li>for a timestamped graph <code>TSG</code> = <code>&lt; ng, DG></code>, where
              <code>DG</code> is the default graph and <code>ng</code> is the named graph pair of
              <code>TSG</code>,and a mapping <code>A</code> from blank nodes to the universe
              <code>IR</code>, <code>[I+A](TSG)</code> is true if and only if <code>[I+A](DG)</code>
            is true and <code>[I+A](ng)</code> is true;</li>
          <li><code>I(TSG)</code> true if and only if there exisits a mapping <code>A</code> from
            blank nodes to the universe <code>IR</code> such that [I+A](TSG) is true.</li>
          <li>for an RDF dataset <code>D</code> = <code>&lt; DG, NG></code>, where <code>DG</code>
            is the default graph and <code>NG</code> is the set of named graph pairs of
              <code>D</code>, <code>I(D)</code> is true if there exists a mapping <code>A</code>
            from blank nodes to the universe <code>IR</code> such that <code>[I+A](DG)</code> is
            true and <code>[I+A](ng)</code> is true for every <code>ng</code> in
            <code>NG</code>;</li>
          <li><code>I(D)</code> is false otherwise.</li>
          <li>for an RDF stream <code>S</code>, I(S) is true if and only if if there exists a
            mapping <code>A</code> from blank nodes to the universe <code>IR</code> such that for
            every element <code>S(i)</code> of <code>S</code>, <code>i &gt;= 1</code>,
              <code>[I+A](S(i))</code> is true.</li>
        </ul> We say that an RSP interpretation <code>I</code> E-satisfies a graph, named graph
        pair, timestamped graph, dataset, or stream <code>X</code> (or satisfies <code>A</code>
        w.r.t the E-entailment regime) when <code>I(X)</code> is true. </p>
      <p>Following standard terminology, we say that a graph, named graph pair, timestamped graph,
        dataset, or stream <code>X</code> RSP-E-entails a graph, named graph pair, timestamped graph,
        dataset, or stream <code>Y</code> if and only if for every RSP interpretation <code>I</code>
        with respect to E-entailment, <code>I</code> E-satisfies <code>Y</code> whenever
          <code>I</code> E-satisfies <code>X</code>.</p>
      <p class="note">The "RSP" prefix in "RSP-E-entails" may be dropped when neither antecedent nor
        consequent is an RDF dataset or named graph pair, as in that case there is no possibility of
        alternate dataset semantics.</p>
      <p class="ednote">It should be straightforward to prove that isomorphic RDF streams simply
        entail each other, and hence are logically equivalent under simple entailment. This should
        also hold for other standard entailment regimes, and perhaps should be required for all RSP
        entailment regimes.</p>
      <p class="ednote"> The notion of RSP-E-entailment is defined here so that it may be used to
        support the definition of query semantics under various entailment regimes in the [[RSP-QL
        Queries]] document, including simple entailment. A choice was made regarding the semantics
        of named graph pairs (i.e., <code>[I+A](ng)</code> is true if and only if
          <code>[I+A](n)</code> is an RDF graph that E-entails <code>G</code>) that affects the
        semantics of timestamped graphs, datasets, and streams. It remains to be seen if this choice
        supports query semantics in a manner that meets the needs of the RSP-QL community. In
        particular, it imposes constraints on the denotation of the name of a named graph pair which
        may be inconsistent with the domain of a predicate that might be considered for use as a
        timestamp predicate (e.g., SSN predicate <code>ssn:observationSamplingTime</code>). </p>
    </section>
    <section id="profiles">
      <h2>Profiles</h2>
      <p>It is possible to restrict the abstract syntax for a class of RDF streams. This is often
        done in order to facilitate the efficient implementation of certain operations and queries
        or to promote more efficient representation. Each such restriction constitutes an <em>RDF
          stream profile</em>.</p>
      <p>The profiles defined in this document fall into two categories: time-series profiles and
        linked-list profiles. Within each category there is a least-restrictive profile and a number
        of subprofiles which apply additional restrictions to it.</p>
      <section id="timeseries">
        <h3>RDF Time-Series Profiles</h3>
        <p>This section describes four profiles which are defined through restrictions on the
          default graphs of the stream's timestamped graph elements or on the relation between
          timestamp values: <ul>
            <li>RDF time series</li>
            <li>RDF distinct time series</li>
            <li>RDF regular time series</li>
            <li>RDF distinct regular time series</li>
          </ul></p>
        <section id="time-series-profile">
          <h4>RDF Time-Series Profile</h4>
          <p>The RDF Stream Time-Series profile is designed to support high-volume, low-latency
            window operations and queries that depend on full knowledge of timestamps.</p>
          <p class="ednote">This motivational paragraph needs some work - the above is a place
            holder.</p>
          <p>An <em>RDF time series</em> is an RDF stream that satisfies the following
            properties:</p>
          <ol>
            <li>The stream uses exactly one timestamp predicate.</li>
            <li>The range of this timestamp predicate is xsd:dateTimeStamp.</li>
            <li>No two elements in the time series have the same graph name.</li>
          </ol>
          <aside class="example">
            <p><a href="#literal-rdf-stream">Example 4</a> satisfies the first and third
              requirements of the time-series profile. However, the range of the predicate
                <code>prov:generatedAtTime</code> is not limited to xsd:dateTimeStamp -
              non-timezoned values of xsd:dateTime are also permitted.</p>
          </aside>
          <p class="ednote">If some property (e.g., in the RSP-QL namespace) is defined as a
            subproperty of <code>prov:generatedAtTime</code>, then this can be substituted in <a
              href="#literal-rdf-stream">Example 4</a>, which will then serve as an example of a
            time series. Alternatively, the definition could be changed so that the only requirement
            is that the timestamps that occur in the stream belong to xsd:dateTime. </p>
          <p class="note">The set of RDF time series is closed under RDF stream merge and union on
            time series that use the same timestamp predicate.</p>
        </section>
        <section id="distinct-time-series-profile">
          <h4>RDF Distinct Time-Series Profile</h4>
          <p>An "RDF distinct time series" is an RDF time series such that no two elements in the
            time series have the same timestamp.</p>
          <aside class="example">
            <p> A distinct time series example ...</p>
          </aside>
          <p class="note">An RDF distinct time series has a unique sequential order - it is
            S-isomorphic only to itself.</p>
          <p class="note">The merge or union of RDF distinct time series is always defined but is
            not necessarily an RDF distinct time series, even if the operation is restricted to time
            series that use the same timestamp predicate, due to the possibility of duplication of
            timestamp. However, the merge or union of RDF distinct time series that use the same
            timestamp predicate is always an RDF time series. </p>
          <p class="note">The restriction of element-count-based window relations 
            to distinct time series results in functional
            relations. This enables the definition of element-count-based window functions for distinct time series. 
            Further, the element-count-based window functions are always computable. However, the
            time-based window functions are not necessarily computable unless there is a finite
            precision to the timestamps, as can be seen from the Zeno's paradox example @@@(this
            example hasn't been written up yet, but is mentioned elsewhere). </p>
          <p class="ednote">There is going to be a lot more to talk about for RDF time series
            relative to queries, but in this document we don't have those definitions.</p>
        </section>
        <section id="regular-time-series-profile">
          <h4>RDF Regular Time-Series Profile</h4> A <em>regular</em> RDF time series is a further
          subclass of distinct time series where the duration between timestamps is an integer
          multiple of a specified duration, called the <em>spacing</em> of the time series. <aside
            class="example">
            <p> A regular time series example ...</p>
          </aside>
          <p class="note"> The restriction of timestamps in the regular time series profile allows
            the timestamp to be represented concisely as an integer, provided the spacing is
            provided elsewhere, e.g., in metadata. </p>
        </section>
        <section id="distinct-regular-time-series-profile">
          <h4>RDF Distinct Regular Time-Series Profile</h4> A <em>distinct regular</em> RDF time
          series is a further subclass of distinct time series that is both a distinct time series
          and a regular time series. <aside class="example">
            <p> A distinct regular time series example ...</p>
          </aside>
          <p class="note"> @@@motivate this profile </p>
        </section>
      </section>
      <section id="linked-list-profiles">
        <h3>RDF Linked-List Profiles</h3>
      </section>
      <aside class="ednote"><p>In order to indicate which profile applies to a stream, the
          transmission must include that information in addition to the stream content. For streams
          which are delivered the method is to supply the profile as a <code>profile</code>
          parameter to the media type. The following profiles are defined <ul>
            <li><code>http://www.w3.org/ns/rsp-ql#time-series</code></li>
            <li><code>http://www.w3.org/ns/rsp-ql#distinct-time-series</code></li>
            <li><code>http://www.w3.org/ns/rsp-ql#synchronous-time-series</code></li>
            <li><code>http://www.w3.org/ns/rsp-ql#linked-stream</code></li>
          </ul>
        </p>
        <p>This information shouldn't stay in this document - it probably should go into the Serialization document.</p>
      </aside>
    </section>
	  -->
		<section id="structural-inferences">
			<div style="text-align: left;">
				<table class="thinborder" style="margin-left: auto; margin-right: auto;">
					<caption id="structural-inferences-table"><b>Table D</b>: Inferences under the
						provenance and structural entailment regimes</caption>
					<tbody>
						<tr>
							<th><b>Structure</b></th>
							<th><b>Regime</b></th>
							<th><b>Property</b></th>
							<th><b>Usage</b>
							</th>
						</tr>
						<tr>
							<td><a>Repository</a>
								<a>Store</a>
								<a>Recursive Store</a></td>
							<td>provenance</td>
							<td>prov:wasRevisionOf</td>
							<td>Usage </td>
						</tr>
						<tr>
							<td><a>Repository</a>
								<a>Store</a>
								<a>Recursive Store</a></td>
							<td>provenance</td>
							<td>repo:isInitialVersion</td>
							<td>Usage </td>
						</tr>
						<tr>
							<td><a>Repository</a>
								<a>Store</a>
								<a>Recursive Store</a></td>
							<td>structural</td>
							<td>repo:isTerminalVersion</td>
							<td>Usage </td>
						</tr>
						<tr>
							<td><a>Named Repository</a>
								<a>Named Recursive Store</a></td>
							<td>provenance</td>
							<td>prov:isSpecializationOf</td>
							<td>Usage </td>
						</tr>
						<tr>
							<td><a>Named Repository</a>
								<a>Named Recursive Store</a></td>
							<td>provenance</td>
							<td>repo:isInitialVersionOf</td>
							<td>Usage </td>
						</tr>
						<tr>
							<td><a>Named Repository</a>
								<a>Named Recursive Store</a></td>
							<td>structural</td>
							<td>repo:isTerminalVersionOf</td>
							<td>Usage </td>
						</tr>
					</tbody>
				</table>
			</div>
		</section>
		<!-- CONFORMANCE -->
		<section id="conformance"> </section>
	</body>
</html>
